Citation Text,predictions
"abbasi (2010) proposed that feature selection methods should be tailored to sentiment analysis by combining syntactic properties of text features with sentiment-related semantic information. syntactic information is computed by considering rule-based relations between various categories of features, like simple words and partof-speech tags. semantic information is computed by assigning weight to features according to their (i) occurrence distributions across categories in the training data and (ii) degree of subjectivity, which is derived from sentiwordnet (baccianella & sebastiani, 2010), a publicly available lexical resource that contains sentiment polarity scores. dang, zhang, and chen (2010) also used various",1
"in this section we review fundamental aspects of three popular supervised classifiers: naive bayes, support vector machines and artificial neural networks. instead of providing a detailed description of these approaches, we focus on reviewing concepts of svm and ann with the purpose of discussing important issues in a fundamental comparative analysis (see section 4.4). despite the low computational cost of the naive bayes technique, it has not been competitive in terms of classification accuracy when compared to svm (abbasi et al., 2008; pang et al., 2002) and therefore we only analyze the nb method comparatively in the context of our experimental results. more details on models for text classification and nb classifiers can be found in mccallum and nigam (1998), manning et al. (2008).",1
"another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to when considering an ig-based term selection of more than 5001000 terms. in summary, although reducing input features is critical to make the ann training process practical, it does not represent a disadvantage in comparison to svm, since svm also benefits from the feature selection (abbasi et al., 2011, 2008; dang et al., 2010; li et al., 2009), especially in the context of large-scale sentiment classification tasks (bespalov et al., 2011).",1
"categories of features, which were refined by applying the information gain (ig) technique (yang & pedersen, 1997). authors proposed a process of extracting sentiment features, what involves identifying adjectives, adverbs and verbs, and assigning sentiment scores to them according to the sentiwordnet. recently, he, lin, and alani (2011) proposed to detect sentiment and topic simultaneously from text and show that a state-of-the-art performance can be achieved by augmenting features with polarity word labels. as discussed above, literature is focused on feature selection, which provides the input to sentiment learning algorithms. the most popular sentiment learning techniques are svm and nb, and many authors have reported better accuracy by using svm (abbasi, 2010; dang et al., 2010; o'keefe & koprinska, 2009; pang et al., 2002; prabowo & thelwall, 2009; ye, zhang, & law, 2009).",1
"categories of features, which were refined by applying the information gain (ig) technique (yang & pedersen, 1997). authors proposed a process of extracting sentiment features, what involves identifying adjectives, adverbs and verbs, and assigning sentiment scores to them according to the sentiwordnet. recently, he, lin, and alani (2011) proposed to detect sentiment and topic simultaneously from text and show that a state-of-the-art performance can be achieved by augmenting features with polarity word labels. as discussed above, literature is focused on feature selection, which provides the input to sentiment learning algorithms. the most popular sentiment learning techniques are svm and nb, and many authors have reported better accuracy by using svm (abbasi, 2010; dang et al., 2010; o'keefe & koprinska, 2009; pang et al., 2002; prabowo & thelwall, 2009; ye, zhang, & law, 2009).",1
"consist of 100 positive and 100 negative reviews. the remaining 1800 reviews (900 positive and 900 negative) are reserved for training the classifiers. most of our results are given as a function of vocabulary sizes, since we aim to compare the behavior of classifiers, and their requirements to achieve better levels of accuracy, as a function of the number of input terms/dimensions. the vocabularies consist of terms that were best ranked by the ig technique in the training stage. we arbitrarily chose seven quantity of terms between 50 and 5000. in order to evaluate how different is the accuracy between svm and ann classifiers, we applied the t student test with 5% of significance (alpaydin, 2004).",1
"another characteristic of the sentiment classification literature is that many methods have been tested only on balanced datasets and there has been little discussion on the effects of learning subjective aspects from unbalanced data, although it is typical of the product domain to have substantially more positive than negative reviews (burns, bi, wang, & anderson, 2011; li, wang, zhou, & lee, 2011a). burns et al. (2011) address sentiment classification on unbalanced data, however the experiments do not involve neither svm nor ann. li et al. (2011a, 2011b) adopt a random under-sampling method, which is a popular approach to deal with imbalanced data. the major drawback of random under-sampling is that it can discard potentially useful data that could be important for the learning process. in order to overcome this problem, wang, li, zhou, li, and zhu (2011) propose combining multiple classifiers, which are trained from multiple instances of under-sampled data. this ensemble learning approach can be computationally expensive (xia, zong, & li, 2011) and no discussion on this issue is reported by the authors. in contrast to those approaches that discuss imbalanced sentiment classification, we evaluate the performance of ann as the learning approach and report results in a context in which no previous stages are considered to deal with imbalanced data, like sampling techniques (van hulse, khoshgoftaar, & napolitano, 2007).",2
"whitelaw, garg, and argamon (2005) proposed considering adjectival expressions as an important indication of the sentiment polarity in textual reviews. zaidan, eisner, and piatko (2007) proposed learning the sentiment polarity of reviews from an additional source of information. basically, human annotators were requested to highlight the most important words and sentences that justify why a review is positive or negative. li et al. (2009) analyzed six popular feature selection methods and conclude that terms with (i) higher document frequency and (ii) higher category ratio are more informative/effective for classification. o'keefe and koprinska (2009) also evaluated feature selection techniques as well as feature weighting methods. best results were achieved using the categorical proportional difference as a feature selection metric, which is close in meaning to the category ratio discussed in li et al. (2009).",1
"for the ann classifier, we used the traditional feed-forward network with a single hidden layer. the number of neurons in the hidden layer m is selected from the set m 2 {15, . . . , 55}. in order to overcome the problem of convergence to a satisfactory solution, our exhaustive procedure of selecting parameters involved training candidate models three times, each of which started with a different set of randomly generated weights. repeated training with random starting weights is a popular method to overcome the convergence problem (atakulreka & sutivong, 2007). we used the back-propagation algorithm to train the neural network models. instead of adopting the traditional gradient descent method, we use the scaled conjugated gradient to speed up the convergence to a solution (fodslette & mnller, 1993), as implemented in the matlab software. in order to accelerate the training process and reduce the risk of overfitting, we adopted the early stopping procedure (bishop, 2007).",1
"abbasi (2010) proposed that feature selection methods should be tailored to sentiment analysis by combining syntactic properties of text features with sentiment-related semantic information. syntactic information is computed by considering rule-based relations between various categories of features, like simple words and partof-speech tags. semantic information is computed by assigning weight to features according to their (i) occurrence distributions across categories in the training data and (ii) degree of subjectivity, which is derived from sentiwordnet (baccianella & sebastiani, 2010), a publicly available lexical resource that contains sentiment polarity scores. dang, zhang, and chen (2010) also used various",1
"another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to",1
"differ from those produced in the balanced data context. in order to approach realistic conditions of unbalanced data, we vary only the number of negative reviews in a training dataset, since in practice the number of positive reviews is substantially greater than the number of negative reviews (burns et al., 2011; glorot, bordes, & bengio, 2011).",1
"another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to",1
"ann has figured rarely in the literature (bespalov, bai, qi, & shokoufandeh, 2011; chen et al., 2011; claster, hung, & shanmuganathan, 2010; zhu, xu, & shi wang, 2010), as can be seen in recent surveys on sentiment analysis (pang & lee, 2008; tsytsarau & palpanas, 2012). in order to reduce the training time, chen et al. (2011) propose combining word features to model an ann-based approach with few input neurons. however, the experiments do not involve unbalanced datasets as well as a comparison with popular sentiment learning techniques like nb and svm. zhu et al. (2010) propose simulating the human's judgment on sentiment polarity by using an ann-based individual model. authors report a comparative analysis between the proposed ann-based method and svm, however a performance evaluation on unbalanced datasets is not discussed. instead of classifying positive versus negative reviews, claster et al. (2010) propose using selforganizing maps (som) to cluster microblog posts according to subjective aspects in movies domain, like ''funny'' and ''predictable''. perhaps the most conclusive experiments that compare variants of an ann-based method with svm-based approaches for sentiment learning are reported in bespalov et al. (2011). however, authors considered only balanced data in the experiments and did not discuss computational issues, hence our work can be understood as an extension of bespalov et al. (2011) to the context of unbalanced datasets as well as in discussing the computational requirements of both ann and svm models to achieve comparable results.",1
"when considering an ig-based term selection of more than 5001000 terms. in summary, although reducing input features is critical to make the ann training process practical, it does not represent a disadvantage in comparison to svm, since svm also benefits from the feature selection (abbasi et al., 2011, 2008; dang et al., 2010; li et al., 2009), especially in the context of large-scale sentiment classification tasks (bespalov et al., 2011).",1
"for the ann classifier, we used the traditional feed-forward network with a single hidden layer. the number of neurons in the hidden layer m is selected from the set m 2 {15, . . . , 55}. in order to overcome the problem of convergence to a satisfactory solution, our exhaustive procedure of selecting parameters involved training candidate models three times, each of which started with a different set of randomly generated weights. repeated training with random starting weights is a popular method to overcome the convergence problem (atakulreka & sutivong, 2007). we used the back-propagation algorithm to train the neural network models. instead of adopting the traditional gradient descent method, we use the scaled conjugated gradient to speed up the convergence to a solution (fodslette & mnller, 1993), as implemented in the matlab software. in order to accelerate the training process and reduce the risk of overfitting, we adopted the early stopping procedure (bishop, 2007).",1
"although we have reported results only with an usual nonlinear kernel, our experiments indicate that svm requires a high number of support vectors to classify reviews as expressing positive or negative opinions at document level (see table 8). therefore, it is reasonable to conclude that the classes cannot be well (and linearly) separated on the basis of single terms as the dimensions of the input space, resulting in a svm's running time much higher than that of an ann. thus, although cristianini and shawe-taylor (2000) mentioned that in practice svm frequently results in very few support vectors, our results are consistent with previous results on text categorization (colas, pacli' k, kok, & brazdil, 2007) in reporting a high number of support vectors.",1
"we focus on comparing svm and ann learning models when they achieve the best classification accuracy. however, both models require a critical parameter setting, which is still a research issue. a detailed discussion covering parameter values and their implications are beyond the scope of our work and can be found in ben-hur and weston (2010), burges (1998), hastie et al. (2001), haykin (1998). for each set of selected features, we perform an exhaustive searching through a subset of critical parameters values. specifically, the parameter c in the svm algorithm and the number of neurons m in the hidden layer of a neural network are empirically fitted in a grid search fashion guided by better values of accuracy.",1
"despite structural similarities in the output function, the models differ in the way the solutions are obtained. the number m of support vectors is usually a result of the optimization problem posed, and the support vectors fxkgm k1 are always a subset of the data in the svm algorithm. this property does not usually hold for ann (romero & toppo, 2007) and the number m of hidden nodes in a neural network is a free parameter which is usually fixed previously. therefore, in contrast to neural networks, svm algorithm has a desired property of automatically selecting their model size m (by selecting the support vectors as a fraction of the training data). however, it is important to note that a large set of support vectors can be needed to form the output function, making svm computationally slow in running time (test phase) and expensive for real-time applications (burges, 1998; romero & toppo, 2007). although the definition of an appropriate number m of hidden nodes in designing of a neural network is not a trivial task, the model complexity is usually controlled by keeping the number of hidden nodes small (haykin, 1998). this is one of the issues under investigation in our work, i.e. how is the performance of an ann in comparison with a svm model when the number of hidden neurons is a fraction of the number of support vectors in a sentiment classification task?",1
"another characteristic of the sentiment classification literature is that many methods have been tested only on balanced datasets and there has been little discussion on the effects of learning subjective aspects from unbalanced data, although it is typical of the product domain to have substantially more positive than negative reviews (burns, bi, wang, & anderson, 2011; li, wang, zhou, & lee, 2011a). burns et al. (2011) address sentiment classification on unbalanced data, however the experiments do not involve neither svm nor ann. li et al. (2011a, 2011b) adopt a random under-sampling method, which is a popular approach to deal with imbalanced data. the major drawback of random under-sampling is that it can discard potentially useful data that could be important for the learning process. in order to overcome this problem, wang, li, zhou, li, and zhu (2011) propose combining multiple classifiers, which are trained from multiple instances of under-sampled data. this ensemble learning approach can be computationally expensive (xia, zong, & li, 2011) and no discussion on this issue is reported by the authors. in contrast to those approaches that discuss imbalanced sentiment classification, we evaluate the performance of ann as the learning approach and report results in a context in which no previous stages are considered to deal with imbalanced data, like sampling techniques (van hulse, khoshgoftaar, & napolitano, 2007).",2
"differ from those produced in the balanced data context. in order to approach realistic conditions of unbalanced data, we vary only the number of negative reviews in a training dataset, since in practice the number of positive reviews is substantially greater than the number of negative reviews (burns et al., 2011; glorot, bordes, & bengio, 2011).",1
"we train the svm classifier with an usual nonlinear kernel (radial basis function) using libsvm software package (chang & lin, 2011). we used default parameter values, except for the cost constant c, whose value is selected from the interval c 2 [10(cid:2)1, 103].",1
"abbasi (2010) proposed that feature selection methods should be tailored to sentiment analysis by combining syntactic properties of text features with sentiment-related semantic information. syntactic information is computed by considering rule-based relations between various categories of features, like simple words and partof-speech tags. semantic information is computed by assigning weight to features according to their (i) occurrence distributions across categories in the training data and (ii) degree of subjectivity, which is derived from sentiwordnet (baccianella & sebastiani, 2010), a publicly available lexical resource that contains sentiment polarity scores. dang, zhang, and chen (2010) also used various",1
"ann has figured rarely in the literature (bespalov, bai, qi, & shokoufandeh, 2011; chen et al., 2011; claster, hung, & shanmuganathan, 2010; zhu, xu, & shi wang, 2010), as can be seen in recent surveys on sentiment analysis (pang & lee, 2008; tsytsarau & palpanas, 2012). in order to reduce the training time, chen et al. (2011) propose combining word features to model an ann-based approach with few input neurons. however, the experiments do not involve unbalanced datasets as well as a comparison with popular sentiment learning techniques like nb and svm. zhu et al. (2010) propose simulating the human's judgment on sentiment polarity by using an ann-based individual model. authors report a comparative analysis between the proposed ann-based method and svm, however a performance evaluation on unbalanced datasets is not discussed. instead of classifying positive versus negative reviews, claster et al. (2010) propose using selforganizing maps (som) to cluster microblog posts according to subjective aspects in movies domain, like ''funny'' and ''predictable''. perhaps the most conclusive experiments that compare variants of an ann-based method with svm-based approaches for sentiment learning are reported in bespalov et al. (2011). however, authors considered only balanced data in the experiments and did not discuss computational issues, hence our work can be understood as an extension of bespalov et al. (2011) to the context of unbalanced datasets as well as in discussing the computational requirements of both ann and svm models to achieve comparable results.",1
"ann has figured rarely in the literature (bespalov, bai, qi, & shokoufandeh, 2011; chen et al., 2011; claster, hung, & shanmuganathan, 2010; zhu, xu, & shi wang, 2010), as can be seen in recent surveys on sentiment analysis (pang & lee, 2008; tsytsarau & palpanas, 2012). in order to reduce the training time, chen et al. (2011) propose combining word features to model an ann-based approach with few input neurons. however, the experiments do not involve unbalanced datasets as well as a comparison with popular sentiment learning techniques like nb and svm. zhu et al. (2010) propose simulating the human's judgment on sentiment polarity by using an ann-based individual model. authors report a comparative analysis between the proposed ann-based method and svm, however a performance evaluation on unbalanced datasets is not discussed. instead of classifying positive versus negative reviews, claster et al. (2010) propose using selforganizing maps (som) to cluster microblog posts according to subjective aspects in movies domain, like ''funny'' and ''predictable''. perhaps the most conclusive experiments that compare variants of an ann-based method with svm-based approaches for sentiment learning are reported in bespalov et al. (2011). however, authors considered only balanced data in the experiments and did not discuss computational issues, hence our work can be understood as an extension of bespalov et al. (2011) to the context of unbalanced datasets as well as in discussing the computational requirements of both ann and svm models to achieve comparable results.",1
"when consumers purchase products, a process of quality evaluation takes place naturally in their minds. from the industry point of view, to know the feelings among consumers may support strategic market decisions. on the other hand, potential consumers are often interested in the opinion of current customers in order to find out the choice that best fits their preferences. nowadays, many people make their opinions available on the internet and researchers have been proposing methods to automate the task of classifying these textual reviews/opinions as positive or negative (hatzivassiloglou & mckeown, 1997; pang, lee, & vaithyanathan, 2002; pang & lee, 2008; turney, 2002). the research field is know as opinion mining or sentiment classification and a complete overview on the subject is presented in liu (2011) and pang and lee (2008). basically, most of the methods in the literature are composed of two parts: (i) feature selection and (ii) sentiment learning/ classification (abbasi, france, zhang, & chen, 2011; chen, liu, & chiu, 2011; li, xia, zong, & huang, 2009; pang et al., 2002). in general, techniques in the literature can be differed in terms of the adopted approach for feature selection, since most of them agree",2
"on the learning techniques: support vector machine (svm) and naive bayes (nb) (abbasi, chen, & salem, 2008; tang, tan, & cheng, 2009; tsytsarau & palpanas, 2012). as discussed in section 2, artificial neural networks (ann) has attracted little attention as an approach for sentiment learning and a comparative study of ann and popular sentiment classifiers under the same framework is still missing.",1
"when consumers purchase products, a process of quality evaluation takes place naturally in their minds. from the industry point of view, to know the feelings among consumers may support strategic market decisions. on the other hand, potential consumers are often interested in the opinion of current customers in order to find out the choice that best fits their preferences. nowadays, many people make their opinions available on the internet and researchers have been proposing methods to automate the task of classifying these textual reviews/opinions as positive or negative (hatzivassiloglou & mckeown, 1997; pang, lee, & vaithyanathan, 2002; pang & lee, 2008; turney, 2002). the research field is know as opinion mining or sentiment classification and a complete overview on the subject is presented in liu (2011) and pang and lee (2008). basically, most of the methods in the literature are composed of two parts: (i) feature selection and (ii) sentiment learning/ classification (abbasi, france, zhang, & chen, 2011; chen, liu, & chiu, 2011; li, xia, zong, & huang, 2009; pang et al., 2002). in general, techniques in the literature can be differed in terms of the adopted approach for feature selection, since most of them agree",2
"ann has figured rarely in the literature (bespalov, bai, qi, & shokoufandeh, 2011; chen et al., 2011; claster, hung, & shanmuganathan, 2010; zhu, xu, & shi wang, 2010), as can be seen in recent surveys on sentiment analysis (pang & lee, 2008; tsytsarau & palpanas, 2012). in order to reduce the training time, chen et al. (2011) propose combining word features to model an ann-based approach with few input neurons. however, the experiments do not involve unbalanced datasets as well as a comparison with popular sentiment learning techniques like nb and svm. zhu et al. (2010) propose simulating the human's judgment on sentiment polarity by using an ann-based individual model. authors report a comparative analysis between the proposed ann-based method and svm, however a performance evaluation on unbalanced datasets is not discussed. instead of classifying positive versus negative reviews, claster et al. (2010) propose using selforganizing maps (som) to cluster microblog posts according to subjective aspects in movies domain, like ''funny'' and ''predictable''. perhaps the most conclusive experiments that compare variants of an ann-based method with svm-based approaches for sentiment learning are reported in bespalov et al. (2011). however, authors considered only balanced data in the experiments and did not discuss computational issues, hence our work can be understood as an extension of bespalov et al. (2011) to the context of unbalanced datasets as well as in discussing the computational requirements of both ann and svm models to achieve comparable results.",1
"categories of features, which were refined by applying the information gain (ig) technique (yang & pedersen, 1997). authors proposed a process of extracting sentiment features, what involves identifying adjectives, adverbs and verbs, and assigning sentiment scores to them according to the sentiwordnet. recently, he, lin, and alani (2011) proposed to detect sentiment and topic simultaneously from text and show that a state-of-the-art performance can be achieved by augmenting features with polarity word labels. as discussed above, literature is focused on feature selection, which provides the input to sentiment learning algorithms. the most popular sentiment learning techniques are svm and nb, and many authors have reported better accuracy by using svm (abbasi, 2010; dang et al., 2010; o'keefe & koprinska, 2009; pang et al., 2002; prabowo & thelwall, 2009; ye, zhang, & law, 2009). another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to when considering an ig-based term selection of more than 5001000 terms. in summary, although reducing input features is critical to make the ann training process practical, it does not represent a disadvantage in comparison to svm, since svm also benefits from the feature selection (abbasi et al., 2011, 2008; dang et al., 2010; li et al., 2009), especially in the context of large-scale sentiment classification tasks (bespalov et al., 2011).",1
"the central idea of a neural network is to derive features from linear combinations of the input data, and then model the output as a nonlinear function of these features (hastie et al., 2001). the result is one of the most popular and effective forms of learning system (russell, norvig, & davis, 2010).",1
"methods, which may not converge to the optimal/global solution (hastie et al., 2001; haykin, 1998). however, some techniques have shown advances in minimizing the chance of local convergence. although the scaled conjugate gradient method (fodslette & mnller, 1993) focus on accelerating the process of convergence, the reported results showed that the method failed to converge less often than traditional conjugate gradient or back-propagation using gradient descent. for the ann classifier, we used the traditional feed-forward network with a single hidden layer. the number of neurons in the hidden layer m is selected from the set m 2 {15, . . . , 55}. in order to overcome the problem of convergence to a satisfactory solution, our exhaustive procedure of selecting parameters involved training candidate models three times, each of which started with a different set of randomly generated weights. repeated training with random starting weights is a popular method to overcome the convergence problem (atakulreka & sutivong, 2007). we used the back-propagation algorithm to train the neural network models. instead of adopting the traditional gradient descent method, we use the scaled conjugated gradient to speed up the convergence to a solution (fodslette & mnller, 1993), as implemented in the matlab software. in order to accelerate the training process and reduce the risk of overfitting, we adopted the early stopping procedure (bishop, 2007).",1
"another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to",1
"svm is a linear learning method that finds an optimal hyperplane to separate two classes. as a supervised classification approach, svm seeks to maximize the distance to the closest training point from either class in order to achieve better generalization/classication performance on test data (hastie, tibshirani, & friedman, 2001). the solution is based only on those training data points which are at the margin of the decision boundary. these points are called support vectors and are illustrated in fig. 2(a). instead of minimizing a global error function in a gradient descent process, which suffers from the existence of multiple local minima solutions, the parameters of the optimal separating hyperplane can be obtained by solving a convex optimization problem, for which there are standard software packages available.",1
"neural networks has been rarely used as an approach for sentiment learning and one of the reasons for this can be the high computational cost of training them on high-dimensional data. however, our experiments indicate that a standard feature selection method (ig) performs well in the task of refining data to be input into a neural network and consequently reducing the computational effort in the training process. our results on balanced data indicate that as the number of selected terms rise beyond 1000 terms, ann not only present a quick increase of training time (see fig. 5(a)) but also result in no significant improvements on the classification accuracy (see figs. 3 and 4), indicating that the ig method perform satisfactorily in filtering noisy terms. on text categorization, some authors (gabrilovich & markovitch, 2004; taira & haruno, 1999) have recommended a svm training process with all available features. however our experiments shown that svm have also benefited from the igbased term selection, since the running time of svm can be significantly reduced by considering less input terms (see fig. 5(b)). according to figs. 3 and 4, there seems to be no need to involve more than a fraction of the vocabulary (less than 10%), since no significant improvements on the classification accuracy are achieved",1
"we focus on comparing svm and ann learning models when they achieve the best classification accuracy. however, both models require a critical parameter setting, which is still a research issue. a detailed discussion covering parameter values and their implications are beyond the scope of our work and can be found in ben-hur and weston (2010), burges (1998), hastie et al. (2001), haykin (1998). for each set of selected features, we perform an exhaustive searching through a subset of critical parameters values. specifically, the parameter c in the svm algorithm and the number of neurons m in the hidden layer of a neural network are empirically fitted in a grid search fashion guided by better values of accuracy.",1
"the central idea of a neural network is to derive features from linear combinations of the input data, and then model the output as a nonlinear function of these features (hastie et al., 2001). the result is one of the most popular and effective forms of learning system (russell, norvig, & davis, 2010). neural networks are typically represented by a network diagram which is composed of nodes connected by directed links. nodes are arranged in layers and the structure of the most used neural network consists of three layers: an input, a hidden and an output layer of nodes (hastie et al., 2001). it is also classified as a feed-forward network, since the nodes are connected only in one direction. each connection has an associated weight, whose methods, which may not converge to the optimal/global solution (hastie et al., 2001; haykin, 1998). however, some techniques have shown advances in minimizing the chance of local convergence. although the scaled conjugate gradient method (fodslette & mnller, 1993) focus on accelerating the process of convergence, the reported results showed that the method failed to converge less often than traditional conjugate gradient or back-propagation using gradient descent.",1
"when consumers purchase products, a process of quality evaluation takes place naturally in their minds. from the industry point of view, to know the feelings among consumers may support strategic market decisions. on the other hand, potential consumers are often interested in the opinion of current customers in order to find out the choice that best fits their preferences. nowadays, many people make their opinions available on the internet and researchers have been proposing methods to automate the task of classifying these textual reviews/opinions as positive or negative (hatzivassiloglou & mckeown, 1997; pang, lee, & vaithyanathan, 2002; pang & lee, 2008; turney, 2002). the research field is know as opinion mining or sentiment classification and a complete overview on the subject is presented in liu (2011) and pang and lee (2008). basically, most of the methods in the literature are composed of two parts: (i) feature selection and (ii) sentiment learning/ classification (abbasi, france, zhang, & chen, 2011; chen, liu, & chiu, 2011; li, xia, zong, & huang, 2009; pang et al., 2002). in general, techniques in the literature can be differed in terms of the adopted approach for feature selection, since most of them agree",2
"we focus on comparing svm and ann learning models when they achieve the best classification accuracy. however, both models require a critical parameter setting, which is still a research issue. a detailed discussion covering parameter values and their implications are beyond the scope of our work and can be found in ben-hur and weston (2010), burges (1998), hastie et al. (2001), haykin (1998). for each set of selected features, we perform an exhaustive searching through a subset of critical parameters values. specifically, the parameter c in the svm algorithm and the number of neurons m in the hidden layer of a neural network are empirically fitted in a grid search fashion guided by better values of accuracy.",1
"when classes cannot be linearly separated, as shown in fig. 2(b), the input data space is transformed into a higher-dimensional feature space in order to make data linearly separable and suitable for the linear svm formulation. usually, this transformation is achieved by using a kernel function h (huang, kecman, & kopriva, 2006). it makes possible to determine a nonlinear decision boundary, which is linear in the higher-dimensional feature space, without computing the parameters of the optimal hyperplane in a feature space of possibly high dimensionality (haykin, 1998). therefore, the solution can be written as a weighted sum of the values of certain kernel function evaluated at the support vectors (horvath, 2003). value is estimated by minimizing a global error function in a gradient descent training process (haykin, 1998). usually, a neuron is a simple mathematical model that produces an output value in two steps. first, the neuron computes a weighted sum of its inputs and then applies an activation function to this sum to derive its output (russell et al., 2010). the activation function is typically a nonlinear function, and it ensures that the entire network can estimate a nonlinear function (e.g. a nonlinear decision boundary), which is learned from the input data. the bias term b is common for both svm and ann (haykin, 1998) and table 1 explains the meaning of the remaining terms from both svm and ann points of view. despite structural similarities in the output function, the models differ in the way the solutions are obtained. the number m of support vectors is usually a result of the optimization problem posed, and the support vectors fxkgm k1 are always a subset of the data in the svm algorithm. this property does not usually hold for ann (romero & toppo, 2007) and the number m of hidden nodes in a neural network is a free parameter which is usually fixed previously. therefore, in contrast to neural networks, svm algorithm has a desired property of automatically selecting their model size m (by selecting the support vectors as a fraction of the training data). however, it is important to note that a large set of support vectors can be needed to form the output function, making svm computationally slow in running time (test phase) and expensive for real-time applications (burges, 1998; romero & toppo, 2007). although the definition of an appropriate number m of hidden nodes in designing of a neural network is not a trivial task, the model complexity is usually controlled by keeping the number of hidden nodes small (haykin, 1998). this is one of the issues under investigation in our work, i.e. how is the performance of an ann in comparison with a svm model when the number of hidden neurons is a fraction of the number of support vectors in a sentiment classification task? methods, which may not converge to the optimal/global solution (hastie et al., 2001; haykin, 1998). however, some techniques have shown advances in minimizing the chance of local convergence. although the scaled conjugate gradient method (fodslette & mnller, 1993) focus on accelerating the process of convergence, the reported results showed that the method failed to converge less often than traditional conjugate gradient or back-propagation using gradient descent.",1
"in summary, our results indicate that ann can be a candidate approach when the task involves sentiment learning. future work will concentrate on three aspects. first, a comparative study between svm and ann by involving features like part-of-speech tags and joint topic-sentiment measurements. second, the maximum entropy (me) classification method (he et al., 2011) has shown promising results in sentiment analysis and therefore we intend to involve me in our comparative study. finally, our results on unbalanced data suggest a relation between data quality and the fact of ann outperforming svm. therefore, we intend to investigate the potential of a feature selection method, like ig, in predicting the best classifier to be used as a function of data quality.",1
"when classes cannot be linearly separated, as shown in fig. 2(b), the input data space is transformed into a higher-dimensional feature space in order to make data linearly separable and suitable for the linear svm formulation. usually, this transformation is achieved by using a kernel function h (huang, kecman, & kopriva, 2006). it makes possible to determine a nonlinear decision boundary, which is linear in the higher-dimensional feature space, without computing the parameters of the optimal hyperplane in a feature space of possibly high dimensionality (haykin, 1998). therefore, the solution can be written as a weighted sum of the values of certain kernel function evaluated at the support vectors (horvath, 2003).",1
"when consumers purchase products, a process of quality evaluation takes place naturally in their minds. from the industry point of view, to know the feelings among consumers may support strategic market decisions. on the other hand, potential consumers are often interested in the opinion of current customers in order to find out the choice that best fits their preferences. nowadays, many people make their opinions available on the internet and researchers have been proposing methods to automate the task of classifying these textual reviews/opinions as positive or negative (hatzivassiloglou & mckeown, 1997; pang, lee, & vaithyanathan, 2002; pang & lee, 2008; turney, 2002). the research field is know as opinion mining or sentiment classification and a complete overview on the subject is presented in liu (2011) and pang and lee (2008). basically, most of the methods in the literature are composed of two parts: (i) feature selection and (ii) sentiment learning/ classification (abbasi, france, zhang, & chen, 2011; chen, liu, & chiu, 2011; li, xia, zong, & huang, 2009; pang et al., 2002). in general, techniques in the literature can be differed in terms of the adopted approach for feature selection, since most of them agree",2
"(cid:4) information gain, a computationally cheap feature selection method, can be used to reduce the computational effort of both ann and svm without affecting significantly the resulting classification accuracy. considering an increasing number of input terms, our results have indicated the existence of thresholds above which little improvements in the resulting accuracy are achieved. our results indicated that ig (i) makes the ann training practical in a bag-of-words approach and (ii) contribute to reduce the running time of svm, although the complexity of the svm solution does not necessarily depend on the number of features (joachims, 1998; suykens, vandewalle, & moor, 2001).",1
"categories of features, which were refined by applying the information gain (ig) technique (yang & pedersen, 1997). authors proposed a process of extracting sentiment features, what involves identifying adjectives, adverbs and verbs, and assigning sentiment scores to them according to the sentiwordnet. recently, he, lin, and alani (2011) proposed to detect sentiment and topic simultaneously from text and show that a state-of-the-art performance can be achieved by augmenting features with polarity word labels. as discussed above, literature is focused on feature selection, which provides the input to sentiment learning algorithms. the most popular sentiment learning techniques are svm and nb, and many authors have reported better accuracy by using svm (abbasi, 2010; dang et al., 2010; o'keefe & koprinska, 2009; pang et al., 2002; prabowo & thelwall, 2009; ye, zhang, & law, 2009).",1
"whitelaw, garg, and argamon (2005) proposed considering adjectival expressions as an important indication of the sentiment polarity in textual reviews. zaidan, eisner, and piatko (2007) proposed learning the sentiment polarity of reviews from an additional source of information. basically, human annotators were requested to highlight the most important words and sentences that justify why a review is positive or negative. li et al. (2009) analyzed six popular feature selection methods and conclude that terms with (i) higher document frequency and (ii) higher category ratio are more informative/effective for classification. o'keefe and koprinska (2009) also evaluated feature selection techniques as well as feature weighting methods. best results were achieved using the categorical proportional difference as a feature selection metric, which is close in meaning to the category ratio discussed in li et al. (2009).",1
"when classes cannot be linearly separated, as shown in fig. 2(b), the input data space is transformed into a higher-dimensional feature space in order to make data linearly separable and suitable for the linear svm formulation. usually, this transformation is achieved by using a kernel function h (huang, kecman, & kopriva, 2006). it makes possible to determine a nonlinear decision boundary, which is linear in the higher-dimensional feature space, without computing the parameters of the optimal hyperplane in a feature space of possibly high dimensionality (haykin, 1998). therefore, the solution can be written as a weighted sum of the values of certain kernel function evaluated at the support vectors (horvath, 2003).",1
"categories of features, which were refined by applying the information gain (ig) technique (yang & pedersen, 1997). authors proposed a process of extracting sentiment features, what involves identifying adjectives, adverbs and verbs, and assigning sentiment scores to them according to the sentiwordnet. recently, he, lin, and alani (2011) proposed to detect sentiment and topic simultaneously from text and show that a state-of-the-art performance can be achieved by augmenting features with polarity word labels. as discussed above, literature is focused on feature selection, which provides the input to sentiment learning algorithms. the most popular sentiment learning techniques are svm and nb, and many authors have reported better accuracy by using svm (abbasi, 2010; dang et al., 2010; o'keefe & koprinska, 2009; pang et al., 2002; prabowo & thelwall, 2009; ye, zhang, & law, 2009).",1
"although some approaches have proposed using unsupervised/ semi-supervised learning methods (lin & he, 2009; turney, 2002), most of the work has focused on supervised learning techniques. pang et al. (2002) proposed a seminal approach in sentiment classification. essentially, they conclude that machine learning techniques, like nb and svm, do not achieve an accuracy as good on sentiment classification as on traditional topic-based categorization. in order to better approach sentiments in textual reviews, pang and lee (2004) also proposed classifying sentences as being either subjective or objective, and then apply sentiment classification on the subjective portion of the text. in addition to the probability of a term being subjective, raychev and nakov (2009) proposed considering the position of terms in the text as a strategy for identifying informative features.",1
"when consumers purchase products, a process of quality evaluation takes place naturally in their minds. from the industry point of view, to know the feelings among consumers may support strategic market decisions. on the other hand, potential consumers are often interested in the opinion of current customers in order to find out the choice that best fits their preferences. nowadays, many people make their opinions available on the internet and researchers have been proposing methods to automate the task of classifying these textual reviews/opinions as positive or negative (hatzivassiloglou & mckeown, 1997; pang, lee, & vaithyanathan, 2002; pang & lee, 2008; turney, 2002). the research field is know as opinion mining or sentiment classification and a complete overview on the subject is presented in liu (2011) and pang and lee (2008). basically, most of the methods in the literature are composed of two parts: (i) feature selection and (ii) sentiment learning/ classification (abbasi, france, zhang, & chen, 2011; chen, liu, & chiu, 2011; li, xia, zong, & huang, 2009; pang et al., 2002). in general, techniques in the literature can be differed in terms of the adopted approach for feature selection, since most of them agree",2
"another characteristic of the sentiment classification literature is that many methods have been tested only on balanced datasets and there has been little discussion on the effects of learning subjective aspects from unbalanced data, although it is typical of the product domain to have substantially more positive than negative reviews (burns, bi, wang, & anderson, 2011; li, wang, zhou, & lee, 2011a). burns et al. (2011) address sentiment classification on unbalanced data, however the experiments do not involve neither svm nor ann. li et al. (2011a, 2011b) adopt a random under-sampling method, which is a popular approach to deal with imbalanced data. the major drawback of random under-sampling is that it can discard potentially useful data that could be important for the learning process. in order to overcome this problem, wang, li, zhou, li, and zhu (2011) propose combining multiple classifiers, which are trained from multiple instances of under-sampled data. this ensemble learning approach can be computationally expensive (xia, zong, & li, 2011) and no discussion on this issue is reported by the authors. in contrast to those approaches that discuss imbalanced sentiment classification, we evaluate the performance of ann as the learning approach and report results in a context in which no previous stages are considered to deal with imbalanced data, like sampling techniques (van hulse, khoshgoftaar, & napolitano, 2007).",2
"whitelaw, garg, and argamon (2005) proposed considering adjectival expressions as an important indication of the sentiment polarity in textual reviews. zaidan, eisner, and piatko (2007) proposed learning the sentiment polarity of reviews from an additional source of information. basically, human annotators were requested to highlight the most important words and sentences that justify why a review is positive or negative. li et al. (2009) analyzed six popular feature selection methods and conclude that terms with (i) higher document frequency and (ii) higher category ratio are more informative/effective for classification. o'keefe and koprinska (2009) also evaluated feature selection techniques as well as feature weighting methods. best results were achieved using the categorical proportional difference as a feature selection metric, which is close in meaning to the category ratio discussed in li et al. (2009).",1
"next, a numerical representation is computed from textual data. binary representation is widely used and only takes into account presence or absence of a term in a document. the number of times a term occurs in a document (i.e., term frequency) is also used as a weighting scheme for textual data (li et al., 2009; paltoglou & thelwall, 2010). tf-idf (term frequency inverse document frequency) is one of the most popular representations and considers not only term frequencies in a document, but also the relevance of a term in the entire collection of documents. the classic tf-idft,d (manning et al., 2008) assigns to term t a weight in document d as another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to when considering an ig-based term selection of more than 5001000 terms. in summary, although reducing input features is critical to make the ann training process practical, it does not represent a disadvantage in comparison to svm, since svm also benefits from the feature selection (abbasi et al., 2011, 2008; dang et al., 2010; li et al., 2009), especially in the context of large-scale sentiment classification tasks (bespalov et al., 2011).",1
"another characteristic of the sentiment classification literature is that many methods have been tested only on balanced datasets and there has been little discussion on the effects of learning subjective aspects from unbalanced data, although it is typical of the product domain to have substantially more positive than negative reviews (burns, bi, wang, & anderson, 2011; li, wang, zhou, & lee, 2011a). burns et al. (2011) address sentiment classification on unbalanced data, however the experiments do not involve neither svm nor ann. li et al. (2011a, 2011b) adopt a random under-sampling method, which is a popular approach to deal with imbalanced data. the major drawback of random under-sampling is that it can discard potentially useful data that could be important for the learning process. in order to overcome this problem, wang, li, zhou, li, and zhu (2011) propose combining multiple classifiers, which are trained from multiple instances of under-sampled data. this ensemble learning approach can be computationally expensive (xia, zong, & li, 2011) and no discussion on this issue is reported by the authors. in contrast to those approaches that discuss imbalanced sentiment classification, we evaluate the performance of ann as the learning approach and report results in a context in which no previous stages are considered to deal with imbalanced data, like sampling techniques (van hulse, khoshgoftaar, & napolitano, 2007).",2
"although some approaches have proposed using unsupervised/ semi-supervised learning methods (lin & he, 2009; turney, 2002), most of the work has focused on supervised learning techniques. pang et al. (2002) proposed a seminal approach in sentiment classification. essentially, they conclude that machine learning techniques, like nb and svm, do not achieve an accuracy as good on sentiment classification as on traditional topic-based categorization. in order to better approach sentiments in textual reviews, pang and lee (2004) also proposed classifying sentences as being either subjective or objective, and then apply sentiment classification on the subjective portion of the text. in addition to the probability of a term being subjective, raychev and nakov (2009) proposed considering the position of terms in the text as a strategy for identifying informative features.",1
"when consumers purchase products, a process of quality evaluation takes place naturally in their minds. from the industry point of view, to know the feelings among consumers may support strategic market decisions. on the other hand, potential consumers are often interested in the opinion of current customers in order to find out the choice that best fits their preferences. nowadays, many people make their opinions available on the internet and researchers have been proposing methods to automate the task of classifying these textual reviews/opinions as positive or negative (hatzivassiloglou & mckeown, 1997; pang, lee, & vaithyanathan, 2002; pang & lee, 2008; turney, 2002). the research field is know as opinion mining or sentiment classification and a complete overview on the subject is presented in liu (2011) and pang and lee (2008). basically, most of the methods in the literature are composed of two parts: (i) feature selection and (ii) sentiment learning/ classification (abbasi, france, zhang, & chen, 2011; chen, liu, & chiu, 2011; li, xia, zong, & huang, 2009; pang et al., 2002). in general, techniques in the literature can be differed in terms of the adopted approach for feature selection, since most of them agree",2
"in this paper, we compare popular machine learning approaches (svm and nb) with an ann-based method in the context of document-level sentiment classification (liu, 2011). in comparison with the sentiment classification literature, the main contributions of our work are: (i) a comparison of a dominant and a computationally efficient approach (svm and nb, respectively) with an ann-based approach under the same context; (ii) a comparison involving realistic contexts in which the ratio of positive and negative reviews is unbalanced; (iii) a performance evaluation of ann on a full version of the benchmark dataset of movies reviews (pang & lee, 2004). we are primarily interested in investigating the potential of an ann-based approach for document-level sentiment classification and therefore we adopted classic supervised methods for feature selection and weighting in a traditional bag-of-words model (manning, raghavan, & schtze, 2008). support vector machines is a supervised learning technique with many desirable qualities that make it a popular algorithm. it has a solid theoretical foundation and performs classification more accurately than most other algorithms in many applications. many researchers have reported that svm is perhaps the most accurate method for text classification (liu, 2011). it is also widely used in sentiment classification (tsytsarau & palpanas, 2012).",1
"pre-processing techniques are often used to remove stopwords, which are common terms like prepositions and articles, and reduce term variations to a single representation by applying stemming procedures (weiss, indurkhya, & zhang, 2004). popular stemmer algorithms for the english language are snowball (porter, 2001), porter (porter, 1980) and lovins (lovins, 1968).",1
"in this section we review fundamental aspects of three popular supervised classifiers: naive bayes, support vector machines and artificial neural networks. instead of providing a detailed description of these approaches, we focus on reviewing concepts of svm and ann with the purpose of discussing important issues in a fundamental comparative analysis (see section 4.4). despite the low computational cost of the naive bayes technique, it has not been competitive in terms of classification accuracy when compared to svm (abbasi et al., 2008; pang et al., 2002) and therefore we only analyze the nb method comparatively in the context of our experimental results. more details on models for text classification and nb classifiers can be found in mccallum and nigam (1998), manning et al. (2008).",1
"next, a numerical representation is computed from textual data. binary representation is widely used and only takes into account presence or absence of a term in a document. the number of times a term occurs in a document (i.e., term frequency) is also used as a weighting scheme for textual data (li et al., 2009; paltoglou & thelwall, 2010). tf-idf (term frequency inverse document frequency) is one of the most popular representations and considers not only term frequencies in a document, but also the relevance of a term in the entire collection of documents. the classic tf-idft,d (manning et al., 2008) assigns to term t a weight in document d as",1
"fodslette, m., & mnller (1993). a scaled conjugate gradient algorithm for fast",1
"(cid:4) information gain, a computationally cheap feature selection method, can be used to reduce the computational effort of both ann and svm without affecting significantly the resulting classification accuracy. considering an increasing number of input terms, our results have indicated the existence of thresholds above which little improvements in the resulting accuracy are achieved. our results indicated that ig (i) makes the ann training practical in a bag-of-words approach and (ii) contribute to reduce the running time of svm, although the complexity of the svm solution does not necessarily depend on the number of features (joachims, 1998; suykens, vandewalle, & moor, 2001).",1
"although some approaches have proposed using unsupervised/ semi-supervised learning methods (lin & he, 2009; turney, 2002), most of the work has focused on supervised learning techniques. pang et al. (2002) proposed a seminal approach in sentiment classification. essentially, they conclude that machine learning techniques, like nb and svm, do not achieve an accuracy as good on sentiment classification as on traditional topic-based categorization. in order to better approach sentiments in textual reviews, pang and lee (2004) also proposed classifying sentences as being either subjective or objective, and then apply sentiment classification on the subjective portion of the text. in addition to the probability of a term being subjective, raychev and nakov (2009) proposed considering the position of terms in the text as a strategy for identifying informative features.",1
"another characteristic of the sentiment classification literature is that many methods have been tested only on balanced datasets and there has been little discussion on the effects of learning subjective aspects from unbalanced data, although it is typical of the product domain to have substantially more positive than negative reviews (burns, bi, wang, & anderson, 2011; li, wang, zhou, & lee, 2011a). burns et al. (2011) address sentiment classification on unbalanced data, however the experiments do not involve neither svm nor ann. li et al. (2011a, 2011b) adopt a random under-sampling method, which is a popular approach to deal with imbalanced data. the major drawback of random under-sampling is that it can discard potentially useful data that could be important for the learning process. in order to overcome this problem, wang, li, zhou, li, and zhu (2011) propose combining multiple classifiers, which are trained from multiple instances of under-sampled data. this ensemble learning approach can be computationally expensive (xia, zong, & li, 2011) and no discussion on this issue is reported by the authors. in contrast to those approaches that discuss imbalanced sentiment classification, we evaluate the performance of ann as the learning approach and report results in a context in which no previous stages are considered to deal with imbalanced data, like sampling techniques (van hulse, khoshgoftaar, & napolitano, 2007).",2
"in this section we review fundamental aspects of three popular supervised classifiers: naive bayes, support vector machines and artificial neural networks. instead of providing a detailed description of these approaches, we focus on reviewing concepts of svm and ann with the purpose of discussing important issues in a fundamental comparative analysis (see section 4.4). despite the low computational cost of the naive bayes technique, it has not been competitive in terms of classification accuracy when compared to svm (abbasi et al., 2008; pang et al., 2002) and therefore we only analyze the nb method comparatively in the context of our experimental results. more details on models for text classification and nb classifiers can be found in mccallum and nigam (1998), manning et al. (2008).",1
"we conducted experiments on four datasets, which are the benchmark movies review dataset (pang & lee, 2004) and collections of reviews extracted from amazon.com in distinct product domains: gps, books and cameras. each dataset consists of 2000 reviews that were classified in terms of the overall orientation as being either positive or negative (1000 positive and 1000 negative reviews). the ground truth was obtained according to the customer 5-stars rating. reviews with more than 3 stars were defined as being positive and reviews with less than 3 stars were labeled as being negative. reviews with 3 stars are not included in our datasets. table 2 characterizes the distribution of terms in the datasets after removing stopwords and stemming.",1
"fig. 5 shows the average training and running time in seconds, as a function of different number of selected terms. in order to facilitate the visual comparison between ann and svm, we did not include the nb classifier, since it is much faster than ann and svm, as shown in tables 4-7. since we conducted the validation as a 10-fold process on 2000 reviews, training time is computed by considering 1800 reviews and running time is computed on the remaining 200 reviews. as discussed in section 4.4, the computational cost of the svm and ann algorithms is connected with the number m of support vectors and neurons in the hidden layer respectively. table 8 shows the requirements of svm and ann in terms of these variables to achieved the accuracy values reported in our experiments.",1
"next, a numerical representation is computed from textual data. binary representation is widely used and only takes into account presence or absence of a term in a document. the number of times a term occurs in a document (i.e., term frequency) is also used as a weighting scheme for textual data (li et al., 2009; paltoglou & thelwall, 2010). tf-idf (term frequency inverse document frequency) is one of the most popular representations and considers not only term frequencies in a document, but also the relevance of a term in the entire collection of documents. the classic tf-idft,d (manning et al., 2008) assigns to term t a weight in document d as",1
"in this paper, we compare popular machine learning approaches (svm and nb) with an ann-based method in the context of document-level sentiment classification (liu, 2011). in comparison with the sentiment classification literature, the main contributions of our work are: (i) a comparison of a dominant and a computationally efficient approach (svm and nb, respectively) with an ann-based approach under the same context; (ii) a comparison involving realistic contexts in which the ratio of positive and negative reviews is unbalanced; (iii) a performance evaluation of ann on a full version of the benchmark dataset of movies reviews (pang & lee, 2004). we are primarily interested in investigating the potential of an ann-based approach for document-level sentiment classification and therefore we adopted classic supervised methods for feature selection and weighting in a traditional bag-of-words model (manning, raghavan, & schtze, 2008). many researchers have been addressing the problem of sentiment classification on textual reviews. some datasets are available and have been used by many researchers in order to compare results, and the dataset of movie reviews (pang & lee, 2004) is the most popular benchmark dataset in the literature.1 since the focus of our study is on the overall opinion (positive or negative) expressed in a review, we have oriented our literature review towards document-level sentiment classification, which assumes that a review document expresses opinions on a single product or service and was written by a single reviewer/customer. we conducted experiments on four datasets, which are the benchmark movies review dataset (pang & lee, 2004) and collections of reviews extracted from amazon.com in distinct product domains: gps, books and cameras. each dataset consists of 2000 reviews that were classified in terms of the overall orientation as being either positive or negative (1000 positive and 1000 negative reviews). the ground truth was obtained according to the customer 5-stars rating. reviews with more than 3 stars were defined as being positive and reviews with less than 3 stars were labeled as being negative. reviews with 3 stars are not included in our datasets. table 2 characterizes the distribution of terms in the datasets after removing stopwords and stemming. (cid:4) in terms of classification accuracy on the benchmark dataset of movies reviews (pang & lee, 2004), ann outperformed significantly (statistically) svm, specially in the context of unbalanced data.",2
"when consumers purchase products, a process of quality evaluation takes place naturally in their minds. from the industry point of view, to know the feelings among consumers may support strategic market decisions. on the other hand, potential consumers are often interested in the opinion of current customers in order to find out the choice that best fits their preferences. nowadays, many people make their opinions available on the internet and researchers have been proposing methods to automate the task of classifying these textual reviews/opinions as positive or negative (hatzivassiloglou & mckeown, 1997; pang, lee, & vaithyanathan, 2002; pang & lee, 2008; turney, 2002). the research field is know as opinion mining or sentiment classification and a complete overview on the subject is presented in liu (2011) and pang and lee (2008). basically, most of the methods in the literature are composed of two parts: (i) feature selection and (ii) sentiment learning/ classification (abbasi, france, zhang, & chen, 2011; chen, liu, & chiu, 2011; li, xia, zong, & huang, 2009; pang et al., 2002). in general, techniques in the literature can be differed in terms of the adopted approach for feature selection, since most of them agree ann has figured rarely in the literature (bespalov, bai, qi, & shokoufandeh, 2011; chen et al., 2011; claster, hung, & shanmuganathan, 2010; zhu, xu, & shi wang, 2010), as can be seen in recent surveys on sentiment analysis (pang & lee, 2008; tsytsarau & palpanas, 2012). in order to reduce the training time, chen et al. (2011) propose combining word features to model an ann-based approach with few input neurons. however, the experiments do not involve unbalanced datasets as well as a comparison with popular sentiment learning techniques like nb and svm. zhu et al. (2010) propose simulating the human's judgment on sentiment polarity by using an ann-based individual model. authors report a comparative analysis between the proposed ann-based method and svm, however a performance evaluation on unbalanced datasets is not discussed. instead of classifying positive versus negative reviews, claster et al. (2010) propose using selforganizing maps (som) to cluster microblog posts according to subjective aspects in movies domain, like ''funny'' and ''predictable''. perhaps the most conclusive experiments that compare variants of an ann-based method with svm-based approaches for sentiment learning are reported in bespalov et al. (2011). however, authors considered only balanced data in the experiments and did not discuss computational issues, hence our work can be understood as an extension of bespalov et al. (2011) to the context of unbalanced datasets as well as in discussing the computational requirements of both ann and svm models to achieve comparable results.",0
"although some approaches have proposed using unsupervised/ semi-supervised learning methods (lin & he, 2009; turney, 2002), most of the work has focused on supervised learning techniques. pang et al. (2002) proposed a seminal approach in sentiment classification. essentially, they conclude that machine learning techniques, like nb and svm, do not achieve an accuracy as good on sentiment classification as on traditional topic-based categorization. in order to better approach sentiments in textual reviews, pang and lee (2004) also proposed classifying sentences as being either subjective or objective, and then apply sentiment classification on the subjective portion of the text. in addition to the probability of a term being subjective, raychev and nakov (2009) proposed considering the position of terms in the text as a strategy for identifying informative features.",1
"when consumers purchase products, a process of quality evaluation takes place naturally in their minds. from the industry point of view, to know the feelings among consumers may support strategic market decisions. on the other hand, potential consumers are often interested in the opinion of current customers in order to find out the choice that best fits their preferences. nowadays, many people make their opinions available on the internet and researchers have been proposing methods to automate the task of classifying these textual reviews/opinions as positive or negative (hatzivassiloglou & mckeown, 1997; pang, lee, & vaithyanathan, 2002; pang & lee, 2008; turney, 2002). the research field is know as opinion mining or sentiment classification and a complete overview on the subject is presented in liu (2011) and pang and lee (2008). basically, most of the methods in the literature are composed of two parts: (i) feature selection and (ii) sentiment learning/ classification (abbasi, france, zhang, & chen, 2011; chen, liu, & chiu, 2011; li, xia, zong, & huang, 2009; pang et al., 2002). in general, techniques in the literature can be differed in terms of the adopted approach for feature selection, since most of them agree categories of features, which were refined by applying the information gain (ig) technique (yang & pedersen, 1997). authors proposed a process of extracting sentiment features, what involves identifying adjectives, adverbs and verbs, and assigning sentiment scores to them according to the sentiwordnet. recently, he, lin, and alani (2011) proposed to detect sentiment and topic simultaneously from text and show that a state-of-the-art performance can be achieved by augmenting features with polarity word labels. as discussed above, literature is focused on feature selection, which provides the input to sentiment learning algorithms. the most popular sentiment learning techniques are svm and nb, and many authors have reported better accuracy by using svm (abbasi, 2010; dang et al., 2010; o'keefe & koprinska, 2009; pang et al., 2002; prabowo & thelwall, 2009; ye, zhang, & law, 2009). another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to in this section we review fundamental aspects of three popular supervised classifiers: naive bayes, support vector machines and artificial neural networks. instead of providing a detailed description of these approaches, we focus on reviewing concepts of svm and ann with the purpose of discussing important issues in a fundamental comparative analysis (see section 4.4). despite the low computational cost of the naive bayes technique, it has not been competitive in terms of classification accuracy when compared to svm (abbasi et al., 2008; pang et al., 2002) and therefore we only analyze the nb method comparatively in the context of our experimental results. more details on models for text classification and nb classifiers can be found in mccallum and nigam (1998), manning et al. (2008).",1
"whitelaw, garg, and argamon (2005) proposed considering adjectival expressions as an important indication of the sentiment polarity in textual reviews. zaidan, eisner, and piatko (2007) proposed learning the sentiment polarity of reviews from an additional source of information. basically, human annotators were requested to highlight the most important words and sentences that justify why a review is positive or negative. li et al. (2009) analyzed six popular feature selection methods and conclude that terms with (i) higher document frequency and (ii) higher category ratio are more informative/effective for classification. o'keefe and koprinska (2009) also evaluated feature selection techniques as well as feature weighting methods. best results were achieved using the categorical proportional difference as a feature selection metric, which is close in meaning to the category ratio discussed in li et al. (2009).",1
"pre-processing techniques are often used to remove stopwords, which are common terms like prepositions and articles, and reduce term variations to a single representation by applying stemming procedures (weiss, indurkhya, & zhang, 2004). popular stemmer algorithms for the english language are snowball (porter, 2001), porter (porter, 1980) and lovins (lovins, 1968).",1
"pre-processing techniques are often used to remove stopwords, which are common terms like prepositions and articles, and reduce term variations to a single representation by applying stemming procedures (weiss, indurkhya, & zhang, 2004). popular stemmer algorithms for the english language are snowball (porter, 2001), porter (porter, 1980) and lovins (lovins, 1968). following most of the experimental reports in the literature, our results are obtained in terms of 10-fold cross-validation. the preprocessing of the datasets consisted of removing stopwords and stemming by applying the snowball stemmer (porter, 2001), resulting in the vocabulary sizes described in table 2. we used single words as features and tf-idf as the weighting approach. for each set of training data, we refine features by using the information gain ranking (ig), as discussed in section 3, and then evaluate the performance of the learning methods as a function of different sets of selected features.",1
"categories of features, which were refined by applying the information gain (ig) technique (yang & pedersen, 1997). authors proposed a process of extracting sentiment features, what involves identifying adjectives, adverbs and verbs, and assigning sentiment scores to them according to the sentiwordnet. recently, he, lin, and alani (2011) proposed to detect sentiment and topic simultaneously from text and show that a state-of-the-art performance can be achieved by augmenting features with polarity word labels. as discussed above, literature is focused on feature selection, which provides the input to sentiment learning algorithms. the most popular sentiment learning techniques are svm and nb, and many authors have reported better accuracy by using svm (abbasi, 2010; dang et al., 2010; o'keefe & koprinska, 2009; pang et al., 2002; prabowo & thelwall, 2009; ye, zhang, & law, 2009).",1
subjectivity and positional conference ranlp-2009 (pp. 360-364).,2
"formally, svm and feed-forward neural networks are structurally similar, since both of them induce an output function which is expressed as a linear combination of simple functions (romero & toppo, 2007): despite structural similarities in the output function, the models differ in the way the solutions are obtained. the number m of support vectors is usually a result of the optimization problem posed, and the support vectors fxkgm k1 are always a subset of the data in the svm algorithm. this property does not usually hold for ann (romero & toppo, 2007) and the number m of hidden nodes in a neural network is a free parameter which is usually fixed previously. therefore, in contrast to neural networks, svm algorithm has a desired property of automatically selecting their model size m (by selecting the support vectors as a fraction of the training data). however, it is important to note that a large set of support vectors can be needed to form the output function, making svm computationally slow in running time (test phase) and expensive for real-time applications (burges, 1998; romero & toppo, 2007). although the definition of an appropriate number m of hidden nodes in designing of a neural network is not a trivial task, the model complexity is usually controlled by keeping the number of hidden nodes small (haykin, 1998). this is one of the issues under investigation in our work, i.e. how is the performance of an ann in comparison with a svm model when the number of hidden neurons is a fraction of the number of support vectors in a sentiment classification task?",1
"value is estimated by minimizing a global error function in a gradient descent training process (haykin, 1998). usually, a neuron is a simple mathematical model that produces an output value in two steps. first, the neuron computes a weighted sum of its inputs and then applies an activation function to this sum to derive its output (russell et al., 2010). the activation function is typically a nonlinear function, and it ensures that the entire network can estimate a nonlinear function (e.g. a nonlinear decision boundary), which is learned from the input data.",1
"on the learning techniques: support vector machine (svm) and naive bayes (nb) (abbasi, chen, & salem, 2008; tang, tan, & cheng, 2009; tsytsarau & palpanas, 2012). as discussed in section 2, artificial neural networks (ann) has attracted little attention as an approach for sentiment learning and a comparative study of ann and popular sentiment classifiers under the same framework is still missing.",1
"in this paper, we compare popular machine learning approaches (svm and nb) with an ann-based method in the context of document-level sentiment classification (liu, 2011). in comparison with the sentiment classification literature, the main contributions of our work are: (i) a comparison of a dominant and a computationally efficient approach (svm and nb, respectively) with an ann-based approach under the same context; (ii) a comparison involving realistic contexts in which the ratio of positive and negative reviews is unbalanced; (iii) a performance evaluation of ann on a full version of the benchmark dataset of movies reviews (pang & lee, 2004). we are primarily interested in investigating the potential of an ann-based approach for document-level sentiment classification and therefore we adopted classic supervised methods for feature selection and weighting in a traditional bag-of-words model (manning, raghavan, & schtze, 2008).",1
"ann has figured rarely in the literature (bespalov, bai, qi, & shokoufandeh, 2011; chen et al., 2011; claster, hung, & shanmuganathan, 2010; zhu, xu, & shi wang, 2010), as can be seen in recent surveys on sentiment analysis (pang & lee, 2008; tsytsarau & palpanas, 2012). in order to reduce the training time, chen et al. (2011) propose combining word features to model an ann-based approach with few input neurons. however, the experiments do not involve unbalanced datasets as well as a comparison with popular sentiment learning techniques like nb and svm. zhu et al. (2010) propose simulating the human's judgment on sentiment polarity by using an ann-based individual model. authors report a comparative analysis between the proposed ann-based method and svm, however a performance evaluation on unbalanced datasets is not discussed. instead of classifying positive versus negative reviews, claster et al. (2010) propose using selforganizing maps (som) to cluster microblog posts according to subjective aspects in movies domain, like ''funny'' and ''predictable''. perhaps the most conclusive experiments that compare variants of an ann-based method with svm-based approaches for sentiment learning are reported in bespalov et al. (2011). however, authors considered only balanced data in the experiments and did not discuss computational issues, hence our work can be understood as an extension of bespalov et al. (2011) to the context of unbalanced datasets as well as in discussing the computational requirements of both ann and svm models to achieve comparable results.",1
"although we have reported results only with an usual nonlinear kernel, our experiments indicate that svm requires a high number of support vectors to classify reviews as expressing positive or negative opinions at document level (see table 8). therefore, it is reasonable to conclude that the classes cannot be well (and linearly) separated on the basis of single terms as the dimensions of the input space, resulting in a svm's running time much higher than that of an ann. thus, although cristianini and shawe-taylor (2000) mentioned that in practice svm frequently results in very few support vectors, our results are consistent with previous results on text categorization (colas, pacli' k, kok, & brazdil, 2007) in reporting a high number of support vectors.",1
"ann has figured rarely in the literature (bespalov, bai, qi, & shokoufandeh, 2011; chen et al., 2011; claster, hung, & shanmuganathan, 2010; zhu, xu, & shi wang, 2010), as can be seen in recent surveys on sentiment analysis (pang & lee, 2008; tsytsarau & palpanas, 2012). in order to reduce the training time, chen et al. (2011) propose combining word features to model an ann-based approach with few input neurons. however, the experiments do not involve unbalanced datasets as well as a comparison with popular sentiment learning techniques like nb and svm. zhu et al. (2010) propose simulating the human's judgment on sentiment polarity by using an ann-based individual model. authors report a comparative analysis between the proposed ann-based method and svm, however a performance evaluation on unbalanced datasets is not discussed. instead of classifying positive versus negative reviews, claster et al. (2010) propose using selforganizing maps (som) to cluster microblog posts according to subjective aspects in movies domain, like ''funny'' and ''predictable''. perhaps the most conclusive experiments that compare variants of an ann-based method with svm-based approaches for sentiment learning are reported in bespalov et al. (2011). however, authors considered only balanced data in the experiments and did not discuss computational issues, hence our work can be understood as an extension of bespalov et al. (2011) to the context of unbalanced datasets as well as in discussing the computational requirements of both ann and svm models to achieve comparable results.",1
"in order to evaluate ann in terms of the convergence to a satisfactory solution, we adopted the strategy of training a neural network more than once. our results come from an ann model that result in the best accuracy among three competing models, which consist of the same number of nodes in the hidden layer but were trained with different starting weights. in practice, the requirement of training a neural network more than once is a disadvantage in comparison with svm, since the svm's optimization method always converge to a unique solution. nevertheless, each ann training process could run simultaneously in parallel as proposed in atakulreka and sutivong (2007). in addition, if the running time is an issue instead of the training time, a common scenario in practice, to use an ann model to classify new data can be a good choice, since the ann classification process (i.e. an ann model at running time) can be much faster than the svm classification process (see fig. 5(b)).",1
"neural networks has been rarely used as an approach for sentiment learning and one of the reasons for this can be the high computational cost of training them on high-dimensional data. however, our experiments indicate that a standard feature selection method (ig) performs well in the task of refining data to be input into a neural network and consequently reducing the computational effort in the training process. our results on balanced data indicate that as the number of selected terms rise beyond 1000 terms, ann not only present a quick increase of training time (see fig. 5(a)) but also result in no significant improvements on the classification accuracy (see figs. 3 and 4), indicating that the ig method perform satisfactorily in filtering noisy terms. on text categorization, some authors (gabrilovich & markovitch, 2004; taira & haruno, 1999) have recommended a svm training process with all available features. however our experiments shown that svm have also benefited from the igbased term selection, since the running time of svm can be significantly reduced by considering less input terms (see fig. 5(b)). according to figs. 3 and 4, there seems to be no need to involve more than a fraction of the vocabulary (less than 10%), since no significant improvements on the classification accuracy are achieved",1
"pang, b., lee, l., & vaithyanathan, s. (2002). thumbs up? sentiment classification using machine learning techniques. in proceedings of the 2002 conference on empirical methods in natural language processing (pp. 79-86).",2
"on the learning techniques: support vector machine (svm) and naive bayes (nb) (abbasi, chen, & salem, 2008; tang, tan, & cheng, 2009; tsytsarau & palpanas, 2012). as discussed in section 2, artificial neural networks (ann) has attracted little attention as an approach for sentiment learning and a comparative study of ann and popular sentiment classifiers under the same framework is still missing. ann has figured rarely in the literature (bespalov, bai, qi, & shokoufandeh, 2011; chen et al., 2011; claster, hung, & shanmuganathan, 2010; zhu, xu, & shi wang, 2010), as can be seen in recent surveys on sentiment analysis (pang & lee, 2008; tsytsarau & palpanas, 2012). in order to reduce the training time, chen et al. (2011) propose combining word features to model an ann-based approach with few input neurons. however, the experiments do not involve unbalanced datasets as well as a comparison with popular sentiment learning techniques like nb and svm. zhu et al. (2010) propose simulating the human's judgment on sentiment polarity by using an ann-based individual model. authors report a comparative analysis between the proposed ann-based method and svm, however a performance evaluation on unbalanced datasets is not discussed. instead of classifying positive versus negative reviews, claster et al. (2010) propose using selforganizing maps (som) to cluster microblog posts according to subjective aspects in movies domain, like ''funny'' and ''predictable''. perhaps the most conclusive experiments that compare variants of an ann-based method with svm-based approaches for sentiment learning are reported in bespalov et al. (2011). however, authors considered only balanced data in the experiments and did not discuss computational issues, hence our work can be understood as an extension of bespalov et al. (2011) to the context of unbalanced datasets as well as in discussing the computational requirements of both ann and svm models to achieve comparable results. support vector machines is a supervised learning technique with many desirable qualities that make it a popular algorithm. it has a solid theoretical foundation and performs classification more accurately than most other algorithms in many applications. many researchers have reported that svm is perhaps the most accurate method for text classification (liu, 2011). it is also widely used in sentiment classification (tsytsarau & palpanas, 2012).",1
"when consumers purchase products, a process of quality evaluation takes place naturally in their minds. from the industry point of view, to know the feelings among consumers may support strategic market decisions. on the other hand, potential consumers are often interested in the opinion of current customers in order to find out the choice that best fits their preferences. nowadays, many people make their opinions available on the internet and researchers have been proposing methods to automate the task of classifying these textual reviews/opinions as positive or negative (hatzivassiloglou & mckeown, 1997; pang, lee, & vaithyanathan, 2002; pang & lee, 2008; turney, 2002). the research field is know as opinion mining or sentiment classification and a complete overview on the subject is presented in liu (2011) and pang and lee (2008). basically, most of the methods in the literature are composed of two parts: (i) feature selection and (ii) sentiment learning/ classification (abbasi, france, zhang, & chen, 2011; chen, liu, & chiu, 2011; li, xia, zong, & huang, 2009; pang et al., 2002). in general, techniques in the literature can be differed in terms of the adopted approach for feature selection, since most of them agree although some approaches have proposed using unsupervised/ semi-supervised learning methods (lin & he, 2009; turney, 2002), most of the work has focused on supervised learning techniques. pang et al. (2002) proposed a seminal approach in sentiment classification. essentially, they conclude that machine learning techniques, like nb and svm, do not achieve an accuracy as good on sentiment classification as on traditional topic-based categorization. in order to better approach sentiments in textual reviews, pang and lee (2004) also proposed classifying sentences as being either subjective or objective, and then apply sentiment classification on the subjective portion of the text. in addition to the probability of a term being subjective, raychev and nakov (2009) proposed considering the position of terms in the text as a strategy for identifying informative features. another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to",1
"when consumers purchase products, a process of quality evaluation takes place naturally in their minds. from the industry point of view, to know the feelings among consumers may support strategic market decisions. on the other hand, potential consumers are often interested in the opinion of current customers in order to find out the choice that best fits their preferences. nowadays, many people make their opinions available on the internet and researchers have been proposing methods to automate the task of classifying these textual reviews/opinions as positive or negative (hatzivassiloglou & mckeown, 1997; pang, lee, & vaithyanathan, 2002; pang & lee, 2008; turney, 2002). the research field is know as opinion mining or sentiment classification and a complete overview on the subject is presented in liu (2011) and pang and lee (2008). basically, most of the methods in the literature are composed of two parts: (i) feature selection and (ii) sentiment learning/ classification (abbasi, france, zhang, & chen, 2011; chen, liu, & chiu, 2011; li, xia, zong, & huang, 2009; pang et al., 2002). in general, techniques in the literature can be differed in terms of the adopted approach for feature selection, since most of them agree",2
"ann has figured rarely in the literature (bespalov, bai, qi, & shokoufandeh, 2011; chen et al., 2011; claster, hung, & shanmuganathan, 2010; zhu, xu, & shi wang, 2010), as can be seen in recent surveys on sentiment analysis (pang & lee, 2008; tsytsarau & palpanas, 2012). in order to reduce the training time, chen et al. (2011) propose combining word features to model an ann-based approach with few input neurons. however, the experiments do not involve unbalanced datasets as well as a comparison with popular sentiment learning techniques like nb and svm. zhu et al. (2010) propose simulating the human's judgment on sentiment polarity by using an ann-based individual model. authors report a comparative analysis between the proposed ann-based method and svm, however a performance evaluation on unbalanced datasets is not discussed. instead of classifying positive versus negative reviews, claster et al. (2010) propose using selforganizing maps (som) to cluster microblog posts according to subjective aspects in movies domain, like ''funny'' and ''predictable''. perhaps the most conclusive experiments that compare variants of an ann-based method with svm-based approaches for sentiment learning are reported in bespalov et al. (2011). however, authors considered only balanced data in the experiments and did not discuss computational issues, hence our work can be understood as an extension of bespalov et al. (2011) to the context of unbalanced datasets as well as in discussing the computational requirements of both ann and svm models to achieve comparable results.",1
"we focus on comparing svm and ann learning models when they achieve the best classification accuracy. however, both models require a critical parameter setting, which is still a research issue. a detailed discussion covering parameter values and their implications are beyond the scope of our work and can be found in ben-hur and weston (2010), burges (1998), hastie et al. (2001), haykin (1998). for each set of selected features, we perform an exhaustive searching through a subset of critical parameters values. specifically, the parameter c in the svm algorithm and the number of neurons m in the hidden layer of a neural network are empirically fitted in a grid search fashion guided by better values of accuracy.",1
"another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to",1
"another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to",1
"categories of features, which were refined by applying the information gain (ig) technique (yang & pedersen, 1997). authors proposed a process of extracting sentiment features, what involves identifying adjectives, adverbs and verbs, and assigning sentiment scores to them according to the sentiwordnet. recently, he, lin, and alani (2011) proposed to detect sentiment and topic simultaneously from text and show that a state-of-the-art performance can be achieved by augmenting features with polarity word labels. as discussed above, literature is focused on feature selection, which provides the input to sentiment learning algorithms. the most popular sentiment learning techniques are svm and nb, and many authors have reported better accuracy by using svm (abbasi, 2010; dang et al., 2010; o'keefe & koprinska, 2009; pang et al., 2002; prabowo & thelwall, 2009; ye, zhang, & law, 2009). another stage commonly found in sentiment classification approaches is feature selection. it can make classifiers more efficient/effective by reducing the amount of data to be analyzed as well as identifying relevant features to be considered in the learning process. usual feature selection methods are document frequency (bai, 2011; dang et al., 2010; pang et al., 2002), mutual information (li et al., 2009; turney, 2002), information gain (abbasi et al., 2011, 2008; li et al., 2009; riloff, patwardhan, & wiebe, 2006) and chi-square (abbasi et al., 2011; li et al., 2009). none of them has been widely accepted as the best feature selection method for sentiment classification or text categorization, however, information gain has often been competitive (abbasi et al., 2011; forman, 2003; li et al., 2009; xia & zong, 2010; yang & pedersen, 1997). it ranks terms by considering their presence and absence in each class (berry & kogan, 2010). a high score is assigned to",1
"pre-processing techniques are often used to remove stopwords, which are common terms like prepositions and articles, and reduce term variations to a single representation by applying stemming procedures (weiss, indurkhya, & zhang, 2004). popular stemmer algorithms for the english language are snowball (porter, 2001), porter (porter, 1980) and lovins (lovins, 1968).",1
"terms that occur frequently in a class (and rarely in the others) as follows (weiss, indurkhya, & zhang, 2010):",1
"another characteristic of the sentiment classification literature is that many methods have been tested only on balanced datasets and there has been little discussion on the effects of learning subjective aspects from unbalanced data, although it is typical of the product domain to have substantially more positive than negative reviews (burns, bi, wang, & anderson, 2011; li, wang, zhou, & lee, 2011a). burns et al. (2011) address sentiment classification on unbalanced data, however the experiments do not involve neither svm nor ann. li et al. (2011a, 2011b) adopt a random under-sampling method, which is a popular approach to deal with imbalanced data. the major drawback of random under-sampling is that it can discard potentially useful data that could be important for the learning process. in order to overcome this problem, wang, li, zhou, li, and zhu (2011) propose combining multiple classifiers, which are trained from multiple instances of under-sampled data. this ensemble learning approach can be computationally expensive (xia, zong, & li, 2011) and no discussion on this issue is reported by the authors. in contrast to those approaches that discuss imbalanced sentiment classification, we evaluate the performance of ann as the learning approach and report results in a context in which no previous stages are considered to deal with imbalanced data, like sampling techniques (van hulse, khoshgoftaar, & napolitano, 2007).",2
"ann has figured rarely in the literature (bespalov, bai, qi, & shokoufandeh, 2011; chen et al., 2011; claster, hung, & shanmuganathan, 2010; zhu, xu, & shi wang, 2010), as can be seen in recent surveys on sentiment analysis (pang & lee, 2008; tsytsarau & palpanas, 2012). in order to reduce the training time, chen et al. (2011) propose combining word features to model an ann-based approach with few input neurons. however, the experiments do not involve unbalanced datasets as well as a comparison with popular sentiment learning techniques like nb and svm. zhu et al. (2010) propose simulating the human's judgment on sentiment polarity by using an ann-based individual model. authors report a comparative analysis between the proposed ann-based method and svm, however a performance evaluation on unbalanced datasets is not discussed. instead of classifying positive versus negative reviews, claster et al. (2010) propose using selforganizing maps (som) to cluster microblog posts according to subjective aspects in movies domain, like ''funny'' and ''predictable''. perhaps the most conclusive experiments that compare variants of an ann-based method with svm-based approaches for sentiment learning are reported in bespalov et al. (2011). however, authors considered only balanced data in the experiments and did not discuss computational issues, hence our work can be understood as an extension of bespalov et al. (2011) to the context of unbalanced datasets as well as in discussing the computational requirements of both ann and svm models to achieve comparable results.",1
"hwa. 2004. just how mad are you? finding strong and weak opinion clauses. in aaai 2004, pages 761-767, san jose, ca.",1
"stars: exploiting class relationships for sentiment categorization with respect to rating scales. in proceedings of acl 2005, pages 115-124, ann arbor, mi.",2
"goldberg, andrew b. and xiaojin zhu. 2006. seeing stars when there aren't many stars: graph-based semi-supervised learning for sentiment categorization. in proceedings of hlt-naacl 2006 workshop on textgraphs: graph-based algorithms for natural language processing, pages 45-52, new york. greenberg, joseph h. 1966. language ng, vincent, sajib dasgupta, and s. m. niaz arifin. 2006. examining the role of linguistic knowledge sources in the identification and classification of reviews. in proceedings of the coling/ acl 2006 main conference poster sessions, pages 611-618, sydney.",2
"ours is not the only method that uses linguistic information or dictionaries. many other systems make use of either the subjectivity dictionary of wiebe and colleagues, or of sentiwordnet (devitt and ahmad 2007; thet et al. 2009), and some work relies on appraisal theory (whitelaw, garg, and argamon 2005; bloom, garg, and argamon 2007), a theory developed by martin and white (2005). we also discuss, in section 2.4, work on incorporating linguistic insights for the treatment of negation (moilanen and pulman 2007; choi and cardie 2008).",1
"jurafsky, and andrew y. ng. 2008. cheap and fast--but is it good? evaluating non-expert annotations for natural language tasks. in proceedings of the conference on empirical methods in nlp (emnlp), pages 254-263, waikiki, hi. sokolova, marina and guy lapalme. 2008. verbs speak loud: verb categories in learning polarity and strength of opinions. in sabine bergler, editor, canadian ai 2008. springer, berlin, pages 320-331. sokolova, marina and guy lapalme. 2009a.",0
"to collect data on the validity of our dictionary, we made use of amazon's mechanical turk service,24 which provides access to a pool of workers who have signed up to perform small-scale tasks that require human intelligence. mechanical turk is quickly becoming a popular resource among computational linguists, and has been used in sentiment, emotion, and subjectivity tasks (akkaya et al. 2010; mellebeek et al. 2010; mohammad and turney 2010; yano, resnik, and smith 2010), although there are mixed reports on its reliability (snow et al. 2008; callison-burch 2009; zaenen to appear).",1
"the majority of the statistical text classification research builds support vector machine classifiers, trained on a particular data set using features such as unigrams or bigrams, and with or without part-of-speech labels, although the most successful features seem to be basic unigrams (pang, lee, and vaithyanathan 2002; salvetti, reichenbach, and lewis 2006). classifiers built using supervised methods reach quite a high accuracy in detecting the polarity of a text (chaovalit and zhou 2005; kennedy and inkpen 2006; boiy et al. 2007; bartlett and albright 2008). however, although such classifiers perform very well in the domain that they are trained on, their performance drops precipitously (almost to chance) when the same classifier is used in",1
"2 with some exceptions: turney (2002) uses two-word phrases; whitelaw, garg, and argamon (2005) the breakdown for the polarity dataset (movies in table 5) is 89.37% precision for negative reviews and 63.2% for positive ones, with an overall accuracy of 76.37%. a number of other papers have used the polarity dataset created by bo pang, most of them following statistical methods. pang and lee's own results show an overall accuracy of 87.15% for polarity classification of whole reviews (pang and lee 2004). fletcher and patrick (2005) used bags-of-words that included appraisal features, and obtained 83.7% accuracy in that same data set, whereas whitelaw, garg, and argamon (2005), using bags-of-words combined with appraisal groups achieved 90.2%. in all cases, the accuracy reflects a single domain and data set. andreevskaia and bergler (2008) show, however, that cross-domain performance drops significantly. they used a hybrid",1
"a 2,400-text corpus of camera, printer, and stroller reviews, taken from a larger set of epinions reviews; also used in bloom, garg, and argamon (2007).",1
"much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim ours is not the only method that uses linguistic information or dictionaries. many other systems make use of either the subjectivity dictionary of wiebe and colleagues, or of sentiwordnet (devitt and ahmad 2007; thet et al. 2009), and some work relies on appraisal theory (whitelaw, garg, and argamon 2005; bloom, garg, and argamon 2007), a theory developed by martin and white (2005). we also discuss, in section 2.4, work on incorporating linguistic insights for the treatment of negation (moilanen and pulman 2007; choi and cardie 2008).",1
"ours is not the only method that uses linguistic information or dictionaries. many other systems make use of either the subjectivity dictionary of wiebe and colleagues, or of sentiwordnet (devitt and ahmad 2007; thet et al. 2009), and some work relies on appraisal theory (whitelaw, garg, and argamon 2005; bloom, garg, and argamon 2007), a theory developed by martin and white (2005). we also discuss, in section 2.4, work on incorporating linguistic insights for the treatment of negation (moilanen and pulman 2007; choi and cardie 2008).",1
"approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively.",1
submission received: 14 december 2009; revised submission received: 22 august 2010; accepted for publication: 28 september 2010.,1
"the sentence level, exploring the types of syntactic patterns that indicate subjectivity and sentiment is also a possibility (greene and resnik 2009). syntactic patterns can also be used to distinguish different types of opinion and appraisal (bednarek 2009).",1
"adjective phrases; and benamara et al. (2007) adjectives with adverbial modifiers. see also section 2.1. we should also point out that turney does not create a static dictionary, but rather scores two-word phrases on the fly.",1
"much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim",1
"a distinction between sentences with a first person subject and with other subjects. other hybrid methods include those of andreevskaia and bergler (2008), dang, zhang, and chen (2010), dasgupta and ng (2009), goldberg and zhu (2006), or prabowo and thelwall (2009). wan (2009) uses co-training in a method that uses english labeled data and an english classifier to learn a classifier for chinese. the breakdown for the polarity dataset (movies in table 5) is 89.37% precision for negative reviews and 63.2% for positive ones, with an overall accuracy of 76.37%. a number of other papers have used the polarity dataset created by bo pang, most of them following statistical methods. pang and lee's own results show an overall accuracy of 87.15% for polarity classification of whole reviews (pang and lee 2004). fletcher and patrick (2005) used bags-of-words that included appraisal features, and obtained 83.7% accuracy in that same data set, whereas whitelaw, garg, and argamon (2005), using bags-of-words combined with appraisal groups achieved 90.2%. in all cases, the accuracy reflects a single domain and data set. andreevskaia and bergler (2008) show, however, that cross-domain performance drops significantly. they used a hybrid table 7 shows the results of the evaluation. included in the table is a baseline for each data set, assigning polarity to the most frequent class for the data. these data sets include much smaller spans of text than are found in consumer reviews, with some sentences or headlines not containing any words from the so-cal dictionaries. this ranged from about 21% of the total in the myspace comments to 54% in the headlines.21 two approaches were used in this cross-domain evaluation when so-cal encountered texts for which it found no words in its dictionaries (so-empty texts). first, the backoff method involves using the most frequent polarity for the corpus (or positive, when they are equal), and assigning that polarity to all so-empty texts. this method provides results that can be directly compared to other results on these data sets, although, like the baseline, it assumes some knowledge about the polarity balance of the corpus. the figures in the first section of table 7 suggest robust performance as compared to a mostfrequent-class baseline, including modest improvement over the relevant cross-domain results of andreevskaia and bergler (2008).22 moilanen, pulman, and zhang (2010) also use the headlines data, and obtain a polarity classification accuracy of 77.94% below our results excluding empty.23",1
"in this article, starting in section 2, we describe the semantic orientation calculator (so-cal) that we have developed over the last few years. we first extract sentiment-bearing words (including adjectives, verbs, nouns, and adverbs), and use them to calculate semantic orientation, taking into account valence shifters (intensifiers, downtoners, negation, and irrealis markers). we show that this lexicon-based method performs well, and that it is robust across domains and texts. one of the criticisms raised against lexicon-based methods is that the dictionaries are unreliable, as they are either built automatically or hand-ranked by humans (andreevskaia and bergler 2008). in section 3, we present the results of several experiments that show that our dictionaries are robust and reliable, both against other existing dictionaries, and as compared to values assigned by humans (through the use of the mechanical turk interface). section 4 provides comparisons to other work, and section 5 conclusions. reference to domain portability, in this article and in other work, is usually limited to portability across different types of reviews (aue and gamon 2005; blitzer, dredze, and pereira 2007; andreevskaia and bergler 2008). this section shows that so-cal's performance is maintained across domains other than reviews, and across different types of text structures. even though so-cal was primarily designed to determine the sentiment of texts roughly a paragraph or longer, the evaluations reported in this section demonstrate comparable performance when applied to shorter texts such as headlines and sentences extracted from news and blogs. we tested so-cal with four different data sets: the multi-perspective question answering (mpqa) corpus, version 2.0 (wiebe, wilson, and cardie 2005); a collection of myspace.com comments from mike thelwall (prabowo and thelwall 2009); a set of news and blog posts from alina andreevskaia (andreevskaia and bergler 2008); and a set of headlines from rada mihalcea and carlo strappavara (strappavara and mihalcea 2007).20 approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively.",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"the majority of the statistical text classification research builds support vector machine classifiers, trained on a particular data set using features such as unigrams or bigrams, and with or without part-of-speech labels, although the most successful features seem to be basic unigrams (pang, lee, and vaithyanathan 2002; salvetti, reichenbach, and lewis 2006). classifiers built using supervised methods reach quite a high accuracy in detecting the polarity of a text (chaovalit and zhou 2005; kennedy and inkpen 2006; boiy et al. 2007; bartlett and albright 2008). however, although such classifiers perform very well in the domain that they are trained on, their performance drops precipitously (almost to chance) when the same classifier is used in",1
"4 to determine part of speech, we use the brill tagger (brill 1992).",1
"a different domain (aue and gamon [2005]; see also the discussion about domain specificity in pang and lee [2008, section 4.4]).3 consider, for example, an experiment using the polarity dataset, a corpus containing 2,000 movie reviews, in which brooke (2009) extracted the 100 most positive and negative unigram features from an svm classifier that reached 85.1% accuracy. many of these features were quite predictable: worst, waste, unfortunately, andmess are among the most negative, whereas memorable, wonderful, laughs, and enjoyed are all highly positive. other features are domain-specific and somewhat inexplicable: if the writer, director, plot, or script are mentioned, the review is likely to be disfavorable towards the movie, whereas the mention of performances, the ending, or even flaws, indicates a good movie. closedclass function words appear frequently; for instance, as, yet, with, and both are all extremely positive, whereas since, have, though, and those have negative weight. names also figure prominently, a problem noted by other researchers (finn and kushmerick 2003; kennedy and inkpen 2006). perhaps most telling is the inclusion of unigrams like 2, video, tv, and series in the list of negative words. the polarity of these words actually makes some sense in context: sequels and movies adapted from video games or tv series do tend to be less well-received than the average movie. however, these real-world facts are not the sort of knowledge a sentiment classifier ought to be learning; within the domain of movie reviews such facts are prejudicial, and in other domains (e.g., video games or tv shows) they are either irrelevant or a source of noise. secondly, so-cal allows for multiple cut-offs. most work in sentiment analysis has focused on binary positive/negative classification. notable exceptions include koppel and schler (2005) and pang and lee (2005), who each adapted relevant svm machinelearning algorithms to sentiment classification with a threeand four-class system, respectively. because so-cal outputs a numerical value that reflects both the polarity and strength of words appearing in the text, it is fairly straightforward to extend the function to any level of granularity required; in particular, the so-cal grouping script takes a list of n cut-off values, and classifies texts into n + 1 classes based on text values. the evaluative output gives information about exact matches and also near-misses (when a text is incorrectly classified into a neighboring class). this allows so-cal to identify, for instance, the star rating that would be assigned to a consumer review, as shown in brooke (2009). table 5 shows the performance of so-cal with a number of different options, on all corpora (recall that all but epinions 1 are completely unseen data). ""neg wand ""rep wrefer to the use of negative weighting (the so of negative terms is increased by 50%) and repetition weighting (the nth appearance of a word in the text has 1/n of its full so value). space considerations preclude a full discussion of the contribution of each part of speech and sub-feature, but see brooke (2009) for a full range of tests using these data. here the asterisks indicate a statistically-significant difference compared to the preceding set of options. we defined, in the introduction, sentiment as polarity plus strength, although the results presented here evaluate only the polarity accuracy. space precludes a full discussion of so-cal's measure of strength, but brooke (2009) shows that so-cal's output correlates well with star ratings in reviews.",1
"in related work, we have also shown that creating a new version of so-cal for a new language, spanish, is as fast as building text classifiers for the new language, and results in better performance (brooke 2009; brooke, tofiloski, and taboada 2009). so-cal has also been successfully deployed for the detection of sentence-level polarity (brooke and hurst 2009).",1
"to collect data on the validity of our dictionary, we made use of amazon's mechanical turk service,24 which provides access to a pool of workers who have signed up to perform small-scale tasks that require human intelligence. mechanical turk is quickly becoming a popular resource among computational linguists, and has been used in sentiment, emotion, and subjectivity tasks (akkaya et al. 2010; mellebeek et al. 2010; mohammad and turney 2010; yano, resnik, and smith 2010), although there are mixed reports on its reliability (snow et al. 2008; callison-burch 2009; zaenen to appear).",1
"another area where the lexicon-based model might be preferable to a classifier model is in simulating the effect of linguistic context. on reading any document, it becomes apparent that aspects of the local context of a word need to be taken into account in so assessment, such as negation (e.g., not good) and intensification (e.g., very good), aspects that polanyi and zaenen (2006) named contextual valence shifters. research by kennedy and inkpen (2006) concentrated on implementing those insights. they dealt with negation and intensification by creating separate features, namely, the appearance of good might be either good (no modification) not good (negated good), int good (intensified good), or dim good (diminished good). the classifier, however, cannot determine that these four types of good are in any way related, and so in order to train accurately there must be enough examples of all four in the training corpus. moreover, we show in section 2.4 that expanding the scope to two-word phrases does not deal with negation adequately, as it is often a long-distance phenomenon. recent work has begun to address this issue. for instance, choi and cardie (2008) present a classifier that treats negation from a compositional point of view by first calculating polarity of terms independently, and then applying inference rules to arrive at a combined polarity score. as we shall see in section 2, our lexicon-based model handles negation and intensification in a way that generalizes to all words that have a semantic orientation value. similarly, negation calculation does not include what choi and cardie (2008) term ""content word negators,words such as eliminate. most of those are included in the respective dictionaries (i.e., the verb dictionary for eliminate) with negative polarity. when they occur in a sentence, aggregation with other sentiment words in the sentence would probably yield a result similar to the compositional approach of choi and cardie or moilanen and pulman (2007).",1
"we tested so-cal with four different data sets: the multi-perspective question answering (mpqa) corpus, version 2.0 (wiebe, wilson, and cardie 2005); a collection of myspace.com comments from mike thelwall (prabowo and thelwall 2009); a set of news and blog posts from alina andreevskaia (andreevskaia and bergler 2008); and a set of headlines from rada mihalcea and carlo strappavara (strappavara and mihalcea 2007).20",1
"another issue is whether a polarity flip (switch negation) is the best way to quantify negation. though it seems to work well in certain cases (choi and cardie 2008), it fails miserably in others (liu and seneff 2009). consider excellent, a +5 adjective: if we negate it, we get not excellent, which intuitively is a far cry from atrocious, a -5 adjective. in fact, not excellent seems more positive than not good, which would negate to a -3. in order to capture these pragmatic intuitions, we implemented another method of negation, a polarity shift (shift negation). instead of changing the sign, the so value is shifted toward the opposite polarity by a fixed amount (in our current implementation, 4). thus a +2 adjective is negated to a -2, but the negation of a -3 adjective (for instance, sleazy) is only slightly positive, an effect we could call ""damning with faint praise.here are a few examples from our corpus. ours is not the only method that uses linguistic information or dictionaries. many other systems make use of either the subjectivity dictionary of wiebe and colleagues, or of sentiwordnet (devitt and ahmad 2007; thet et al. 2009), and some work relies on appraisal theory (whitelaw, garg, and argamon 2005; bloom, garg, and argamon 2007), a theory developed by martin and white (2005). we also discuss, in section 2.4, work on incorporating linguistic insights for the treatment of negation (moilanen and pulman 2007; choi and cardie 2008).",1
"a middle ground exists, however, with semi-supervised approaches to the problem. read and carroll (2009), for instance, use semi-supervised methods to build domainindependent polarity classifiers. read and carroll built different classifiers and show that they are more robust across domains. their classifiers are, in effect, dictionarybased, differing only in the methodology used to build the dictionary. li et al. (2010) use co-training to incorporate labeled and unlabeled examples, also making use of",1
"a distinction between sentences with a first person subject and with other subjects. other hybrid methods include those of andreevskaia and bergler (2008), dang, zhang, and chen (2010), dasgupta and ng (2009), goldberg and zhu (2006), or prabowo and thelwall (2009). wan (2009) uses co-training in a method that uses english labeled data and an english classifier to learn a classifier for chinese.",1
"creating subjective and objective sentence classifiers from unannotated texts. in proceedings of the sixth international conference on intelligent text processing and computational linguistics (cicling-2005), mexico city.",1
"groups for sentiment analysis. in proceedings of acm sigir conference on information and knowledge management (cikm 2005), pages 625-631, bremen. wiebe, janyce. 1994. tracking point of view in narrative. computational linguistics, 20(2):233-287.",1
"determining the sentiment of opinions. in proceedings of coling 2004, pages 1367-1373, geneva.",0
"approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively.",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
submission received: 14 december 2009; revised submission received: 22 august 2010; accepted for publication: 28 september 2010.,1
"it is difficult to measure the coverage of our dictionaries, because there is no direct way to estimate the number of so-carrying words and expressions in english (although it should probably be larger than 5,000, the rough total of our current dictionaries). wilson, wiebe, and hoffmann (2005) provide a list of subjectivity cues with over 8,000 entries; there are many more neutral, repeated, and inflectionally related entries than in our dictionaries, however, as well as many more nouns, and far fewer adjectives. automatically generated dictionaries are generally much larger: sentiwordnet (baccianella, esuli, and sebastiani 2010) includes 38,182 non-neutral words (when the polarity of senses is averaged--see discussion in section 3.4), and the maryland dictionary (mohammad, dorr, and dunne 2009) has 76,775 words and phrases tagged for polarity. we will see, in section 3.4, that larger dictionaries are not necessarily better, in some cases because the information contained is not as detailed (the maryland dictionary is not classified by part of speech), or because, in general, including more words may also lead to including more noise. the ""marylanddictionary (mohammad, dorr, and dunne 2009) is a very large collection of words and phrases (around 70,000) extracted from the macquarie thesaurus. the dictionary is not classified according to part of speech, and only contains information on whether the word is positive or negative. to integrate it into our system, we assigned all positive words an so value of 3, and all negative words a value of -3.33 we used the same type of quantification for the general inquirer (gi; stone et al. 1966), which also has only positive and negative tags; a word was included in the dictionary if any of the senses listed in the gi were polar.",1
"from wordnet glosses. in proceedings of the 11th conference of the european chapter of the association for computational linguistics, eacl-2006, pages 209-216, trento.",0
"much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim",1
"mohammad, saif, bonnie dorr, and cody dunne. 2009. generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus. in proceedings of the conference on empirical methods in natural language processing (emnlp-2009), pages 599-608, singapore.",1
"our current work focuses on developing discourse parsing methods, both general and specific to the review genre. at the same time, we will investigate different aggregation strategies for the different types of relations in the text (see also asher, benamara, and mathieu [2008, 2009] for preliminary work in this area), and build on existing discourse parsing systems and proposals (schilder 2002; soricut and marcu 2003; subba and di eugenio 2009).",1
"another publicly available corpus is sentiwordnet (esuli and sebastiani 2006; baccianella, esuli, and sebastiani 2010), an extension of wordnet (fellbaum 1998) where each synset is annotated with labels indicating how objective, positive, and negative the terms in the synset are. we use the average across senses for each word given in version 3.0 (see discussion in the next section). figure 9 gives the result for the the sentiwordnet dictionary (esuli and sebastiani 2006; baccianella, esuli, and sebastiani 2010), also used in the previous section, was built using wordnet (fellbaum 1998), and retains its synset structure. there are two main versions of sentiwordnet available, 1.0 and 3.0, and two straightforward methods to calculate an so value:34 use the first sense so, or average the so across senses. for the first sense method, we calculate the so value of a word w (of a given pos) based on its first sense f as follows:",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"to a theater near you! sentiment classification techniques using sas text miner. in sas global forum 2008, san antonio, tx. batson, c. daniel, laura l. shaw, and",1
"much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim reference to domain portability, in this article and in other work, is usually limited to portability across different types of reviews (aue and gamon 2005; blitzer, dredze, and pereira 2007; andreevskaia and bergler 2008). this section shows that so-cal's performance is maintained across domains other than reviews, and across different types of text structures. even though so-cal was primarily designed to determine the sentiment of texts roughly a paragraph or longer, the evaluations reported in this section demonstrate comparable performance when applied to shorter texts such as headlines and sentences extracted from news and blogs. approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively. ours is not the only method that uses linguistic information or dictionaries. many other systems make use of either the subjectivity dictionary of wiebe and colleagues, or of sentiwordnet (devitt and ahmad 2007; thet et al. 2009), and some work relies on appraisal theory (whitelaw, garg, and argamon 2005; bloom, garg, and argamon 2007), a theory developed by martin and white (2005). we also discuss, in section 2.4, work on incorporating linguistic insights for the treatment of negation (moilanen and pulman 2007; choi and cardie 2008).",1
"in our treatment of negation, we consider mostly negators, but not negative polarity items (npis), such as any, anything, ever, or at all. in some cases, searching for an npi would be more effective than searching for a negator. npis occur in negative sentences, but also in nonveridical contexts (zwarts 1995; giannakidou 1998), which also affect semantic orientation. for instance, any occurs in contexts other than negative sentences, as shown in example (11), from giannakidou (2001, page 99), where in all cases the presence of any is due to a nonveridical situation. using npis would allow us to reduce semantic orientation values in such contexts. we address some of these issues through irrealis blocking, as we explain in the next section.",1
"one other interesting aspect of the pragmatics of negation is that negative statements tend to be perceived as more marked than their affirmative counterparts, both pragmatically and psychologically (osgood and richards 1973; horn 1989, chapter 3). this markedness is true in terms of linguistic form, with negative forms being marked across languages (greenberg 1966), and it is also manifested as (token) frequency distribution, with negatives being less frequent (boucher and osgood 1969).15 negation tends to be expressed in euphemistic ways, which makes negative sentiment more difficult to identify in general.",1
"to build the system and run our experiments, we use the corpus described in taboada and grieve (2004) and taboada, anthony, and voll (2006), which consists of a 400-text collection of epinions reviews extracted from eight different categories: books, cars, computers, cookware, hotels, movies, music, and phones, a corpus we named ""epinions 1.within each collection, the reviews were split into 25 positive and 25 two other features merit discussion: weighting and multiple cut-offs. first of all, socal incorporates an option to assign different weights to sentences or portions of a text. taboada and grieve (2004) improved performance of an earlier version of the so calculator by assigning the most weight at the two-thirds mark of a text, and significantly less at the beginning. the current version has a user-configurable form of this weighting system, allowing any span of the text (with the end points represented by fractions of the entire text) to be given a certain weight. an even more flexible and powerful system is provided by the xml weighting option. when this option is enabled, xml tag pairs in the text (e.g., <topic>, </topic>) can be used as a signal to the calculator that any words appearing between these tags should be multiplied by a certain given weight. this gives so-cal an interface to outside modules. for example, one module could pre-process the text and tag spans that are believed to be topic sentences, another module could provide discourse information such as rhetorical relations (mann and thompson 1988), and a third module could label the sentences that seem to be subjective. armed with this information, so-cal can disregard or de-emphasize parts of the text that are less relevant to sentiment analysis. this weighting feature is used in taboada, brooke, and",0
"previous versions of so-cal (taboada and grieve 2004; taboada, anthony, and voll 2006) relied on an adjective dictionary to predict the overall so of a document, using a simple aggregate-and-average method: the individual scores for each adjective in a document are added together and then divided by the total number of adjectives in that document.4 as we describe subsequently, the current version of so-cal takes other parts of speech into account, and makes use of more sophisticated methods to determine the true contribution of each word. our original collection of 400 review texts (taboada and grieve 2004), used in various phases of development. the collection consists of 50 reviews each of: books, cars, computers, cookware, hotels, movies, music, and phones.",1
"except when one sense was very uncommon, the value chosen reflected an averaging across possible interpretations. in some cases, the verb and related noun have a different so value. for instance, exaggerate is -1, whereas exaggeration is -2, and the same values are applied to complicate and complication, respectively. we find that grammatical metaphor (halliday 1985), that is, the use of a noun to refer to an action, adds a more negative connotation to negative words.",0
"wiebe, and rada mihalcea. 2010. amazon mechanical turk for subjectivity word sense disambiguation. in proceedings of the naacl hlt 2010 workshop on creating speech and language data with amazon's mechanical turk, pages 195-203, los angeles, ca. jens grivolla, joan codina, marta r. costa-juss`a, and rafael banchs. 2010. opinion mining of spanish customer comments with non-expert annotations on mechanical turk. in proceedings of the naacl hlt 2010 workshop on creating speech and language data with amazon's mechanical turk, pages 114-121, los angeles, ca. mohammad, saif and peter turney. 2010. emotions evoked by common words and phrases: using mechanical turk to create an emotion lexicon. in proceedings of the naacl hlt 2010 workshop on computational approaches to analysis and generation of emotion in text, pages 26-34, los angeles, ca. 2010. shedding (a thousand points of) light on biased language. in proceedings of the naacl hlt 2010 workshop on creating speech and language data with amazon's mechanical turk, pages 152-158, los angeles, ca.",1
"subba, rajen and barbara di eugenio. 2009. an effective discourse parser that uses rich linguistic information. in proceedings of hlt-acl 2009, pages 566-574, boulder, co.",0
"goldberg, andrew b. and xiaojin zhu. 2006. seeing stars when there aren't many stars: graph-based semi-supervised learning for sentiment categorization. in proceedings of hlt-naacl 2006 workshop on textgraphs: graph-based algorithms for natural language processing, pages 45-52, new york. greenberg, joseph h. 1966. language",2
"it is difficult to measure the coverage of our dictionaries, because there is no direct way to estimate the number of so-carrying words and expressions in english (although it should probably be larger than 5,000, the rough total of our current dictionaries). wilson, wiebe, and hoffmann (2005) provide a list of subjectivity cues with over 8,000 entries; there are many more neutral, repeated, and inflectionally related entries than in our dictionaries, however, as well as many more nouns, and far fewer adjectives. automatically generated dictionaries are generally much larger: sentiwordnet (baccianella, esuli, and sebastiani 2010) includes 38,182 non-neutral words (when the polarity of senses is averaged--see discussion in section 3.4), and the maryland dictionary (mohammad, dorr, and dunne 2009) has 76,775 words and phrases tagged for polarity. we will see, in section 3.4, that larger dictionaries are not necessarily better, in some cases because the information contained is not as detailed (the maryland dictionary is not classified by part of speech), or because, in general, including more words may also lead to including more noise. our next comparison is with the subjectivity dictionary of wilson, wiebe, and hoffmann (2005). words are rated for polarity (positive or negative) and strength (weak or strong), meaning that their scale is much more coarse-grained than ours. the dictionary is derived from both manual and automatic sources. it is fairly comprehensive (over 8,000 entries), so we assume that any word not mentioned in the dictionary is neutral. figure 7 shows the result for the single word task. the subjectivity dictionary is the collection of subjective expressions compiled by wilson, wiebe, and hoffmann (2005), also used in our mechanical turk experiments in the previous section. the subjectivity dictionary only contains a distinction between weak and strong opinion words. for our tests, weak words were assigned 2 or -2 values, depending on whether they were positive or negative, and strong words were assigned 4 or -4.",1
"the problems with polarity shift could probably be resolved by fine-tuning so values and modifiers; the polarity flip model seems fundamentally flawed, however. polarity shifts seem to better reflect the pragmatic reality of negation, and is supported by horn (1989), who suggests that affirmative and negative sentences are not symmetrical.",1
"negation search in so-cal includes two options: look backwards until a clause boundary marker is reached;13 or look backwards as long as the words/tags found are in a backward search skip list, with a different list for each part of speech. the first approach is fairly liberal, and allows us to capture the true effects of negation raising (horn 1989), where the negator for a verb moves up and attaches to the verb in the matrix clause. in the following examples the don't that negates the verb think is actually negating the embedded clause. one other interesting aspect of the pragmatics of negation is that negative statements tend to be perceived as more marked than their affirmative counterparts, both pragmatically and psychologically (osgood and richards 1973; horn 1989, chapter 3). this markedness is true in terms of linguistic form, with negative forms being marked across languages (greenberg 1966), and it is also manifested as (token) frequency distribution, with negatives being less frequent (boucher and osgood 1969).15 negation tends to be expressed in euphemistic ways, which makes negative sentiment more difficult to identify in general. interestingly, although we expected relatively equal numbers of positive and negative judgments at so = 0, that was not the result. instead, words with so = 0 were sometimes interpreted as positive, but almost never interpreted as negative. this is mostly likely attributable to the default status of positive, and the marked character of negative expression (boucher and osgood 1969; horn 1989; jing-schmidt 2007); neutral description might be taken as being vaguely positive, but it would not be mistaken for negative expression.26",1
"following osgood, suci, and tannenbaum (1957), the calculation of sentiment in so-cal begins with two assumptions: that individual words have what is referred to as prior polarity, that is, a semantic orientation that is independent of context; and that said semantic orientation can be expressed as a numerical value. several lexiconbased approaches have adopted these assumptions (bruce and wiebe 2000; hu and liu 2004; kim and hovy 2004). in this section, we describe the different dictionaries used in so-cal, and the incorporation of valence shifters. we conclude the section with tests that show so-cal's performance on different data sets. much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim and hovy 2004; esuli and sebastiani 2006). the dictionary may also be produced automatically via association, where the score for each new adjective is calculated using the frequency of the proximity of that adjective with respect to one or more seed words. seed words are a small set of words with strong negative or positive associations, such as excellent or abysmal. in principle, a positive adjective should occur more frequently alongside the positive seed words, and thus will obtain a positive score, whereas negative adjectives will occur most often in the vicinity of negative seed words, thus obtaining a negative score. the association is usually calculated following turney's method for computing mutual information (turney 2002; turney and littman 2003), but see also rao and ravichandran (2009) and velikovich et al. (2010) for other methods using seed words.",0
"the second method used in evaluating so-cal on so-empty texts is to only classify texts for which it has direct evidence to make a judgment. thus, we exclude such so-empty texts from the evaluation. the second part of table 7 shows the results of this evaluation. the results are strikingly similar to the performance we saw on full review texts, with most attaining a minimum of 75-80% accuracy. although missing vocabulary (domain-specific or otherwise) undoubtedly plays a role, the results provide strong evidence that relative text size is the primary cause of so-empty texts in these data sets. when the so-empty texts are removed, the results are entirely comparable to those that we saw in the previous section. although sentence-level polarity detection is a more difficult task, and not one that so-cal was specifically designed for, the system has performed well on this task, here, and in related work (murray et al. 2008; brooke and hurst 2009). the main conclusion of our work is that lexicon-based methods for sentiment analysis are robust, result in good cross-domain performance, and can be easily enhanced with multiple sources of knowledge (taboada, brooke, and stede 2009). so-cal has performed well on blog postings (murray et al. 2008) and video game reviews (brooke and hurst 2009), without any need for further development or training. in related work, we have also shown that creating a new version of so-cal for a new language, spanish, is as fast as building text classifiers for the new language, and results in better performance (brooke 2009; brooke, tofiloski, and taboada 2009). so-cal has also been successfully deployed for the detection of sentence-level polarity (brooke and hurst 2009).",1
and hwa 2004; pang and lee 2005).,1
"koppel, moshe and jonathan schler. 2005. using neutral examples for learning polarity. in proceedings of ijcai 2005, pages 1616-1617, edinburgh.",0
"another area where the lexicon-based model might be preferable to a classifier model is in simulating the effect of linguistic context. on reading any document, it becomes apparent that aspects of the local context of a word need to be taken into account in so assessment, such as negation (e.g., not good) and intensification (e.g., very good), aspects that polanyi and zaenen (2006) named contextual valence shifters. research by kennedy and inkpen (2006) concentrated on implementing those insights. they dealt with negation and intensification by creating separate features, namely, the appearance of good might be either good (no modification) not good (negated good), int good (intensified good), or dim good (diminished good). the classifier, however, cannot determine that these four types of good are in any way related, and so in order to train accurately there must be enough examples of all four in the training corpus. moreover, we show in section 2.4 that expanding the scope to two-word phrases does not deal with negation adequately, as it is often a long-distance phenomenon. recent work has begun to address this issue. for instance, choi and cardie (2008) present a classifier that treats negation from a compositional point of view by first calculating polarity of terms independently, and then applying inference rules to arrive at a combined polarity score. as we shall see in section 2, our lexicon-based model handles negation and intensification in a way that generalizes to all words that have a semantic orientation value. a related problem for the polarity flip model, as noted by kennedy and inkpen (2006), is that negative polarity items interact with intensifiers in undesirable ways. not very good, for instance, comes out more negative than not good. another way to handle this problem while preserving the notion of a polarity flip is to allow the negative item to flip the polarity of both the adjective and the intensifier; in this way, an amplifier becomes a downtoner: intensification, as also shown by kennedy and inkpen (2006). in summary, it is the combination of carefully crafted dictionaries and features inspired by linguistic insights that we believe makes so-cal a robust method for sentiment analysis.",1
"the majority of the statistical text classification research builds support vector machine classifiers, trained on a particular data set using features such as unigrams or bigrams, and with or without part-of-speech labels, although the most successful features seem to be basic unigrams (pang, lee, and vaithyanathan 2002; salvetti, reichenbach, and lewis 2006). classifiers built using supervised methods reach quite a high accuracy in detecting the polarity of a text (chaovalit and zhou 2005; kennedy and inkpen 2006; boiy et al. 2007; bartlett and albright 2008). however, although such classifiers perform very well in the domain that they are trained on, their performance drops precipitously (almost to chance) when the same classifier is used in a different domain (aue and gamon [2005]; see also the discussion about domain specificity in pang and lee [2008, section 4.4]).3 consider, for example, an experiment using the polarity dataset, a corpus containing 2,000 movie reviews, in which brooke (2009) extracted the 100 most positive and negative unigram features from an svm classifier that reached 85.1% accuracy. many of these features were quite predictable: worst, waste, unfortunately, andmess are among the most negative, whereas memorable, wonderful, laughs, and enjoyed are all highly positive. other features are domain-specific and somewhat inexplicable: if the writer, director, plot, or script are mentioned, the review is likely to be disfavorable towards the movie, whereas the mention of performances, the ending, or even flaws, indicates a good movie. closedclass function words appear frequently; for instance, as, yet, with, and both are all extremely positive, whereas since, have, though, and those have negative weight. names also figure prominently, a problem noted by other researchers (finn and kushmerick 2003; kennedy and inkpen 2006). perhaps most telling is the inclusion of unigrams like 2, video, tv, and series in the list of negative words. the polarity of these words actually makes some sense in context: sequels and movies adapted from video games or tv series do tend to be less well-received than the average movie. however, these real-world facts are not the sort of knowledge a sentiment classifier ought to be learning; within the domain of movie reviews such facts are prejudicial, and in other domains (e.g., video games or tv shows) they are either irrelevant or a source of noise. quirk et al. (1985) classify intensifiers into two major categories, depending on their polarity: amplifiers (e.g., very) increase the semantic intensity of a neighboring lexical item, whereas downtoners (e.g., slightly) decrease it. some researchers in sentiment analysis (kennedy and inkpen 2006; polanyi and zaenen 2006) have implemented lexicon-based sentiment classifiers generally show a positive bias (kennedy and inkpen 2006), likely the result of a universal human tendency to favor positive language (boucher and osgood 1969).16 in order to overcome this problem, voll and taboada (2007) implemented normalization, shifting the numerical cut-off point between positive and negative reviews. in the current version of so-cal, we have used a somewhat approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively.",1
"15 some researchers argue that there is a negative bias in the human representation of experience (negative events are more salient), and the positive bias found by boucher and osgood is the result of euphemisms and political correctness in language (jing-schmidt 2007). interestingly, although we expected relatively equal numbers of positive and negative judgments at so = 0, that was not the result. instead, words with so = 0 were sometimes interpreted as positive, but almost never interpreted as negative. this is mostly likely attributable to the default status of positive, and the marked character of negative expression (boucher and osgood 1969; horn 1989; jing-schmidt 2007); neutral description might be taken as being vaguely positive, but it would not be mistaken for negative expression.26",1
"hu, minqing and bing liu. 2004. mining and summarizing customer reviews. in proceedings of the acm sigkdd international conference on knowledge discovery and data mining (kdd-2004), pages 168-177, seattle, wa.",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"it is important to note that how a dictionary is created affects the overall accuracy of subsequent results. in taboada, anthony, and voll (2006) we report on experiments using different search engines and operators in trying to create dictionaries semiautomatically. we found that, although usable, dictionaries created using the google search engine were unstable. when rerun, the results for each word were subject to change, sometimes by extreme amounts, something that kilgarriff (2007) also notes, arguing against the use of google for linguistic research of this type. an alternative would be to use a sufficiently large static corpus, as turney (2006) does to measure relational similarity across word pairs.",1
"a different domain (aue and gamon [2005]; see also the discussion about domain specificity in pang and lee [2008, section 4.4]).3 consider, for example, an experiment using the polarity dataset, a corpus containing 2,000 movie reviews, in which brooke (2009) extracted the 100 most positive and negative unigram features from an svm classifier that reached 85.1% accuracy. many of these features were quite predictable: worst, waste, unfortunately, andmess are among the most negative, whereas memorable, wonderful, laughs, and enjoyed are all highly positive. other features are domain-specific and somewhat inexplicable: if the writer, director, plot, or script are mentioned, the review is likely to be disfavorable towards the movie, whereas the mention of performances, the ending, or even flaws, indicates a good movie. closedclass function words appear frequently; for instance, as, yet, with, and both are all extremely positive, whereas since, have, though, and those have negative weight. names also figure prominently, a problem noted by other researchers (finn and kushmerick 2003; kennedy and inkpen 2006). perhaps most telling is the inclusion of unigrams like 2, video, tv, and series in the list of negative words. the polarity of these words actually makes some sense in context: sequels and movies adapted from video games or tv series do tend to be less well-received than the average movie. however, these real-world facts are not the sort of knowledge a sentiment classifier ought to be learning; within the domain of movie reviews such facts are prejudicial, and in other domains (e.g., video games or tv shows) they are either irrelevant or a source of noise.",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively.",1
"much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim",1
"secondly, so-cal allows for multiple cut-offs. most work in sentiment analysis has focused on binary positive/negative classification. notable exceptions include koppel and schler (2005) and pang and lee (2005), who each adapted relevant svm machinelearning algorithms to sentiment classification with a threeand four-class system, respectively. because so-cal outputs a numerical value that reflects both the polarity and strength of words appearing in the text, it is fairly straightforward to extend the function to any level of granularity required; in particular, the so-cal grouping script takes a list of n cut-off values, and classifies texts into n + 1 classes based on text values. the evaluative output gives information about exact matches and also near-misses (when a text is incorrectly classified into a neighboring class). this allows so-cal to identify, for instance, the star rating that would be assigned to a consumer review, as shown in brooke (2009).",1
1 pang and lee (2008) provide an excellent recent survey of the opinion mining or sentiment analysis,1
"in order to make use of this additional information, we created separate noun, verb, and adverb dictionaries, hand-ranked using the same +5 to -5 scale as our adjective dictionary. the enhanced dictionaries contain 2,252 adjective entries, 1,142 nouns, 903 verbs, and 745 adverbs.6 the so-carrying words in these dictionaries were taken from a variety of sources, the three largest being epinions 1, the 400-text corpus described in the previous section; a 100-text subset of the 2,000 movie reviews in the polarity dataset (pang, lee, and vaithyanathan 2002; pang and lee 2004, 2005);7 and positive and negative words from the general inquirer dictionary (stone et al. 1966; stone 1997).8 the sources provide a fairly good range in terms of register: the epinions and movie reviews represent informal language, with words such as ass-kicking and nifty; at the other end of the spectrum, the general inquirer was clearly built from much more formal texts, and contributed words such as adroit and jubilant, which may be more useful in the processing of literary reviews (taboada, gillies, and mcfetridge 2006; taboada et al. 2008) or other more formal texts. 1,900 texts from the polarity dataset (pang and lee 2004).18 the breakdown for the polarity dataset (movies in table 5) is 89.37% precision for negative reviews and 63.2% for positive ones, with an overall accuracy of 76.37%. a number of other papers have used the polarity dataset created by bo pang, most of them following statistical methods. pang and lee's own results show an overall accuracy of 87.15% for polarity classification of whole reviews (pang and lee 2004). fletcher and patrick (2005) used bags-of-words that included appraisal features, and obtained 83.7% accuracy in that same data set, whereas whitelaw, garg, and argamon (2005), using bags-of-words combined with appraisal groups achieved 90.2%. in all cases, the accuracy reflects a single domain and data set. andreevskaia and bergler (2008) show, however, that cross-domain performance drops significantly. they used a hybrid",0
and hwa 2004; pang and lee 2005).,1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"the majority of the statistical text classification research builds support vector machine classifiers, trained on a particular data set using features such as unigrams or bigrams, and with or without part-of-speech labels, although the most successful features seem to be basic unigrams (pang, lee, and vaithyanathan 2002; salvetti, reichenbach, and lewis 2006). classifiers built using supervised methods reach quite a high accuracy in detecting the polarity of a text (chaovalit and zhou 2005; kennedy and inkpen 2006; boiy et al. 2007; bartlett and albright 2008). however, although such classifiers perform very well in the domain that they are trained on, their performance drops precipitously (almost to chance) when the same classifier is used in",1
"a middle ground exists, however, with semi-supervised approaches to the problem. read and carroll (2009), for instance, use semi-supervised methods to build domainindependent polarity classifiers. read and carroll built different classifiers and show that they are more robust across domains. their classifiers are, in effect, dictionarybased, differing only in the methodology used to build the dictionary. li et al. (2010) use co-training to incorporate labeled and unlabeled examples, also making use of",1
"dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also stone et al. 1966; tong 2001), or automatically, using seed words to expand the list of words (hatzivassiloglou and mckeown 1997; turney 2002; turney and littman 2003). much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text (hatzivassiloglou and mckeown 1997; wiebe 2000; hu and liu 2004; taboada, anthony, and voll 2006).2 first, a list of adjectives and corresponding so values is compiled into a dictionary. then, for any given text, all adjectives are extracted and annotated with their so value, using the dictionary scores. the so scores are in turn aggregated into a single score for the text. much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim and hovy 2004; esuli and sebastiani 2006). the dictionary may also be produced automatically via association, where the score for each new adjective is calculated using the frequency of the proximity of that adjective with respect to one or more seed words. seed words are a small set of words with strong negative or positive associations, such as excellent or abysmal. in principle, a positive adjective should occur more frequently alongside the positive seed words, and thus will obtain a positive score, whereas negative adjectives will occur most often in the vicinity of negative seed words, thus obtaining a negative score. the association is usually calculated following turney's method for computing mutual information (turney 2002; turney and littman 2003), but see also rao and ravichandran (2009) and velikovich et al. (2010) for other methods using seed words.",0
"dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also stone et al. 1966; tong 2001), or automatically, using seed words to expand the list of words (hatzivassiloglou and mckeown 1997; turney 2002; turney and littman 2003). much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text (hatzivassiloglou and mckeown 1997; wiebe 2000; hu and liu 2004; taboada, anthony, and voll 2006).2 first, a list of adjectives and corresponding so values is compiled into a dictionary. then, for any given text, all adjectives are extracted and annotated with their so value, using the dictionary scores. the so scores are in turn aggregated into a single score for the text. following osgood, suci, and tannenbaum (1957), the calculation of sentiment in so-cal begins with two assumptions: that individual words have what is referred to as prior polarity, that is, a semantic orientation that is independent of context; and that said semantic orientation can be expressed as a numerical value. several lexiconbased approaches have adopted these assumptions (bruce and wiebe 2000; hu and liu 2004; kim and hovy 2004). in this section, we describe the different dictionaries used in so-cal, and the incorporation of valence shifters. we conclude the section with tests that show so-cal's performance on different data sets. much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"the classification of paragraphs into comment and description is but one of the many ways in which contextual information can be incorporated into a robust approach to sentiment extraction. in previous work (voll and taboada 2007), we showed a prototype for extracting topic sentences, and performing sentiment analysis on those only. we also showed how a sentence-level discourse parser, developed by soricut and marcu (2003), could be used to differentiate between main and secondary parts of the text. at",1
"our current work focuses on developing discourse parsing methods, both general and specific to the review genre. at the same time, we will investigate different aggregation strategies for the different types of relations in the text (see also asher, benamara, and mathieu [2008, 2009] for preliminary work in this area), and build on existing discourse parsing systems and proposals (schilder 2002; soricut and marcu 2003; subba and di eugenio 2009).",1
"in order to make use of this additional information, we created separate noun, verb, and adverb dictionaries, hand-ranked using the same +5 to -5 scale as our adjective dictionary. the enhanced dictionaries contain 2,252 adjective entries, 1,142 nouns, 903 verbs, and 745 adverbs.6 the so-carrying words in these dictionaries were taken from a variety of sources, the three largest being epinions 1, the 400-text corpus described in the previous section; a 100-text subset of the 2,000 movie reviews in the polarity dataset (pang, lee, and vaithyanathan 2002; pang and lee 2004, 2005);7 and positive and negative words from the general inquirer dictionary (stone et al. 1966; stone 1997).8 the sources provide a fairly good range in terms of register: the epinions and movie reviews represent informal language, with words such as ass-kicking and nifty; at the other end of the spectrum, the general inquirer was clearly built from much more formal texts, and contributed words such as adroit and jubilant, which may be more useful in the processing of literary reviews (taboada, gillies, and mcfetridge 2006; taboada et al. 2008) or other more formal texts.",1
17 one of the reviewers points out that this is similar to the use of term frequency (tf-idf) in information retrieval (salton and mcgill 1983). see also paltoglou and thelwall (2010) for a use of information retrieval techniques in sentiment analysis.,1
"dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also stone et al. 1966; tong 2001), or automatically, using seed words to expand the list of words (hatzivassiloglou and mckeown 1997; turney 2002; turney and littman 2003). much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text (hatzivassiloglou and mckeown 1997; wiebe 2000; hu and liu 2004; taboada, anthony, and voll 2006).2 first, a list of adjectives and corresponding so values is compiled into a dictionary. then, for any given text, all adjectives are extracted and annotated with their so value, using the dictionary scores. the so scores are in turn aggregated into a single score for the text. much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim",1
"to collect data on the validity of our dictionary, we made use of amazon's mechanical turk service,24 which provides access to a pool of workers who have signed up to perform small-scale tasks that require human intelligence. mechanical turk is quickly becoming a popular resource among computational linguists, and has been used in sentiment, emotion, and subjectivity tasks (akkaya et al. 2010; mellebeek et al. 2010; mohammad and turney 2010; yano, resnik, and smith 2010), although there are mixed reports on its reliability (snow et al. 2008; callison-burch 2009; zaenen to appear).",1
"the affective text data from rada mihalcea and carlo strappavara was used in the 2007 semeval task. it contains 1,000 news headlines annotated with a range between -100 (very negative) and 100 (very positive). we excluded six headlines that had been labeled as 0 (therefore neutral), yielding 468 positive and 526 negative headlines. in addition to the full evaluation, strappavara and mihalcea (2007) also propose a coarse evaluation, where headlines with scores -100 to -50 are classified as negative, and those 50 to 100 as positive. excluding the headlines in the middle gives us 155 positive headlines and 255 negative ones.",2
"we tested so-cal with four different data sets: the multi-perspective question answering (mpqa) corpus, version 2.0 (wiebe, wilson, and cardie 2005); a collection of myspace.com comments from mike thelwall (prabowo and thelwall 2009); a set of news and blog posts from alina andreevskaia (andreevskaia and bergler 2008); and a set of headlines from rada mihalcea and carlo strappavara (strappavara and mihalcea 2007).20",1
"the second method used in evaluating so-cal on so-empty texts is to only classify texts for which it has direct evidence to make a judgment. thus, we exclude such so-empty texts from the evaluation. the second part of table 7 shows the results of this evaluation. the results are strikingly similar to the performance we saw on full review texts, with most attaining a minimum of 75-80% accuracy. although missing vocabulary (domain-specific or otherwise) undoubtedly plays a role, the results provide strong evidence that relative text size is the primary cause of so-empty texts in these data sets. when the so-empty texts are removed, the results are entirely comparable to those that we saw in the previous section. although sentence-level polarity detection is a more difficult task, and not one that so-cal was specifically designed for, the system has performed well on this task, here, and in related work (murray et al. 2008; brooke and hurst 2009). the main conclusion of our work is that lexicon-based methods for sentiment analysis are robust, result in good cross-domain performance, and can be easily enhanced with multiple sources of knowledge (taboada, brooke, and stede 2009). so-cal has performed well on blog postings (murray et al. 2008) and video game reviews (brooke and hurst 2009), without any need for further development or training.",1
"a distinction between sentences with a first person subject and with other subjects. other hybrid methods include those of andreevskaia and bergler (2008), dang, zhang, and chen (2010), dasgupta and ng (2009), goldberg and zhu (2006), or prabowo and thelwall (2009). wan (2009) uses co-training in a method that uses english labeled data and an english classifier to learn a classifier for chinese.",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"table 6 shows the accuracy of the so-cal system across different review types in epinions 1 and epinions 2 (the latter unseen data), including the f-measure for classification of positive and negative texts. the table shows that so-cal's performance on positive reviews is generally better than on negative reviews (run with all options and shifted negation). this is despite the fact that all of our dictionaries contain far more negative words than positive ones. as noted already by boucher and osgood (1969), there is a strong preference for avoiding negation and negative terms even when expressing negative opinions, making the detection of text sentiment difficult for systems which depend solely on these indicators (also see results in dave, lawrence, and pennock 2003, turney 2002). the exception are books and movies, where performance is more even across positive and negative, or often better in negative reviews. we hypothesize this is because consumer product reviews contain more factual information that the reader is required to interpret as positive or negative (for instance, the range for a cordless phone or the leg room in the back seat of a car). some of that factual information may be implicitly negative, and therefore missed by so-cal.",1
"one other interesting aspect of the pragmatics of negation is that negative statements tend to be perceived as more marked than their affirmative counterparts, both pragmatically and psychologically (osgood and richards 1973; horn 1989, chapter 3). this markedness is true in terms of linguistic form, with negative forms being marked across languages (greenberg 1966), and it is also manifested as (token) frequency distribution, with negatives being less frequent (boucher and osgood 1969).15 negation tends to be expressed in euphemistic ways, which makes negative sentiment more difficult to identify in general. lexicon-based sentiment classifiers generally show a positive bias (kennedy and inkpen 2006), likely the result of a universal human tendency to favor positive language (boucher and osgood 1969).16 in order to overcome this problem, voll and taboada (2007) implemented normalization, shifting the numerical cut-off point between positive and negative reviews. in the current version of so-cal, we have used a somewhat interestingly, although we expected relatively equal numbers of positive and negative judgments at so = 0, that was not the result. instead, words with so = 0 were sometimes interpreted as positive, but almost never interpreted as negative. this is mostly likely attributable to the default status of positive, and the marked character of negative expression (boucher and osgood 1969; horn 1989; jing-schmidt 2007); neutral description might be taken as being vaguely positive, but it would not be mistaken for negative expression.26",1
"the breakdown for the polarity dataset (movies in table 5) is 89.37% precision for negative reviews and 63.2% for positive ones, with an overall accuracy of 76.37%. a number of other papers have used the polarity dataset created by bo pang, most of them following statistical methods. pang and lee's own results show an overall accuracy of 87.15% for polarity classification of whole reviews (pang and lee 2004). fletcher and patrick (2005) used bags-of-words that included appraisal features, and obtained 83.7% accuracy in that same data set, whereas whitelaw, garg, and argamon (2005), using bags-of-words combined with appraisal groups achieved 90.2%. in all cases, the accuracy reflects a single domain and data set. andreevskaia and bergler (2008) show, however, that cross-domain performance drops significantly. they used a hybrid",1
"table 6 shows the accuracy of the so-cal system across different review types in epinions 1 and epinions 2 (the latter unseen data), including the f-measure for classification of positive and negative texts. the table shows that so-cal's performance on positive reviews is generally better than on negative reviews (run with all options and shifted negation). this is despite the fact that all of our dictionaries contain far more negative words than positive ones. as noted already by boucher and osgood (1969), there is a strong preference for avoiding negation and negative terms even when expressing negative opinions, making the detection of text sentiment difficult for systems which depend solely on these indicators (also see results in dave, lawrence, and pennock 2003, turney 2002). the exception are books and movies, where performance is more even across positive and negative, or often better in negative reviews. we hypothesize this is because consumer product reviews contain more factual information that the reader is required to interpret as positive or negative (for instance, the range for a cordless phone or the leg room in the back seat of a car). some of that factual information may be implicitly negative, and therefore missed by so-cal.",1
"3 blitzer, dredze, and pereira (2007) do show some success in transferring knowledge across domains, so",1
"reference to domain portability, in this article and in other work, is usually limited to portability across different types of reviews (aue and gamon 2005; blitzer, dredze, and pereira 2007; andreevskaia and bergler 2008). this section shows that so-cal's performance is maintained across domains other than reviews, and across different types of text structures. even though so-cal was primarily designed to determine the sentiment of texts roughly a paragraph or longer, the evaluations reported in this section demonstrate comparable performance when applied to shorter texts such as headlines and sentences extracted from news and blogs.",1
"similarly, negation calculation does not include what choi and cardie (2008) term ""content word negators,words such as eliminate. most of those are included in the respective dictionaries (i.e., the verb dictionary for eliminate) with negative polarity. when they occur in a sentence, aggregation with other sentiment words in the sentence would probably yield a result similar to the compositional approach of choi and cardie or moilanen and pulman (2007).",1
"ours is not the only method that uses linguistic information or dictionaries. many other systems make use of either the subjectivity dictionary of wiebe and colleagues, or of sentiwordnet (devitt and ahmad 2007; thet et al. 2009), and some work relies on appraisal theory (whitelaw, garg, and argamon 2005; bloom, garg, and argamon 2007), a theory developed by martin and white (2005). we also discuss, in section 2.4, work on incorporating linguistic insights for the treatment of negation (moilanen and pulman 2007; choi and cardie 2008).",1
"quirk et al. (1985) classify intensifiers into two major categories, depending on their polarity: amplifiers (e.g., very) increase the semantic intensity of a neighboring lexical item, whereas downtoners (e.g., slightly) decrease it. some researchers in sentiment analysis (kennedy and inkpen 2006; polanyi and zaenen 2006) have implemented",1
"and hovy 2004; esuli and sebastiani 2006). the dictionary may also be produced automatically via association, where the score for each new adjective is calculated using the frequency of the proximity of that adjective with respect to one or more seed words. seed words are a small set of words with strong negative or positive associations, such as excellent or abysmal. in principle, a positive adjective should occur more frequently alongside the positive seed words, and thus will obtain a positive score, whereas negative adjectives will occur most often in the vicinity of negative seed words, thus obtaining a negative score. the association is usually calculated following turney's method for computing mutual information (turney 2002; turney and littman 2003), but see also rao and ravichandran (2009) and velikovich et al. (2010) for other methods using seed words.",0
"much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim",1
"the sentence level, exploring the types of syntactic patterns that indicate subjectivity and sentiment is also a possibility (greene and resnik 2009). syntactic patterns can also be used to distinguish different types of opinion and appraisal (bednarek 2009).",1
"one other interesting aspect of the pragmatics of negation is that negative statements tend to be perceived as more marked than their affirmative counterparts, both pragmatically and psychologically (osgood and richards 1973; horn 1989, chapter 3). this markedness is true in terms of linguistic form, with negative forms being marked across languages (greenberg 1966), and it is also manifested as (token) frequency distribution, with negatives being less frequent (boucher and osgood 1969).15 negation tends to be expressed in euphemistic ways, which makes negative sentiment more difficult to identify in general.",1
"the bulk of the work in sentiment analysis has focused on classification at either the sentence level, for example, the subjectivity/polarity detection of wiebe and riloff (2005), or alternatively at the level of the entire text. with regard to the latter, two major",1
"the obvious approach to negation is simply to reverse the polarity of the lexical item next to a negator, changing good (+3) into not good (-3). this we may refer to as switch negation (saur'i 2008). there are a number of subtleties related to negation that need to be taken into account, however. one is the fact that there are negators, including not, none, nobody, never, andnothing, and other words, such as without or lack (verb and noun), which have an equivalent effect, some of which might occur at a significant distance from the lexical item which they affect; a backwards search is required to find these negators, one that is tailored to the particular part of speech involved. we assume that for adjectives and adverbs the negation is fairly local, though it is sometimes necessary to look past determiners, copulas, and certain verbs, as we see in example (6).",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"our current work focuses on developing discourse parsing methods, both general and specific to the review genre. at the same time, we will investigate different aggregation strategies for the different types of relations in the text (see also asher, benamara, and mathieu [2008, 2009] for preliminary work in this area), and build on existing discourse parsing systems and proposals (schilder 2002; soricut and marcu 2003; subba and di eugenio 2009).",1
"secondly, so-cal allows for multiple cut-offs. most work in sentiment analysis has focused on binary positive/negative classification. notable exceptions include koppel and schler (2005) and pang and lee (2005), who each adapted relevant svm machinelearning algorithms to sentiment classification with a threeand four-class system, respectively. because so-cal outputs a numerical value that reflects both the polarity and strength of words appearing in the text, it is fairly straightforward to extend the function to any level of granularity required; in particular, the so-cal grouping script takes a list of n cut-off values, and classifies texts into n + 1 classes based on text values. the evaluative output gives information about exact matches and also near-misses (when a text is incorrectly classified into a neighboring class). this allows so-cal to identify, for instance, the star rating that would be assigned to a consumer review, as shown in brooke (2009).",1
"and hovy 2004; esuli and sebastiani 2006). the dictionary may also be produced automatically via association, where the score for each new adjective is calculated using the frequency of the proximity of that adjective with respect to one or more seed words. seed words are a small set of words with strong negative or positive associations, such as excellent or abysmal. in principle, a positive adjective should occur more frequently alongside the positive seed words, and thus will obtain a positive score, whereas negative adjectives will occur most often in the vicinity of negative seed words, thus obtaining a negative score. the association is usually calculated following turney's method for computing mutual information (turney 2002; turney and littman 2003), but see also rao and ravichandran (2009) and velikovich et al. (2010) for other methods using seed words. another publicly available corpus is sentiwordnet (esuli and sebastiani 2006; baccianella, esuli, and sebastiani 2010), an extension of wordnet (fellbaum 1998) where each synset is annotated with labels indicating how objective, positive, and negative the terms in the synset are. we use the average across senses for each word given in version 3.0 (see discussion in the next section). figure 9 gives the result for the the sentiwordnet dictionary (esuli and sebastiani 2006; baccianella, esuli, and sebastiani 2010), also used in the previous section, was built using wordnet (fellbaum 1998), and retains its synset structure. there are two main versions of sentiwordnet available, 1.0 and 3.0, and two straightforward methods to calculate an so value:34 use the first sense so, or average the so across senses. for the first sense method, we calculate the so value of a word w (of a given pos) based on its first sense f as follows: approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively.",1
"it is difficult to measure the coverage of our dictionaries, because there is no direct way to estimate the number of so-carrying words and expressions in english (although it should probably be larger than 5,000, the rough total of our current dictionaries). wilson, wiebe, and hoffmann (2005) provide a list of subjectivity cues with over 8,000 entries; there are many more neutral, repeated, and inflectionally related entries than in our dictionaries, however, as well as many more nouns, and far fewer adjectives. automatically generated dictionaries are generally much larger: sentiwordnet (baccianella, esuli, and sebastiani 2010) includes 38,182 non-neutral words (when the polarity of senses is averaged--see discussion in section 3.4), and the maryland dictionary (mohammad, dorr, and dunne 2009) has 76,775 words and phrases tagged for polarity. we will see, in section 3.4, that larger dictionaries are not necessarily better, in some cases because the information contained is not as detailed (the maryland dictionary is not classified by part of speech), or because, in general, including more words may also lead to including more noise. another publicly available corpus is sentiwordnet (esuli and sebastiani 2006; baccianella, esuli, and sebastiani 2010), an extension of wordnet (fellbaum 1998) where each synset is annotated with labels indicating how objective, positive, and negative the terms in the synset are. we use the average across senses for each word given in version 3.0 (see discussion in the next section). figure 9 gives the result for the the sentiwordnet dictionary (esuli and sebastiani 2006; baccianella, esuli, and sebastiani 2010), also used in the previous section, was built using wordnet (fellbaum 1998), and retains its synset structure. there are two main versions of sentiwordnet available, 1.0 and 3.0, and two straightforward methods to calculate an so value:34 use the first sense so, or average the so across senses. for the first sense method, we calculate the so value of a word w (of a given pos) based on its first sense f as follows:",1
"strappavara, carlo and rada mihalcea. 2007. semeval-2007 task 14: affective text. in proceedings of the 4th international workshop on the semantic evaluations (semeval 2007), prague.",2
"strappavara, carlo and rada mihalcea. 2007. semeval-2007 task 14: affective text. in proceedings of the 4th international workshop on the semantic evaluations (semeval 2007), prague.",2
"another issue is whether a polarity flip (switch negation) is the best way to quantify negation. though it seems to work well in certain cases (choi and cardie 2008), it fails miserably in others (liu and seneff 2009). consider excellent, a +5 adjective: if we negate it, we get not excellent, which intuitively is a far cry from atrocious, a -5 adjective. in fact, not excellent seems more positive than not good, which would negate to a -3. in order to capture these pragmatic intuitions, we implemented another method of negation, a polarity shift (shift negation). instead of changing the sign, the so value is shifted toward the opposite polarity by a fixed amount (in our current implementation, 4). thus a +2 adjective is negated to a -2, but the negation of a -3 adjective (for instance, sleazy) is only slightly positive, an effect we could call ""damning with faint praise.here are a few examples from our corpus.",1
submission received: 14 december 2009; revised submission received: 22 august 2010; accepted for publication: 28 september 2010.,1
"system for detecting and tracking opinions in on-line discussions. in working notes of the acm sigir 2001 workshop on operational text classification, pages 1-6, new york, ny.",0
"to collect data on the validity of our dictionary, we made use of amazon's mechanical turk service,24 which provides access to a pool of workers who have signed up to perform small-scale tasks that require human intelligence. mechanical turk is quickly becoming a popular resource among computational linguists, and has been used in sentiment, emotion, and subjectivity tasks (akkaya et al. 2010; mellebeek et al. 2010; mohammad and turney 2010; yano, resnik, and smith 2010), although there are mixed reports on its reliability (snow et al. 2008; callison-burch 2009; zaenen to appear).",1
"to collect data on the validity of our dictionary, we made use of amazon's mechanical turk service,24 which provides access to a pool of workers who have signed up to perform small-scale tasks that require human intelligence. mechanical turk is quickly becoming a popular resource among computational linguists, and has been used in sentiment, emotion, and subjectivity tasks (akkaya et al. 2010; mellebeek et al. 2010; mohammad and turney 2010; yano, resnik, and smith 2010), although there are mixed reports on its reliability (snow et al. 2008; callison-burch 2009; zaenen to appear).",1
"stede (2009) to lower the weight of descriptive paragraphs, as opposed to paragraphs that contain mostly commentary. the existing so-cal can be enhanced with many other sources of information. in taboada, brooke, and stede (2009) we built classifiers to distinguish among paragraphs that contained mostly description, mostly comment, a combination of the two, or meta-information (such as titles, authors, review ratings, or movie ratings). weighting paragraphs according to this classification, with lower weights assigned to description, results in a statistically-significant improvement in the polarity classification task.",1
"we must note that the comparison is different for the maryland dictionary, where we turned off negative weighting. this resulted in anomalously high performance on the movies corpus, despite poor performance elsewhere. in general, there is significantly less positive bias in the movie review domain, most likely due to the use of negative terms in plot and character description (taboada, brooke, and stede 2009), thus the negative weighting that is appropriate for other domains is often excessive for movie reviews. the main conclusion of our work is that lexicon-based methods for sentiment analysis are robust, result in good cross-domain performance, and can be easily enhanced with multiple sources of knowledge (taboada, brooke, and stede 2009). so-cal has performed well on blog postings (murray et al. 2008) and video game reviews (brooke and hurst 2009), without any need for further development or training.",1
"in order to make use of this additional information, we created separate noun, verb, and adverb dictionaries, hand-ranked using the same +5 to -5 scale as our adjective dictionary. the enhanced dictionaries contain 2,252 adjective entries, 1,142 nouns, 903 verbs, and 745 adverbs.6 the so-carrying words in these dictionaries were taken from a variety of sources, the three largest being epinions 1, the 400-text corpus described in the previous section; a 100-text subset of the 2,000 movie reviews in the polarity dataset (pang, lee, and vaithyanathan 2002; pang and lee 2004, 2005);7 and positive and negative words from the general inquirer dictionary (stone et al. 1966; stone 1997).8 the sources provide a fairly good range in terms of register: the epinions and movie reviews represent informal language, with words such as ass-kicking and nifty; at the other end of the spectrum, the general inquirer was clearly built from much more formal texts, and contributed words such as adroit and jubilant, which may be more useful in the processing of literary reviews (taboada, gillies, and mcfetridge 2006; taboada et al. 2008) or other more formal texts.",1
"dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also stone et al. 1966; tong 2001), or automatically, using seed words to expand the list of words (hatzivassiloglou and mckeown 1997; turney 2002; turney and littman 2003). much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text (hatzivassiloglou and mckeown 1997; wiebe 2000; hu and liu 2004; taboada, anthony, and voll 2006).2 first, a list of adjectives and corresponding so values is compiled into a dictionary. then, for any given text, all adjectives are extracted and annotated with their so value, using the dictionary scores. the so scores are in turn aggregated into a single score for the text. much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim in order to make use of this additional information, we created separate noun, verb, and adverb dictionaries, hand-ranked using the same +5 to -5 scale as our adjective dictionary. the enhanced dictionaries contain 2,252 adjective entries, 1,142 nouns, 903 verbs, and 745 adverbs.6 the so-carrying words in these dictionaries were taken from a variety of sources, the three largest being epinions 1, the 400-text corpus described in the previous section; a 100-text subset of the 2,000 movie reviews in the polarity dataset (pang, lee, and vaithyanathan 2002; pang and lee 2004, 2005);7 and positive and negative words from the general inquirer dictionary (stone et al. 1966; stone 1997).8 the sources provide a fairly good range in terms of register: the epinions and movie reviews represent informal language, with words such as ass-kicking and nifty; at the other end of the spectrum, the general inquirer was clearly built from much more formal texts, and contributed words such as adroit and jubilant, which may be more useful in the processing of literary reviews (taboada, gillies, and mcfetridge 2006; taboada et al. 2008) or other more formal texts. the ""marylanddictionary (mohammad, dorr, and dunne 2009) is a very large collection of words and phrases (around 70,000) extracted from the macquarie thesaurus. the dictionary is not classified according to part of speech, and only contains information on whether the word is positive or negative. to integrate it into our system, we assigned all positive words an so value of 3, and all negative words a value of -3.33 we used the same type of quantification for the general inquirer (gi; stone et al. 1966), which also has only positive and negative tags; a word was included in the dictionary if any of the senses listed in the gi were polar. sentiwordnet performs better than either the google or maryland dictionaries, but it is still somewhat low; again, we believe it suffers from the same problem of too much coverage: potentially, every word in wordnet will receive a score, and many of those are not sentiment-bearing words. the general inquirer lexicon (stone et al. 1966), the only other fully manually dictionary considered here, does comparably quite well despite being relatively small. finally, the subjectivity dictionary, with the added strong/weak distinctions, is the closest in performance to our dictionary, though significantly worse when all features are enabled. the comparison is not completely fair to the subjectivity dictionary, as it was built to recognize subjectivity, not polarity.",1
"26 this is the opposite result from the impressions reported by cabral and hortac,su (2010), where, in an",1
"lexicon-based sentiment classifiers generally show a positive bias (kennedy and inkpen 2006), likely the result of a universal human tendency to favor positive language (boucher and osgood 1969).16 in order to overcome this problem, voll and taboada (2007) implemented normalization, shifting the numerical cut-off point between positive and negative reviews. in the current version of so-cal, we have used a somewhat",1
"the classification of paragraphs into comment and description is but one of the many ways in which contextual information can be incorporated into a robust approach to sentiment extraction. in previous work (voll and taboada 2007), we showed a prototype for extracting topic sentences, and performing sentiment analysis on those only. we also showed how a sentence-level discourse parser, developed by soricut and marcu (2003), could be used to differentiate between main and secondary parts of the text. at",1
"comparing the performance of various dictionaries with or without so-cal features, two facts are apparent: first, so-cal features are generally beneficial no matter what dictionary is used (in fact, all overall improvements from basic to full in table 10 are statistically significant); the only exceptions are due to negative weighting in the movie domain, which for most of the dictionaries causes a drop in performance.35 second, the benefit provided by so-cal seems to be somewhat dependent on the reliability of the dictionary; in general, automatically derived so dictionaries derive less benefit from the use of linguistic features, and the effects are, on the whole, much less consistent; this is in fact the same conclusion we reached in other work where we compared automatically translated dictionaries to manually built ones for spanish (brooke, tofiloski, and taboada 2009). interestingly, the subjectivity dictionary performs slightly above the so-cal dictionary in some data sets when no features are enabled (which we might attribute to a mixture of basic reliability with respect to polarity and an appropriate level of coverage), but its lack of granularity seems to blunt the benefit of so-cal features, which were designed to take advantage of a finer-grained so scale, an effect which is even more pronounced in binary dictionaries like the gi. we can summarize this result as follows: when using lexical methods, the effectiveness of any linguistic enhancements will to some extent depend on the characteristics of the underlying lexicon and, as such, the two cannot be considered in isolation. in related work, we have also shown that creating a new version of so-cal for a new language, spanish, is as fast as building text classifiers for the new language, and results in better performance (brooke 2009; brooke, tofiloski, and taboada 2009). so-cal has also been successfully deployed for the detection of sentence-level polarity (brooke and hurst 2009).",1
"in order to make use of this additional information, we created separate noun, verb, and adverb dictionaries, hand-ranked using the same +5 to -5 scale as our adjective dictionary. the enhanced dictionaries contain 2,252 adjective entries, 1,142 nouns, 903 verbs, and 745 adverbs.6 the so-carrying words in these dictionaries were taken from a variety of sources, the three largest being epinions 1, the 400-text corpus described in the previous section; a 100-text subset of the 2,000 movie reviews in the polarity dataset (pang, lee, and vaithyanathan 2002; pang and lee 2004, 2005);7 and positive and negative words from the general inquirer dictionary (stone et al. 1966; stone 1997).8 the sources provide a fairly good range in terms of register: the epinions and movie reviews represent informal language, with words such as ass-kicking and nifty; at the other end of the spectrum, the general inquirer was clearly built from much more formal texts, and contributed words such as adroit and jubilant, which may be more useful in the processing of literary reviews (taboada, gillies, and mcfetridge 2006; taboada et al. 2008) or other more formal texts.",1
"university of british columbia at tac 2008. in proceedings of tac 2008, gaithersburg, md.",1
"following osgood, suci, and tannenbaum (1957), the calculation of sentiment in so-cal begins with two assumptions: that individual words have what is referred to as prior polarity, that is, a semantic orientation that is independent of context; and that said semantic orientation can be expressed as a numerical value. several lexiconbased approaches have adopted these assumptions (bruce and wiebe 2000; hu and liu 2004; kim and hovy 2004). in this section, we describe the different dictionaries used in so-cal, and the incorporation of valence shifters. we conclude the section with tests that show so-cal's performance on different data sets.",1
"towards a subject topic, person, or idea (osgood, suci, and tannenbaum 1957). when used in the analysis of public opinion, such as the automated interpretation of on-line product reviews, semantic orientation can be extremely helpful in marketing, measures of popularity and success, and compiling reviews.",1
"hoffmann. 2005. recognizing contextual polarity in phrase-level sentiment analysis. in proceedings of the 2005 human language technology conference and the conference on empirical methods in natural language processing (hlt/emnlp-05), pages 347-354, vancouver.",1
"the affective text data from rada mihalcea and carlo strappavara was used in the 2007 semeval task. it contains 1,000 news headlines annotated with a range between -100 (very negative) and 100 (very positive). we excluded six headlines that had been labeled as 0 (therefore neutral), yielding 468 positive and 526 negative headlines. in addition to the full evaluation, strappavara and mihalcea (2007) also propose a coarse evaluation, where headlines with scores -100 to -50 are classified as negative, and those 50 to 100 as positive. excluding the headlines in the middle gives us 155 positive headlines and 255 negative ones.",2
"zaidan, omar f. and jason eisner. 2008. modeling annotators: a generative approach to learning from annotator rationales. in proceedings of the 2008 conference on empirical methods in natural language processing, pages 31-40, honolulu, hi.",1
"callison-burch, chris. 2009. fast, cheap and creative: evaluating translation quality using amazon's mechanical turk. in proceedings of the 2009 conference on empirical methods in natural language processing, pages 286-295, singapore. chafe, wallace and johanna nichols. 1986. evidentiality: the linguistic coding of epistemology. ablex, norwood, nj. greene, stephan and philip resnik. 2009. more than words: syntactic packaging and implicit sentiment. in proceedings of human language technologies: the 2009 parse-and-paraphrase paradigm. in proceedings of the 2009 conference on empirical methods in natural language processing, pages 161-169, singapore. lyons, john. 1981. language, meaning and",1
"a distinction between sentences with a first person subject and with other subjects. other hybrid methods include those of andreevskaia and bergler (2008), dang, zhang, and chen (2010), dasgupta and ng (2009), goldberg and zhu (2006), or prabowo and thelwall (2009). wan (2009) uses co-training in a method that uses english labeled data and an english classifier to learn a classifier for chinese.",1
17 one of the reviewers points out that this is similar to the use of term frequency (tf-idf) in information retrieval (salton and mcgill 1983). see also paltoglou and thelwall (2010) for a use of information retrieval techniques in sentiment analysis.,1
"we tested so-cal with four different data sets: the multi-perspective question answering (mpqa) corpus, version 2.0 (wiebe, wilson, and cardie 2005); a collection of myspace.com comments from mike thelwall (prabowo and thelwall 2009); a set of news and blog posts from alina andreevskaia (andreevskaia and bergler 2008); and a set of headlines from rada mihalcea and carlo strappavara (strappavara and mihalcea 2007).20",1
"23 our results are not comparable to those of thelwall et al. (2010) on the myspace comments, as they",1
"ours is not the only method that uses linguistic information or dictionaries. many other systems make use of either the subjectivity dictionary of wiebe and colleagues, or of sentiwordnet (devitt and ahmad 2007; thet et al. 2009), and some work relies on appraisal theory (whitelaw, garg, and argamon 2005; bloom, garg, and argamon 2007), a theory developed by martin and white (2005). we also discuss, in section 2.4, work on incorporating linguistic insights for the treatment of negation (moilanen and pulman 2007; choi and cardie 2008).",1
"12 the discourse connective but plays a role in linking clauses and sentences in a rhetorical relation (mann and thompson 1988). there are more sophisticated ways of making use of those relations, but we have not implemented them yet. two other features merit discussion: weighting and multiple cut-offs. first of all, socal incorporates an option to assign different weights to sentences or portions of a text. taboada and grieve (2004) improved performance of an earlier version of the so calculator by assigning the most weight at the two-thirds mark of a text, and significantly less at the beginning. the current version has a user-configurable form of this weighting system, allowing any span of the text (with the end points represented by fractions of the entire text) to be given a certain weight. an even more flexible and powerful system is provided by the xml weighting option. when this option is enabled, xml tag pairs in the text (e.g., <topic>, </topic>) can be used as a signal to the calculator that any words appearing between these tags should be multiplied by a certain given weight. this gives so-cal an interface to outside modules. for example, one module could pre-process the text and tag spans that are believed to be topic sentences, another module could provide discourse information such as rhetorical relations (mann and thompson 1988), and a third module could label the sentences that seem to be subjective. armed with this information, so-cal can disregard or de-emphasize parts of the text that are less relevant to sentiment analysis. this weighting feature is used in taboada, brooke, and",1
"dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also stone et al. 1966; tong 2001), or automatically, using seed words to expand the list of words (hatzivassiloglou and mckeown 1997; turney 2002; turney and littman 2003). much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text (hatzivassiloglou and mckeown 1997; wiebe 2000; hu and liu 2004; taboada, anthony, and voll 2006).2 first, a list of adjectives and corresponding so values is compiled into a dictionary. then, for any given text, all adjectives are extracted and annotated with their so value, using the dictionary scores. the so scores are in turn aggregated into a single score for the text.",1
"2 with some exceptions: turney (2002) uses two-word phrases; whitelaw, garg, and argamon (2005) 5 something that turney (2002) already partially addressed, by extracting two-word phrases. 6 each dictionary also has associated with it a stop-word list. for instance, the adjective dictionary has a stop-word list that includes more, much, andmany , which are tagged as adjectives by the brill tagger.",1
"it is important to note that how a dictionary is created affects the overall accuracy of subsequent results. in taboada, anthony, and voll (2006) we report on experiments using different search engines and operators in trying to create dictionaries semiautomatically. we found that, although usable, dictionaries created using the google search engine were unstable. when rerun, the results for each word were subject to change, sometimes by extreme amounts, something that kilgarriff (2007) also notes, arguing against the use of google for linguistic research of this type. an alternative would be to use a sufficiently large static corpus, as turney (2006) does to measure relational similarity across word pairs.",1
"there exist two main approaches to the problem of extracting sentiment automatically.1 the lexicon-based approach involves calculating orientation for a document from the semantic orientation of words or phrases in the document (turney 2002). the text classification approach involves building classifiers from labeled instances of texts or sentences (pang, lee, and vaithyanathan 2002), essentially a supervised classification task. the latter approach could also be described as a statistical or machine-learning approach. we follow the first method, in which we use dictionaries of words annotated with the word's semantic orientation, or polarity. dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also stone et al. 1966; tong 2001), or automatically, using seed words to expand the list of words (hatzivassiloglou and mckeown 1997; turney 2002; turney and littman 2003). much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text (hatzivassiloglou and mckeown 1997; wiebe 2000; hu and liu 2004; taboada, anthony, and voll 2006).2 first, a list of adjectives and corresponding so values is compiled into a dictionary. then, for any given text, all adjectives are extracted and annotated with their so value, using the dictionary scores. the so scores are in turn aggregated into a single score for the text. much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim and hovy 2004; esuli and sebastiani 2006). the dictionary may also be produced automatically via association, where the score for each new adjective is calculated using the frequency of the proximity of that adjective with respect to one or more seed words. seed words are a small set of words with strong negative or positive associations, such as excellent or abysmal. in principle, a positive adjective should occur more frequently alongside the positive seed words, and thus will obtain a positive score, whereas negative adjectives will occur most often in the vicinity of negative seed words, thus obtaining a negative score. the association is usually calculated following turney's method for computing mutual information (turney 2002; turney and littman 2003), but see also rao and ravichandran (2009) and velikovich et al. (2010) for other methods using seed words. table 6 shows the accuracy of the so-cal system across different review types in epinions 1 and epinions 2 (the latter unseen data), including the f-measure for classification of positive and negative texts. the table shows that so-cal's performance on positive reviews is generally better than on negative reviews (run with all options and shifted negation). this is despite the fact that all of our dictionaries contain far more negative words than positive ones. as noted already by boucher and osgood (1969), there is a strong preference for avoiding negation and negative terms even when expressing negative opinions, making the detection of text sentiment difficult for systems which depend solely on these indicators (also see results in dave, lawrence, and pennock 2003, turney 2002). the exception are books and movies, where performance is more even across positive and negative, or often better in negative reviews. we hypothesize this is because consumer product reviews contain more factual information that the reader is required to interpret as positive or negative (for instance, the range for a cordless phone or the leg room in the back seat of a car). some of that factual information may be implicitly negative, and therefore missed by so-cal. our first comparison is with the dictionary of adjectives that was derived using the so-pmi method (turney 2002), using google hit counts (taboada, anthony, and voll 2006). the so values for the words tested here vary from 8.2 to -5.74. we have already noted in section 2 that using this dictionary instead of our manually-ranked one has a strongly negative effect on performance. because the google dictionary is continuous, we place the individual so values into evenly spaced buckets so that we can graph their distribution. for easy comparison with our dictionary, we present the results when buckets equivalent to our 11-point so scale are used. the results for the single-word task are given in figure 5.29",0
"to collect data on the validity of our dictionary, we made use of amazon's mechanical turk service,24 which provides access to a pool of workers who have signed up to perform small-scale tasks that require human intelligence. mechanical turk is quickly becoming a popular resource among computational linguists, and has been used in sentiment, emotion, and subjectivity tasks (akkaya et al. 2010; mellebeek et al. 2010; mohammad and turney 2010; yano, resnik, and smith 2010), although there are mixed reports on its reliability (snow et al. 2008; callison-burch 2009; zaenen to appear).",1
"pang, lee, and vaithyanathan (2002) found that their machine-learning classifier performed better when a binary feature was used indicating the presence of a unigram in the text, instead of a numerical feature indicating the number of appearances. counting each word only once does not seem to work equally well for word-counting models. we have, however, improved overall performance by decreasing the weight of words that appear more often in the text: the nth appearance of a word in the text will have only 1/n of its full so value.17 consider the following invented example.",1
"there exist two main approaches to the problem of extracting sentiment automatically.1 the lexicon-based approach involves calculating orientation for a document from the semantic orientation of words or phrases in the document (turney 2002). the text classification approach involves building classifiers from labeled instances of texts or sentences (pang, lee, and vaithyanathan 2002), essentially a supervised classification task. the latter approach could also be described as a statistical or machine-learning approach. we follow the first method, in which we use dictionaries of words annotated with the word's semantic orientation, or polarity. the majority of the statistical text classification research builds support vector machine classifiers, trained on a particular data set using features such as unigrams or bigrams, and with or without part-of-speech labels, although the most successful features seem to be basic unigrams (pang, lee, and vaithyanathan 2002; salvetti, reichenbach, and lewis 2006). classifiers built using supervised methods reach quite a high accuracy in detecting the polarity of a text (chaovalit and zhou 2005; kennedy and inkpen 2006; boiy et al. 2007; bartlett and albright 2008). however, although such classifiers perform very well in the domain that they are trained on, their performance drops precipitously (almost to chance) when the same classifier is used in in order to make use of this additional information, we created separate noun, verb, and adverb dictionaries, hand-ranked using the same +5 to -5 scale as our adjective dictionary. the enhanced dictionaries contain 2,252 adjective entries, 1,142 nouns, 903 verbs, and 745 adverbs.6 the so-carrying words in these dictionaries were taken from a variety of sources, the three largest being epinions 1, the 400-text corpus described in the previous section; a 100-text subset of the 2,000 movie reviews in the polarity dataset (pang, lee, and vaithyanathan 2002; pang and lee 2004, 2005);7 and positive and negative words from the general inquirer dictionary (stone et al. 1966; stone 1997).8 the sources provide a fairly good range in terms of register: the epinions and movie reviews represent informal language, with words such as ass-kicking and nifty; at the other end of the spectrum, the general inquirer was clearly built from much more formal texts, and contributed words such as adroit and jubilant, which may be more useful in the processing of literary reviews (taboada, gillies, and mcfetridge 2006; taboada et al. 2008) or other more formal texts. approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively.",1
"14 full parsing is also an option, but the speed of the parser could pose problems if the goal is to process text on-line. parsing would still produce ambiguities, and may not be able to correctly interpret scope. another option is to use parser results to learn the scope (councill, mcdonald, and velikovich 2010).",1
"and hovy 2004; esuli and sebastiani 2006). the dictionary may also be produced automatically via association, where the score for each new adjective is calculated using the frequency of the proximity of that adjective with respect to one or more seed words. seed words are a small set of words with strong negative or positive associations, such as excellent or abysmal. in principle, a positive adjective should occur more frequently alongside the positive seed words, and thus will obtain a positive score, whereas negative adjectives will occur most often in the vicinity of negative seed words, thus obtaining a negative score. the association is usually calculated following turney's method for computing mutual information (turney 2002; turney and littman 2003), but see also rao and ravichandran (2009) and velikovich et al. (2010) for other methods using seed words. automatically or semi-automatically created dictionaries have some advantages. we found many novel words in our initial google-generated dictionary. for instance, unlistenable was tagged accurately as highly negative, an advantage that velikovich et al. (2010) point out. however, in light of the lack of stability for automatically generated dictionaries, we decided to create manual ones. these were produced by hand-tagging all adjectives found in our development corpus, a 400-text corpus of reviews (see the following) on a scale ranging from -5 for extremely negative to +5 for extremely positive, where 0 indicates a neutral word (excluded from our dictionaries). ""positiveand ""negativewere decided on the basis of the word's prior polarity, that is, its meaning in most contexts. we do not deal with word sense disambiguation but suspect that using even a simple method to disambiguate would be beneficial. some word sense ambiguities are addressed by taking part of speech into account. for instance, as we mention in section 3.4, plot is only negative when it is a verb, but should not be so in a noun dictionary; novel is a positive adjective, but a neutral noun.",0
"it is important to note that how a dictionary is created affects the overall accuracy of subsequent results. in taboada, anthony, and voll (2006) we report on experiments using different search engines and operators in trying to create dictionaries semiautomatically. we found that, although usable, dictionaries created using the google search engine were unstable. when rerun, the results for each word were subject to change, sometimes by extreme amounts, something that kilgarriff (2007) also notes, arguing against the use of google for linguistic research of this type. an alternative would be to use a sufficiently large static corpus, as turney (2006) does to measure relational similarity across word pairs. to build the system and run our experiments, we use the corpus described in taboada and grieve (2004) and taboada, anthony, and voll (2006), which consists of a 400-text collection of epinions reviews extracted from eight different categories: books, cars, computers, cookware, hotels, movies, music, and phones, a corpus we named ""epinions 1.within each collection, the reviews were split into 25 positive and 25 the first dictionary that we incorporated into so-cal was the google-generated pmi-based dictionary described in taboada, anthony, and voll (2006), and mentioned earlier in this article.",1
"dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also stone et al. 1966; tong 2001), or automatically, using seed words to expand the list of words (hatzivassiloglou and mckeown 1997; turney 2002; turney and littman 2003). much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text (hatzivassiloglou and mckeown 1997; wiebe 2000; hu and liu 2004; taboada, anthony, and voll 2006).2 first, a list of adjectives and corresponding so values is compiled into a dictionary. then, for any given text, all adjectives are extracted and annotated with their so value, using the dictionary scores. the so scores are in turn aggregated into a single score for the text. much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (hatzivassiloglou and mckeown 1997; hu and liu 2004; taboada, anthony, and voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (benamara et al. 2007); adjectives and verbs (kim and hovy 2004); adjective phrases (whitelaw, garg, and argamon 2005); two-word phrases (turney 2002; turney and littman 2003); adjectives, verbs, and adverbs (subrahmanian and reforgiato 2008); the exclusive use of verbs (sokolova and lapalme 2008); the use of non-affective adjectives and adverbs (sokolova and lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (zaidan and eisner 2008). in general, the so of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). the dictionary can be created in different ways: manually, using existing dictionaries such as the general inquirer (stone et al. 1966), or semi-automatically, making use of resources like wordnet (hu and liu 2004; kim previous versions of so-cal (taboada and grieve 2004; taboada, anthony, and voll 2006) relied on an adjective dictionary to predict the overall so of a document, using a simple aggregate-and-average method: the individual scores for each adjective in a document are added together and then divided by the total number of adjectives in that document.4 as we describe subsequently, the current version of so-cal takes other parts of speech into account, and makes use of more sophisticated methods to determine the true contribution of each word. our first comparison is with the dictionary of adjectives that was derived using the so-pmi method (turney 2002), using google hit counts (taboada, anthony, and voll 2006). the so values for the words tested here vary from 8.2 to -5.74. we have already noted in section 2 that using this dictionary instead of our manually-ranked one has a strongly negative effect on performance. because the google dictionary is continuous, we place the individual so values into evenly spaced buckets so that we can graph their distribution. for easy comparison with our dictionary, we present the results when buckets equivalent to our 11-point so scale are used. the results for the single-word task are given in figure 5.29 table 10 shows the performance of the various dictionaries when run within so-cal. for all dictionaries and corpora, the performance of the original so-cal dictionary is significantly better (p < 0.05). we have already discussed the google dictionary, which contains only adjectives, and whose results are not reliable (see also taboada, anthony, and voll 2006). the maryland dictionary suffers from too much coverage: most words in a text are identified by this dictionary as containing some form of subjectivity or opinion, but a cursory examination of the texts reveals that this is not the case. in some cases, the problem is part-of-speech assignment (the maryland dictionary is not classified according to part of speech). for example, the noun plot was classified as negative when referring to a movie's plot. we imagine this is negative in the dictionary because of the negative meaning of the verb plot. similarly, novel as a noun is classified as positive, although we believe this ought to be the case in the adjective use only. more problematic is the presence of words such as book, cotton, here, legal, reading, saying, oryear . approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively.",0
"a distinction between sentences with a first person subject and with other subjects. other hybrid methods include those of andreevskaia and bergler (2008), dang, zhang, and chen (2010), dasgupta and ng (2009), goldberg and zhu (2006), or prabowo and thelwall (2009). wan (2009) uses co-training in a method that uses english labeled data and an english classifier to learn a classifier for chinese.",1
"yue zhang. 2010. packed feelings and ordered sentiments: sentiment parsing with quasi-compositional polarity sequencing and compression. in proceedings of the 1st workshop on computational approaches to subjectivity and sentiment analysis (wassa 2010), pages 36-43, lisbon.",1
"ours is not the only method that uses linguistic information or dictionaries. many other systems make use of either the subjectivity dictionary of wiebe and colleagues, or of sentiwordnet (devitt and ahmad 2007; thet et al. 2009), and some work relies on appraisal theory (whitelaw, garg, and argamon 2005; bloom, garg, and argamon 2007), a theory developed by martin and white (2005). we also discuss, in section 2.4, work on incorporating linguistic insights for the treatment of negation (moilanen and pulman 2007; choi and cardie 2008).",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"the analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (pang and lee 2008), subjectivity (lyons 1981; langacker 1985), opinion mining (pang and lee 2008), analysis of stance (biber and finegan 1988; conrad and biber 2000), appraisal (martin and white 2005), point of view (wiebe 1994; scheibman 2002), evidentiality (chafe and nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (ketal 1975; ortony, clore, and collins 1988) and affect (batson, shaw, and oleson 1992). in this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal.",1
"dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also stone et al. 1966; tong 2001), or automatically, using seed words to expand the list of words (hatzivassiloglou and mckeown 1997; turney 2002; turney and littman 2003). much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text (hatzivassiloglou and mckeown 1997; wiebe 2000; hu and liu 2004; taboada, anthony, and voll 2006).2 first, a list of adjectives and corresponding so values is compiled into a dictionary. then, for any given text, all adjectives are extracted and annotated with their so value, using the dictionary scores. the so scores are in turn aggregated into a single score for the text. following osgood, suci, and tannenbaum (1957), the calculation of sentiment in so-cal begins with two assumptions: that individual words have what is referred to as prior polarity, that is, a semantic orientation that is independent of context; and that said semantic orientation can be expressed as a numerical value. several lexiconbased approaches have adopted these assumptions (bruce and wiebe 2000; hu and liu 2004; kim and hovy 2004). in this section, we describe the different dictionaries used in so-cal, and the incorporation of valence shifters. we conclude the section with tests that show so-cal's performance on different data sets.",1
"approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively.",1
"approaches have emerged: the use of machine-learning classifiers trained on n-grams or similar features (pang, lee, and vaithyanathan 2002), and the use of sentiment dictionaries (esuli and sebastiani 2006; taboada, anthony, and voll 2006). support vector machine (svm) classifiers have been shown to outperform lexicon-based models within a single domain (kennedy and inkpen 2006); they have trouble with cross-domain tasks (aue and gamon 2005), however, and some researchers have argued for hybrid classifiers (andreevskaia and bergler 2008). although some of the machine-learningbased work makes use of linguistic features for training (riloff and wiebe 2003; mullen and collier 2004; wiebe et al. 2004; kennedy and inkpen 2006; ng, dasgupta, and niaz arifin 2006; sokolova and lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. the results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively.",1
"dave, kushal, steve lawrence, and david m. pennock. 2003. mining the peanut gallery: opinion extraction and semantic classification of product reviews. in proceedings of the twelfth international world wide web conference (www 2003), pages 519-528, budapest.",2
"another area where the lexicon-based model might be preferable to a classifier model is in simulating the effect of linguistic context. on reading any document, it becomes apparent that aspects of the local context of a word need to be taken into account in so assessment, such as negation (e.g., not good) and intensification (e.g., very good), aspects that polanyi and zaenen (2006) named contextual valence shifters. research by kennedy and inkpen (2006) concentrated on implementing those insights. they dealt with negation and intensification by creating separate features, namely, the appearance of good might be either good (no modification) not good (negated good), int good (intensified good), or dim good (diminished good). the classifier, however, cannot determine that these four types of good are in any way related, and so in order to train accurately there must be enough examples of all four in the training corpus. moreover, we show in section 2.4 that expanding the scope to two-word phrases does not deal with negation adequately, as it is often a long-distance phenomenon. recent work has begun to address this issue. for instance, choi and cardie (2008) present a classifier that treats negation from a compositional point of view by first calculating polarity of terms independently, and then applying inference rules to arrive at a combined polarity score. as we shall see in section 2, our lexicon-based model handles negation and intensification in a way that generalizes to all words that have a semantic orientation value. in the following example, adapted from polanyi and zaenen (2006), we see that lexical items other than adjectives can carry important semantic polarity information. the simple dictionary is a version of our main dictionary that has been simplified to 2/-2 values, switch negation, and 1/-1 intensification, following polanyi and zaenen (2006). only-adj excludes dictionaries other than our main adjective dictionary, and the one-word dictionary uses all the dictionaries, but disregards multi-word expressions. asterisks indicate a statistically-significant difference using chi-square tests, with respect to the full version of so-cal, with all features enabled and at default settings. to creating hand-ranked, fine-grained, multiple-part-of-speech dictionaries for lexicon-based sentiment analysis; the full dictionary outperforms all but the one-word dictionary to a significant degree (p < 0.05) in the corpora as a whole. it is important to note that some of the parameters and features that we have described so far (the fixed number for negative shifting, percentages for intensifiers, negative weighting, etc.) were fine-tuned in the process of creating the software, mostly by developing and testing on epinions 1. once we were theoretically and experimentally satisfied that the features were reasonable, we tested the final set of parameters on the other corpora. the so-cal improvements described in this article have been directly inspired by the work of polanyi and zaenen (2006), who proposed that ""valence shifterschange the base value of a word. we have implemented their idea in the form of intensifiers and downtoners, adding a treatment of negation that does not involve switching polarity, but instead shifting the value of a word when in the scope of a negator.",1
"quirk et al. (1985) classify intensifiers into two major categories, depending on their polarity: amplifiers (e.g., very) increase the semantic intensity of a neighboring lexical item, whereas downtoners (e.g., slightly) decrease it. some researchers in sentiment analysis (kennedy and inkpen 2006; polanyi and zaenen 2006) have implemented",1
"table 7 shows the results of the evaluation. included in the table is a baseline for each data set, assigning polarity to the most frequent class for the data. these data sets include much smaller spans of text than are found in consumer reviews, with some sentences or headlines not containing any words from the so-cal dictionaries. this ranged from about 21% of the total in the myspace comments to 54% in the headlines.21 two approaches were used in this cross-domain evaluation when so-cal encountered texts for which it found no words in its dictionaries (so-empty texts). first, the backoff method involves using the most frequent polarity for the corpus (or positive, when they are equal), and assigning that polarity to all so-empty texts. this method provides results that can be directly compared to other results on these data sets, although, like the baseline, it assumes some knowledge about the polarity balance of the corpus. the figures in the first section of table 7 suggest robust performance as compared to a mostfrequent-class baseline, including modest improvement over the relevant cross-domain results of andreevskaia and bergler (2008).22 moilanen, pulman, and zhang (2010) also use the headlines data, and obtain a polarity classification accuracy of 77.94% below our results excluding empty.23",1
"the majority of the statistical text classification research builds support vector machine classifiers, trained on a particular data set using features such as unigrams or bigrams, and with or without part-of-speech labels, although the most successful features seem to be basic unigrams (pang, lee, and vaithyanathan 2002; salvetti, reichenbach, and lewis 2006). classifiers built using supervised methods reach quite a high accuracy in detecting the polarity of a text (chaovalit and zhou 2005; kennedy and inkpen 2006; boiy et al. 2007; bartlett and albright 2008). however, although such classifiers perform very well in the domain that they are trained on, their performance drops precipitously (almost to chance) when the same classifier is used in",1
"a distinction between sentences with a first person subject and with other subjects. other hybrid methods include those of andreevskaia and bergler (2008), dang, zhang, and chen (2010), dasgupta and ng (2009), goldberg and zhu (2006), or prabowo and thelwall (2009). wan (2009) uses co-training in a method that uses english labeled data and an english classifier to learn a classifier for chinese.",1
"in our treatment of negation, we consider mostly negators, but not negative polarity items (npis), such as any, anything, ever, or at all. in some cases, searching for an npi would be more effective than searching for a negator. npis occur in negative sentences, but also in nonveridical contexts (zwarts 1995; giannakidou 1998), which also affect semantic orientation. for instance, any occurs in contexts other than negative sentences, as shown in example (11), from giannakidou (2001, page 99), where in all cases the presence of any is due to a nonveridical situation. using npis would allow us to reduce semantic orientation values in such contexts. we address some of these issues through irrealis blocking, as we explain in the next section.",1
"in contrast, a svm classifier, like other parametric approaches, such as naive bayes (govert, lalmas, & fuhr, 1999), rocchio (ittner, lewis, & ahn, 1995), and neural network (yin and savio, 1996) classifiers, regards a document collection as a set of significant features, each of which is assigned a weight, and each document is represented by a feature set. therefore, given a training set, the associated model focuses on optimising the weights and other parameters required, such that the model can achieve a high level of effectiveness on an unseen sample set. here, the independence of features is assumed, which is not always true, as explained in lewis (1998) and belew (2000). in addition, we sometimes have to make a trade-off between the effectiveness of the model and the time needed to train the model. an efficient algorithm is usually required to train the model. the associated classifier simply makes use of the model containing the learned weights and parameters to classify an unseen sample set.",1
"where i = {a . . . d} represents the value of each cell in a 2 x 2 contingency table. the yates continuity correction is applied to each (cid:2)2 calculation as the degree of freedom is 1. the (cid:2)2 calculation used in this experiment does not approximate the (cid:2)2 value, such as described in yang and pedersen (1997) and swan and allan (2000). the larger a (cid:2)2value, the stronger the evidence to reject the null hypothesis, which means that word and antecedent are dependent on each other. for the (cid:2)2-test, in order to reliably accept or reject h0, the expected values should be > 5. otherwise, it tends to underestimate small probabilities, which incorrectly results in accepting h1 (cochran, 1954).",1
"the first, simplest rule set was based on 3672 pre-classified words found in the general inquirer lexicon (stone, dunphy, smith, & ogilvie, 1966), 1598 of which were pre-classified as positive and 2074 of which were pre-classified as negative. here, each rule depends solely on one sentiment bearing word representing an antecedent. we implemented a general inquirer based classifier (gibc) that applied the rule set to classify document collections.",1
"market-sentinel. (2007). market sentinel. ! http://www.marketsentinel.com? accessed 4 october 2007. mettrop, w., & nieuwenbuysen, p. (2001). internet search engines: fluctuations in document accessibility. journal of documentation, 57(5), 623-651. miller, g. a. (1995). wordnet: a lexical database for english. communications of the acm, 38(11), 39-41. myspace. (2007). myspace. ! http://www.myspace.com? accessed april 2007. nasukawa, t., & yi, j. (2003, october 23-25). sentiment analysis: capturing favorability using natural language processing. in proceedings of the 2nd",1
"pang, b. (2007). polarity data set v2.0. october 1997. ! http://www.cs.cornell.edu/people/pabo/moviereview-data/? accessed 4 august 2007. pang, b., & lee, l. (2004, july 21-26). a sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. in proceedings",1
"in contrast, if we cannot hire human classifiers to produce a training set, we can apply the sbc and a closeness measure to use a corpus to determine the consequents of antecedents. this has three advantages. first, no human classifiers are required to produce a training set. hence, it significantly reduces cost and time. second, the sbc can show the antecedents as the reason for selecting an appropriate category, and assign the associated antecedent to the category. third, by using a human classifier to judge the assigned consequent, we can build a rule database that grows over time. to build this rule database, we created a sentiment analysis tool (sat) that can assist a human classifier in checking the correctness of a new rule and integrating it into the existing rule database if it does not exist. by using this rule database, the sat can also build a training set, which can be exploited by the svm to classify the documents that cannot be classified by both the rbc and sbc. in this respect, the use of rbc, sbc, and svm in a hybrid and semi-automatic manner can be interpreted as a complementary approach, i.e., each classifier contributes to other classifiers to achieve a good level of effectiveness, as illustrated in fig. 4. the sbc and sat provide the rbc with a rule database, and the svm with a training set. as a result, the rbc and svm can assist the sbc to achieve a better level of effectiveness and efficiency. the problem arises if we do not have our own relatively large corpus, because we will overload a search engine with a huge amount of queries (an ethical issue) and spend a lot of time to collect the hit counts required (an efficiency issue). another problem is to deal with both the coverage level and fluctuation of a search engine, which can affect the effectiveness of the sbc as shown in table 8. the coverage issues are discussed in bar-ilan (2001) and thelwall (2000), and the fluctuation issue in bar-ilan (1999) and mettrop and nieuwenhuysen (2001). the fluctuation and ethical issues are the main reasons for not being able to collect the hit counts for the s1 sample set. this means that no classifier outperforms other classifiers: they need each other to achieve the best performance. in our experimental setting, we decided to assign a document one sentiment only (binary classification), so that the f1 measure could be applied without the risk of over-fitting. the rule-based classifiers used can carry out a multiple classification, however, i.e., assigning a document more than one category. the classifiers classify each sentence within a document, and then rank all possible sentiments in descending order. for a binary classification, the classifiers only select the top rank. for a multiple classification, the classifiers can select the best n categories, where n >= 1. a further extension of the system would be to use it to create fuzzy classifications (kuncheva, 2000), i.e., assigning sentiment on a probability rather than a binary basis. fuzzy hybrid classification has been achieved before (ishibuchi, nakashima, & kuroda, 2000) and so this should be possible for our system.",0
"in contrast, if we cannot hire human classifiers to produce a training set, we can apply the sbc and a closeness measure to use a corpus to determine the consequents of antecedents. this has three advantages. first, no human classifiers are required to produce a training set. hence, it significantly reduces cost and time. second, the sbc can show the antecedents as the reason for selecting an appropriate category, and assign the associated antecedent to the category. third, by using a human classifier to judge the assigned consequent, we can build a rule database that grows over time. to build this rule database, we created a sentiment analysis tool (sat) that can assist a human classifier in checking the correctness of a new rule and integrating it into the existing rule database if it does not exist. by using this rule database, the sat can also build a training set, which can be exploited by the svm to classify the documents that cannot be classified by both the rbc and sbc. in this respect, the use of rbc, sbc, and svm in a hybrid and semi-automatic manner can be interpreted as a complementary approach, i.e., each classifier contributes to other classifiers to achieve a good level of effectiveness, as illustrated in fig. 4. the sbc and sat provide the rbc with a rule database, and the svm with a training set. as a result, the rbc and svm can assist the sbc to achieve a better level of effectiveness and efficiency. the problem arises if we do not have our own relatively large corpus, because we will overload a search engine with a huge amount of queries (an ethical issue) and spend a lot of time to collect the hit counts required (an efficiency issue). another problem is to deal with both the coverage level and fluctuation of a search engine, which can affect the effectiveness of the sbc as shown in table 8. the coverage issues are discussed in bar-ilan (2001) and thelwall (2000), and the fluctuation issue in bar-ilan (1999) and mettrop and nieuwenhuysen (2001). the fluctuation and ethical issues are the main reasons for not being able to collect the hit counts for the s1 sample set. this means that no classifier outperforms other classifiers: they need each other to achieve the best performance. in our experimental setting, we decided to assign a document one sentiment only (binary classification), so that the f1 measure could be applied without the risk of over-fitting. the rule-based classifiers used can carry out a multiple classification, however, i.e., assigning a document more than one category. the classifiers classify each sentence within a document, and then rank all possible sentiments in descending order. for a binary classification, the classifiers only select the top rank. for a multiple classification, the classifiers can select the best n categories, where n >= 1. a further extension of the system would be to use it to create fuzzy classifications (kuncheva, 2000), i.e., assigning sentiment on a probability rather than a binary basis. fuzzy hybrid classification has been achieved before (ishibuchi, nakashima, & kuroda, 2000) and so this should be possible for our system.",0
"in contrast, a svm classifier, like other parametric approaches, such as naive bayes (govert, lalmas, & fuhr, 1999), rocchio (ittner, lewis, & ahn, 1995), and neural network (yin and savio, 1996) classifiers, regards a document collection as a set of significant features, each of which is assigned a weight, and each document is represented by a feature set. therefore, given a training set, the associated model focuses on optimising the weights and other parameters required, such that the model can achieve a high level of effectiveness on an unseen sample set. here, the independence of features is assumed, which is not always true, as explained in lewis (1998) and belew (2000). in addition, we sometimes have to make a trade-off between the effectiveness of the model and the time needed to train the model. an efficient algorithm is usually required to train the model. the associated classifier simply makes use of the model containing the learned weights and parameters to classify an unseen sample set.",1
4. hybrid classification the idea of hybrid classification was used in konig and brill (2006). section 4.3 describes our hybrid konig and brill (2006),1
"whilst most researchers focus on assigning sentiments to documents, others focus on more specific tasks: finding the sentiments of words (hatzivassiloglou & mckeown, 1997), subjective expressions (kim & hovy, 2004; wilson, wiebe, & hoffmann, 2005), subjective sentences (pang & lee, 2004) and topics (hiroshi, tetsuya, & hideo, 2004; nasukawa & yi, 2003; yi, nasukawa, niblack, & bunescu, 2003). these tasks analyse sentiment at a fine-grained level and can be used to improve the effectiveness of sentiment classification, as shown in pang and lee (2004). instead of carrying out a sentiment classification or an opinion extraction, choi, cardie, riloff, and patwardhan (2005) focus on extracting the sources of opinions, e.g., the persons or organizations who play a crucial role in influencing other individuals' opinions. various data sources have been used, ranging from product reviews, customer feedback, the document understanding conference (duc) corpus, the multi-perspective question answering (mpqa) corpus and the wall street journal (wsj) corpus.",0
"micro-averaging treats each document equally. this means that micro-averaging results in averaging over a set of documents. the performance of a classifier tends to be dominated by common classes. in contrast, macro-averaging treats each class equally. this means that macro-averaging results in averaging over a set of classes. the performance of a classifier tends to be dominated by infrequent classes. one class that results in a bad performance can deteriorate the overall performance significantly. hence, it is common that macro-averaged performance is lower than micro-averaged performance, as shown in a classification performance evaluation conducted by yang and liu (1999) and calvo and ceccatto (2000).",1
"we used support vector machine (svmlight) v6.01 (joachims, 1998). as explained in dumais and chen (2000) and pang et al. (2002) given a category set, c = {+1, -1} and two pre-classified training sets, i.e., a positive sample set, t + i=1(di, +1) and a negative sample set, t i=1(di, -1), the svm finds a hyperplane that separates the two sets with maximum margin (or the largest possible distance from both sets), as illustrated in fig. 1. at pre-processing step, each training sample is converted into a real vector, xi that consists of a set of significant features representing the associated document, di. hence, tr+ = 2 and tein our setting, given a pre-classified document set, we automatically converted all the characters into lower case, and carried out tokenisation. given all the tokens found, a set of significant features was selected by using a feature selection method, i.e., document frequency as used by pang et al. (2002), so that we can compare our results with some existing results. as observed by dumais and chen (2000) and pang et al. (2002), to improve the performance of the svm, the frequencies of all the features within each document should be treated as binary and then normalised (document-length normalisation).",1
choi et al. (2005),1
"(eds.), proceedings of the 8th international conference on information and knowledge management (cikm 1999) (pp. 474-482).",2
"where i = {a . . . d} represents the value of each cell in a 2 x 2 contingency table. the yates continuity correction is applied to each (cid:2)2 calculation as the degree of freedom is 1. the (cid:2)2 calculation used in this experiment does not approximate the (cid:2)2 value, such as described in yang and pedersen (1997) and swan and allan (2000). the larger a (cid:2)2value, the stronger the evidence to reject the null hypothesis, which means that word and antecedent are dependent on each other. for the (cid:2)2-test, in order to reliably accept or reject h0, the expected values should be > 5. otherwise, it tends to underestimate small probabilities, which incorrectly results in accepting h1 (cochran, 1954).",1
"in a machine learning based classification, two sets of documents are required: a training and a test set. a training set (tr ) is used by an automatic classifier to learn the differentiating characteristics of documents, and a test set (te) is used to validate the performance of the automatic classifier. the machine learning based classification approach focuses on optimising either a set of parameter values with respect to a set of (binary or weighted) features or a set of induced rules with respect to a set of attribute-value pairs. for example, a support vector machines based approach focuses on finding a hyperplane that separates positive from negative sample sets by learning and optimising the weights of features as explained in section 4.2. in contrast, id3 (quinlan, 1986) and ripper (cohen, 1995) focus on reducing an initial large set of rules to improve the efficiency of a rule-based classifier by sacrificing a degree of effectiveness if necessary. sebastiani (2002) states that machine learning based classification is practical since automatic classifiers can achieve a level of accuracy comparable to that achieved by human experts. on the other hand, there are some drawbacks. the approach requires a large amount of time to assign significant features and a class to each document in the training set, and to train the automatic classifier such that a set of parameter values are optimised, or a set of induced rules are correctly constructed. in the case where the number of rules required is large, the process of acquiring and defining rules can be laborious and unreliable (dubitzky, 1997). it is especially significant if we have to deal with a huge collection of web documents, and have to collect appropriate documents for a training set. there is also no guarantee that a high level of accuracy obtained in one test set can also be obtained in another test set. in this context, we empirically examine the benefits and drawbacks of machine learning based classification approaches (section 5). 3. machine learning. we used support vector machines (svm) (joachims, 1998), the most widely used machine learning algorithm, to measure the effectiveness of machine learning approaches. we also examined the effectiveness of two induction algorithms, id3 (quinlan, 1986) and ripper (cohen, 1995). given the two rule sets generated by the rule-based classifier (rbc) and statistics based classifier (sbc), we applied two existing induction algorithms, id3 (quinlan, 1986) and ripper (cohen, 1995) provided by weka (witten, & frank, 2005), to generate two induced rule sets, and built a classifier that could use the two induced rule sets to classify a document collection. these two induced rule sets can hint about how well an induction algorithm works on an uncontrolled antecedent set, in the sense that the antecedent tokens representing attributes are not predefined, but simply derived from a preclassified document set. the expected result of using an induction algorithm was to have both an efficient rule set and better effectiveness in terms of both precision and recall.",1
"in proceedings of the 20th international conference on computational linguistics (coling 2004) geneva, switzerland, (pp. 841-847). international conference on computational linguistics (coling 2004) geneva, switzerland, (pp. 494-500). linguistics (coling 2004) geneva, switzerland, (pp. 1367-1373).",2
"in a machine learning based classification, two sets of documents are required: a training and a test set. a training set (tr ) is used by an automatic classifier to learn the differentiating characteristics of documents, and a test set (te) is used to validate the performance of the automatic classifier. the machine learning based classification approach focuses on optimising either a set of parameter values with respect to a set of (binary or weighted) features or a set of induced rules with respect to a set of attribute-value pairs. for example, a support vector machines based approach focuses on finding a hyperplane that separates positive from negative sample sets by learning and optimising the weights of features as explained in section 4.2. in contrast, id3 (quinlan, 1986) and ripper (cohen, 1995) focus on reducing an initial large set of rules to improve the efficiency of a rule-based classifier by sacrificing a degree of effectiveness if necessary. sebastiani (2002) states that machine learning based classification is practical since automatic classifiers can achieve a level of accuracy comparable to that achieved by human experts. on the other hand, there are some drawbacks. the approach requires a large amount of time to assign significant features and a class to each document in the training set, and to train the automatic classifier such that a set of parameter values are optimised, or a set of induced rules are correctly constructed. in the case where the number of rules required is large, the process of acquiring and defining rules can be laborious and unreliable (dubitzky, 1997). it is especially significant if we have to deal with a huge collection of web documents, and have to collect appropriate documents for a training set. there is also no guarantee that a high level of accuracy obtained in one test set can also be obtained in another test set. in this context, we empirically examine the benefits and drawbacks of machine learning based classification approaches (section 5).",1
"i = {a, b, c, d} and j = {c1, c2, r1, r2}. log likelihood ratio (dunning, 1993) follows the (cid:2)2 hypothesis, i.e., the larger a log likelihood ratio value, the stronger the evidence to reject the null hypothesis, which means that word and antecedent are dependent on each other. the log likelihood ratio is more accurate than (cid:2)2 for handling rare events. as a ranking function, the log likelihood ratio is therefore a better measure than (cid:2)2 for handling rare events.",1
of the 10th european conference on machine learning (ecml 1998) (pp. 4-15).,2
"on empirical methods in natural language processing (emnlp 2002) philadelphia, pa, usa, (pp. 79-86).",2
"in proceedings of the conference on empirical methods in natural language processing (emnlp 2005) vancouver, bc, canada, (pp. 355-362). on empirical methods in natural language processing (emnlp 2005) vancouver, bc, canada, (pp. 347-354).",2
accessed 1 february 2005.,1
"given the two rule sets generated by the rule-based classifier (rbc) and statistics based classifier (sbc), we applied two existing induction algorithms, id3 (quinlan, 1986) and ripper (cohen, 1995) provided by weka (witten, & frank, 2005), to generate two induced rule sets, and built a classifier that could use the two induced rule sets to classify a document collection. these two induced rule sets can hint about how well an induction algorithm works on an uncontrolled antecedent set, in the sense that the antecedent tokens representing attributes are not predefined, but simply derived from a preclassified document set. the expected result of using an induction algorithm was to have both an efficient rule set and better effectiveness in terms of both precision and recall.",2
"in contrast, a svm classifier, like other parametric approaches, such as naive bayes (govert, lalmas, & fuhr, 1999), rocchio (ittner, lewis, & ahn, 1995), and neural network (yin and savio, 1996) classifiers, regards a document collection as a set of significant features, each of which is assigned a weight, and each document is represented by a feature set. therefore, given a training set, the associated model focuses on optimising the weights and other parameters required, such that the model can achieve a high level of effectiveness on an unseen sample set. here, the independence of features is assumed, which is not always true, as explained in lewis (1998) and belew (2000). in addition, we sometimes have to make a trade-off between the effectiveness of the model and the time needed to train the model. an efficient algorithm is usually required to train the model. the associated classifier simply makes use of the model containing the learned weights and parameters to classify an unseen sample set.",1
gamon (2004),1
"the larger an mi value, the greater the association strength between antecedent and word, where mi(antecedent, word) must be >0. this means that the joint probability, p(antecedent, word) must be greater than the product of the probability of p(antecedent) and p(word). two examples of the use of this method for measuring the strength of two terms association can be found in conrad and utt (1994) and church and hanks (1989).",0
"whilst most researchers focus on assigning sentiments to documents, others focus on more specific tasks: finding the sentiments of words (hatzivassiloglou & mckeown, 1997), subjective expressions (kim & hovy, 2004; wilson, wiebe, & hoffmann, 2005), subjective sentences (pang & lee, 2004) and topics (hiroshi, tetsuya, & hideo, 2004; nasukawa & yi, 2003; yi, nasukawa, niblack, & bunescu, 2003). these tasks analyse sentiment at a fine-grained level and can be used to improve the effectiveness of sentiment classification, as shown in pang and lee (2004). instead of carrying out a sentiment classification or an opinion extraction, choi, cardie, riloff, and patwardhan (2005) focus on extracting the sources of opinions, e.g., the persons or organizations who play a crucial role in influencing other individuals' opinions. various data sources have been used, ranging from product reviews, customer feedback, the document understanding conference (duc) corpus, the multi-perspective question answering (mpqa) corpus and the wall street journal (wsj) corpus.",0
"whilst most researchers focus on assigning sentiments to documents, others focus on more specific tasks: finding the sentiments of words (hatzivassiloglou & mckeown, 1997), subjective expressions (kim & hovy, 2004; wilson, wiebe, & hoffmann, 2005), subjective sentences (pang & lee, 2004) and topics (hiroshi, tetsuya, & hideo, 2004; nasukawa & yi, 2003; yi, nasukawa, niblack, & bunescu, 2003). these tasks analyse sentiment at a fine-grained level and can be used to improve the effectiveness of sentiment classification, as shown in pang and lee (2004). instead of carrying out a sentiment classification or an opinion extraction, choi, cardie, riloff, and patwardhan (2005) focus on extracting the sources of opinions, e.g., the persons or organizations who play a crucial role in influencing other individuals' opinions. various data sources have been used, ranging from product reviews, customer feedback, the document understanding conference (duc) corpus, the multi-perspective question answering (mpqa) corpus and the wall street journal (wsj) corpus.",0
hiroshi et al. (2004),1
"to automate sentiment analysis, different approaches have been applied to predict the sentiments of words, expressions or documents. these are natural language processing (nlp) and pattern-based (hiroshi et al., 2004; konig & brill, 2006; nasukawa & yi, 2003; yi et al., 2003), machine learning algorithms, such as naive bayes (nb), maximum entropy (me), support vector machine (svm) (joachims, 1998), and unsupervised learning (turney, 2002).",1
"whilst most researchers focus on assigning sentiments to documents, others focus on more specific tasks: finding the sentiments of words (hatzivassiloglou & mckeown, 1997), subjective expressions (kim & hovy, 2004; wilson, wiebe, & hoffmann, 2005), subjective sentences (pang & lee, 2004) and topics (hiroshi, tetsuya, & hideo, 2004; nasukawa & yi, 2003; yi, nasukawa, niblack, & bunescu, 2003). these tasks analyse sentiment at a fine-grained level and can be used to improve the effectiveness of sentiment classification, as shown in pang and lee (2004). instead of carrying out a sentiment classification or an opinion extraction, choi, cardie, riloff, and patwardhan (2005) focus on extracting the sources of opinions, e.g., the persons or organizations who play a crucial role in influencing other individuals' opinions. various data sources have been used, ranging from product reviews, customer feedback, the document understanding conference (duc) corpus, the multi-perspective question answering (mpqa) corpus and the wall street journal (wsj) corpus.",0
kim and hovy (2004),1
"processing techniques. in proceedings of the 3rd ieee international conference on data mining (icdm 2003) florida, usa, (pp. 427-434).",2
learning (icml 1995) (pp. 115-123).,2
"on machine learning (icml 1997) nashville, tennessee, (pp. 412-420).",2
article history: received 31 july 2008 received in revised form 21 january 2009 accepted 22 january 2009,1
"to automate sentiment analysis, different approaches have been applied to predict the sentiments of words, expressions or documents. these are natural language processing (nlp) and pattern-based (hiroshi et al., 2004; konig & brill, 2006; nasukawa & yi, 2003; yi et al., 2003), machine learning algorithms, such as naive bayes (nb), maximum entropy (me), support vector machine (svm) (joachims, 1998), and unsupervised learning (turney, 2002). 3. machine learning. we used support vector machines (svm) (joachims, 1998), the most widely used machine learning algorithm, to measure the effectiveness of machine learning approaches. we also examined the effectiveness of two induction algorithms, id3 (quinlan, 1986) and ripper (cohen, 1995). we used support vector machine (svmlight) v6.01 (joachims, 1998). as explained in dumais and chen (2000) and pang et al. (2002) given a category set, c = {+1, -1} and two pre-classified training sets, i.e., a positive sample set, t + i=1(di, +1) and a negative sample set, t i=1(di, -1), the svm finds a hyperplane that separates the two sets with maximum margin (or the largest possible distance from both sets), as illustrated in fig. 1. at pre-processing step, each training sample is converted into a real vector, xi that consists of a set of significant features representing the associated document, di. hence, tr+ =",1
article history: received 31 july 2008 received in revised form 21 january 2009 accepted 22 january 2009,1
"whilst most researchers focus on assigning sentiments to documents, others focus on more specific tasks: finding the sentiments of words (hatzivassiloglou & mckeown, 1997), subjective expressions (kim & hovy, 2004; wilson, wiebe, & hoffmann, 2005), subjective sentences (pang & lee, 2004) and topics (hiroshi, tetsuya, & hideo, 2004; nasukawa & yi, 2003; yi, nasukawa, niblack, & bunescu, 2003). these tasks analyse sentiment at a fine-grained level and can be used to improve the effectiveness of sentiment classification, as shown in pang and lee (2004). instead of carrying out a sentiment classification or an opinion extraction, choi, cardie, riloff, and patwardhan (2005) focus on extracting the sources of opinions, e.g., the persons or organizations who play a crucial role in influencing other individuals' opinions. various data sources have been used, ranging from product reviews, customer feedback, the document understanding conference (duc) corpus, the multi-perspective question answering (mpqa) corpus and the wall street journal (wsj) corpus.",0
"to automate sentiment analysis, different approaches have been applied to predict the sentiments of words, expressions or documents. these are natural language processing (nlp) and pattern-based (hiroshi et al., 2004; konig & brill, 2006; nasukawa & yi, 2003; yi et al., 2003), machine learning algorithms, such as naive bayes (nb), maximum entropy (me), support vector machine (svm) (joachims, 1998), and unsupervised learning (turney, 2002).",1
"in contrast, if we cannot hire human classifiers to produce a training set, we can apply the sbc and a closeness measure to use a corpus to determine the consequents of antecedents. this has three advantages. first, no human classifiers are required to produce a training set. hence, it significantly reduces cost and time. second, the sbc can show the antecedents as the reason for selecting an appropriate category, and assign the associated antecedent to the category. third, by using a human classifier to judge the assigned consequent, we can build a rule database that grows over time. to build this rule database, we created a sentiment analysis tool (sat) that can assist a human classifier in checking the correctness of a new rule and integrating it into the existing rule database if it does not exist. by using this rule database, the sat can also build a training set, which can be exploited by the svm to classify the documents that cannot be classified by both the rbc and sbc. in this respect, the use of rbc, sbc, and svm in a hybrid and semi-automatic manner can be interpreted as a complementary approach, i.e., each classifier contributes to other classifiers to achieve a good level of effectiveness, as illustrated in fig. 4. the sbc and sat provide the rbc with a rule database, and the svm with a training set. as a result, the rbc and svm can assist the sbc to achieve a better level of effectiveness and efficiency. the problem arises if we do not have our own relatively large corpus, because we will overload a search engine with a huge amount of queries (an ethical issue) and spend a lot of time to collect the hit counts required (an efficiency issue). another problem is to deal with both the coverage level and fluctuation of a search engine, which can affect the effectiveness of the sbc as shown in table 8. the coverage issues are discussed in bar-ilan (2001) and thelwall (2000), and the fluctuation issue in bar-ilan (1999) and mettrop and nieuwenhuysen (2001). the fluctuation and ethical issues are the main reasons for not being able to collect the hit counts for the s1 sample set. this means that no classifier outperforms other classifiers: they need each other to achieve the best performance. in our experimental setting, we decided to assign a document one sentiment only (binary classification), so that the f1 measure could be applied without the risk of over-fitting. the rule-based classifiers used can carry out a multiple classification, however, i.e., assigning a document more than one category. the classifiers classify each sentence within a document, and then rank all possible sentiments in descending order. for a binary classification, the classifiers only select the top rank. for a multiple classification, the classifiers can select the best n categories, where n >= 1. a further extension of the system would be to use it to create fuzzy classifications (kuncheva, 2000), i.e., assigning sentiment on a probability rather than a binary basis. fuzzy hybrid classification has been achieved before (ishibuchi, nakashima, & kuroda, 2000) and so this should be possible for our system.",0
"in contrast, if we cannot hire human classifiers to produce a training set, we can apply the sbc and a closeness measure to use a corpus to determine the consequents of antecedents. this has three advantages. first, no human classifiers are required to produce a training set. hence, it significantly reduces cost and time. second, the sbc can show the antecedents as the reason for selecting an appropriate category, and assign the associated antecedent to the category. third, by using a human classifier to judge the assigned consequent, we can build a rule database that grows over time. to build this rule database, we created a sentiment analysis tool (sat) that can assist a human classifier in checking the correctness of a new rule and integrating it into the existing rule database if it does not exist. by using this rule database, the sat can also build a training set, which can be exploited by the svm to classify the documents that cannot be classified by both the rbc and sbc. in this respect, the use of rbc, sbc, and svm in a hybrid and semi-automatic manner can be interpreted as a complementary approach, i.e., each classifier contributes to other classifiers to achieve a good level of effectiveness, as illustrated in fig. 4. the sbc and sat provide the rbc with a rule database, and the svm with a training set. as a result, the rbc and svm can assist the sbc to achieve a better level of effectiveness and efficiency. the problem arises if we do not have our own relatively large corpus, because we will overload a search engine with a huge amount of queries (an ethical issue) and spend a lot of time to collect the hit counts required (an efficiency issue). another problem is to deal with both the coverage level and fluctuation of a search engine, which can affect the effectiveness of the sbc as shown in table 8. the coverage issues are discussed in bar-ilan (2001) and thelwall (2000), and the fluctuation issue in bar-ilan (1999) and mettrop and nieuwenhuysen (2001). the fluctuation and ethical issues are the main reasons for not being able to collect the hit counts for the s1 sample set. this means that no classifier outperforms other classifiers: they need each other to achieve the best performance. in our experimental setting, we decided to assign a document one sentiment only (binary classification), so that the f1 measure could be applied without the risk of over-fitting. the rule-based classifiers used can carry out a multiple classification, however, i.e., assigning a document more than one category. the classifiers classify each sentence within a document, and then rank all possible sentiments in descending order. for a binary classification, the classifiers only select the top rank. for a multiple classification, the classifiers can select the best n categories, where n >= 1. a further extension of the system would be to use it to create fuzzy classifications (kuncheva, 2000), i.e., assigning sentiment on a probability rather than a binary basis. fuzzy hybrid classification has been achieved before (ishibuchi, nakashima, & kuroda, 2000) and so this should be possible for our system.",0
"whilst most researchers focus on assigning sentiments to documents, others focus on more specific tasks: finding the sentiments of words (hatzivassiloglou & mckeown, 1997), subjective expressions (kim & hovy, 2004; wilson, wiebe, & hoffmann, 2005), subjective sentences (pang & lee, 2004) and topics (hiroshi, tetsuya, & hideo, 2004; nasukawa & yi, 2003; yi, nasukawa, niblack, & bunescu, 2003). these tasks analyse sentiment at a fine-grained level and can be used to improve the effectiveness of sentiment classification, as shown in pang and lee (2004). instead of carrying out a sentiment classification or an opinion extraction, choi, cardie, riloff, and patwardhan (2005) focus on extracting the sources of opinions, e.g., the persons or organizations who play a crucial role in influencing other individuals' opinions. various data sources have been used, ranging from product reviews, customer feedback, the document understanding conference (duc) corpus, the multi-perspective question answering (mpqa) corpus and the wall street journal (wsj) corpus. pang and lee (2004)",0
pang and lee (2005),1
"in contrast, a svm classifier, like other parametric approaches, such as naive bayes (govert, lalmas, & fuhr, 1999), rocchio (ittner, lewis, & ahn, 1995), and neural network (yin and savio, 1996) classifiers, regards a document collection as a set of significant features, each of which is assigned a weight, and each document is represented by a feature set. therefore, given a training set, the associated model focuses on optimising the weights and other parameters required, such that the model can achieve a high level of effectiveness on an unseen sample set. here, the independence of features is assumed, which is not always true, as explained in lewis (1998) and belew (2000). in addition, we sometimes have to make a trade-off between the effectiveness of the model and the time needed to train the model. an efficient algorithm is usually required to train the model. the associated classifier simply makes use of the model containing the learned weights and parameters to classify an unseen sample set.",1
"micro-averaging treats each document equally. this means that micro-averaging results in averaging over a set of documents. the performance of a classifier tends to be dominated by common classes. in contrast, macro-averaging treats each class equally. this means that macro-averaging results in averaging over a set of classes. the performance of a classifier tends to be dominated by infrequent classes. one class that results in a bad performance can deteriorate the overall performance significantly. hence, it is common that macro-averaged performance is lower than micro-averaged performance, as shown in a classification performance evaluation conducted by yang and liu (1999) and calvo and ceccatto (2000).",1
"1. nlp and pattern-based approaches. these focus on using existing natural language processing tools, such as part-of-speech (pos)-taggers and parsers, or n-grams, such as unigrams, bigrams and trigrams. the results generated by the tools or ngram processors are further processed to generate a set of patterns. each pattern is assigned a sentiment, either positive or negative. in our setting, we used the montylingua (liu, 2004) parser to produce a collection of parsed sentences that can be further processed to form a set of rules (section 4.1). the following procedure was used to generate a set of antecedents. the montylingua (liu, 2004) chunker was used to parse all the sentences found in the document set. given these parsed sentences, a set of proper nouns, i.e., all terms tagged with nnp and nnps, was automatically identified and replaced by '?'. to reduce the error rate of parsing, we automatically scanned and tested all the proper nouns identified by montylingua against all the nouns (nn and nns) in wordnet 2.0 (miller, 1995). when wordnet regarded the proper nouns as standard nouns, the proper nouns were regarded as incorrectly tagged, and were not replaced with '?'. in addition, all target words were replaced with '#'. as a result, a set of antecedents was generated. a suffix array (manber & myers, 1990) was then built to speed up antecedent matching.",1
"the following procedure was used to generate a set of antecedents. the montylingua (liu, 2004) chunker was used to parse all the sentences found in the document set. given these parsed sentences, a set of proper nouns, i.e., all terms tagged with nnp and nnps, was automatically identified and replaced by '?'. to reduce the error rate of parsing, we automatically scanned and tested all the proper nouns identified by montylingua against all the nouns (nn and nns) in wordnet 2.0 (miller, 1995). when wordnet regarded the proper nouns as standard nouns, the proper nouns were regarded as incorrectly tagged, and were not replaced with '?'. in addition, all target words were replaced with '#'. as a result, a set of antecedents was generated. a suffix array (manber & myers, 1990) was then built to speed up antecedent matching.",1
"the first data set was downloaded from pang (2007). the second data set was a small version of the first data set, i.e., the first 200 samples extracted from the first data set. this small data set was required to test the effectiveness of the sbc and the induction algorithms (section 4.1.4), which could not handle a large data set. the third data set was proprietary data provided by market-sentinel (2007). this is a clean data set that was pre-classified by experts. the fourth data set was also proprietary, provided by thelwall (2008), extracted from myspace (2007), and pre-classified by three assessors with kappa ((cid:4)) = 100%, i.e., the three assessors completely agreed with each other. whilst the movie review data contains a lot of sentences per document, the product reviews and myspace comments are quite sparse.",2
hatzivassiloglou and mckeown (1997),1
"the following procedure was used to generate a set of antecedents. the montylingua (liu, 2004) chunker was used to parse all the sentences found in the document set. given these parsed sentences, a set of proper nouns, i.e., all terms tagged with nnp and nnps, was automatically identified and replaced by '?'. to reduce the error rate of parsing, we automatically scanned and tested all the proper nouns identified by montylingua against all the nouns (nn and nns) in wordnet 2.0 (miller, 1995). when wordnet regarded the proper nouns as standard nouns, the proper nouns were regarded as incorrectly tagged, and were not replaced with '?'. in addition, all target words were replaced with '#'. as a result, a set of antecedents was generated. a suffix array (manber & myers, 1990) was then built to speed up antecedent matching.",1
"the first data set was downloaded from pang (2007). the second data set was a small version of the first data set, i.e., the first 200 samples extracted from the first data set. this small data set was required to test the effectiveness of the sbc and the induction algorithms (section 4.1.4), which could not handle a large data set. the third data set was proprietary data provided by market-sentinel (2007). this is a clean data set that was pre-classified by experts. the fourth data set was also proprietary, provided by thelwall (2008), extracted from myspace (2007), and pre-classified by three assessors with kappa ((cid:4)) = 100%, i.e., the three assessors completely agreed with each other. whilst the movie review data contains a lot of sentences per document, the product reviews and myspace comments are quite sparse.",2
"whilst most researchers focus on assigning sentiments to documents, others focus on more specific tasks: finding the sentiments of words (hatzivassiloglou & mckeown, 1997), subjective expressions (kim & hovy, 2004; wilson, wiebe, & hoffmann, 2005), subjective sentences (pang & lee, 2004) and topics (hiroshi, tetsuya, & hideo, 2004; nasukawa & yi, 2003; yi, nasukawa, niblack, & bunescu, 2003). these tasks analyse sentiment at a fine-grained level and can be used to improve the effectiveness of sentiment classification, as shown in pang and lee (2004). instead of carrying out a sentiment classification or an opinion extraction, choi, cardie, riloff, and patwardhan (2005) focus on extracting the sources of opinions, e.g., the persons or organizations who play a crucial role in influencing other individuals' opinions. various data sources have been used, ranging from product reviews, customer feedback, the document understanding conference (duc) corpus, the multi-perspective question answering (mpqa) corpus and the wall street journal (wsj) corpus. to automate sentiment analysis, different approaches have been applied to predict the sentiments of words, expressions or documents. these are natural language processing (nlp) and pattern-based (hiroshi et al., 2004; konig & brill, 2006; nasukawa & yi, 2003; yi et al., 2003), machine learning algorithms, such as naive bayes (nb), maximum entropy (me), support vector machine (svm) (joachims, 1998), and unsupervised learning (turney, 2002).",1
"the work was supported by a european union grant for activity code nest-2003-path-1 and the future & emerging technologies scheme. it is part of the creen (critical events in evolving networks, contract 012684) and cyberemotions projects. we would like to thank mark rogers of market sentinel for help with providing classified data.",1
"in contrast, if we cannot hire human classifiers to produce a training set, we can apply the sbc and a closeness measure to use a corpus to determine the consequents of antecedents. this has three advantages. first, no human classifiers are required to produce a training set. hence, it significantly reduces cost and time. second, the sbc can show the antecedents as the reason for selecting an appropriate category, and assign the associated antecedent to the category. third, by using a human classifier to judge the assigned consequent, we can build a rule database that grows over time. to build this rule database, we created a sentiment analysis tool (sat) that can assist a human classifier in checking the correctness of a new rule and integrating it into the existing rule database if it does not exist. by using this rule database, the sat can also build a training set, which can be exploited by the svm to classify the documents that cannot be classified by both the rbc and sbc. in this respect, the use of rbc, sbc, and svm in a hybrid and semi-automatic manner can be interpreted as a complementary approach, i.e., each classifier contributes to other classifiers to achieve a good level of effectiveness, as illustrated in fig. 4. the sbc and sat provide the rbc with a rule database, and the svm with a training set. as a result, the rbc and svm can assist the sbc to achieve a better level of effectiveness and efficiency. the problem arises if we do not have our own relatively large corpus, because we will overload a search engine with a huge amount of queries (an ethical issue) and spend a lot of time to collect the hit counts required (an efficiency issue). another problem is to deal with both the coverage level and fluctuation of a search engine, which can affect the effectiveness of the sbc as shown in table 8. the coverage issues are discussed in bar-ilan (2001) and thelwall (2000), and the fluctuation issue in bar-ilan (1999) and mettrop and nieuwenhuysen (2001). the fluctuation and ethical issues are the main reasons for not being able to collect the hit counts for the s1 sample set. this means that no classifier outperforms other classifiers: they need each other to achieve the best performance. in our experimental setting, we decided to assign a document one sentiment only (binary classification), so that the f1 measure could be applied without the risk of over-fitting. the rule-based classifiers used can carry out a multiple classification, however, i.e., assigning a document more than one category. the classifiers classify each sentence within a document, and then rank all possible sentiments in descending order. for a binary classification, the classifiers only select the top rank. for a multiple classification, the classifiers can select the best n categories, where n >= 1. a further extension of the system would be to use it to create fuzzy classifications (kuncheva, 2000), i.e., assigning sentiment on a probability rather than a binary basis. fuzzy hybrid classification has been achieved before (ishibuchi, nakashima, & kuroda, 2000) and so this should be possible for our system.",0
"pang, b. (2007). polarity data set v2.0. october 1997. ! http://www.cs.cornell.edu/people/pabo/moviereview-data/? accessed 4 august 2007. pang, b., & lee, l. (2004, july 21-26). a sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. in proceedings",1
"market-sentinel. (2007). market sentinel. ! http://www.marketsentinel.com? accessed 4 october 2007. mettrop, w., & nieuwenbuysen, p. (2001). internet search engines: fluctuations in document accessibility. journal of documentation, 57(5), 623-651. miller, g. a. (1995). wordnet: a lexical database for english. communications of the acm, 38(11), 39-41. myspace. (2007). myspace. ! http://www.myspace.com? accessed april 2007. nasukawa, t., & yi, j. (2003, october 23-25). sentiment analysis: capturing favorability using natural language processing. in proceedings of the 2nd",1
"the first, simplest rule set was based on 3672 pre-classified words found in the general inquirer lexicon (stone, dunphy, smith, & ogilvie, 1966), 1598 of which were pre-classified as positive and 2074 of which were pre-classified as negative. here, each rule depends solely on one sentiment bearing word representing an antecedent. we implemented a general inquirer based classifier (gibc) that applied the rule set to classify document collections.",1
"whilst most researchers focus on assigning sentiments to documents, others focus on more specific tasks: finding the sentiments of words (hatzivassiloglou & mckeown, 1997), subjective expressions (kim & hovy, 2004; wilson, wiebe, & hoffmann, 2005), subjective sentences (pang & lee, 2004) and topics (hiroshi, tetsuya, & hideo, 2004; nasukawa & yi, 2003; yi, nasukawa, niblack, & bunescu, 2003). these tasks analyse sentiment at a fine-grained level and can be used to improve the effectiveness of sentiment classification, as shown in pang and lee (2004). instead of carrying out a sentiment classification or an opinion extraction, choi, cardie, riloff, and patwardhan (2005) focus on extracting the sources of opinions, e.g., the persons or organizations who play a crucial role in influencing other individuals' opinions. various data sources have been used, ranging from product reviews, customer feedback, the document understanding conference (duc) corpus, the multi-perspective question answering (mpqa) corpus and the wall street journal (wsj) corpus.",0
"the first data set was downloaded from pang (2007). the second data set was a small version of the first data set, i.e., the first 200 samples extracted from the first data set. this small data set was required to test the effectiveness of the sbc and the induction algorithms (section 4.1.4), which could not handle a large data set. the third data set was proprietary data provided by market-sentinel (2007). this is a clean data set that was pre-classified by experts. the fourth data set was also proprietary, provided by thelwall (2008), extracted from myspace (2007), and pre-classified by three assessors with kappa ((cid:4)) = 100%, i.e., the three assessors completely agreed with each other. whilst the movie review data contains a lot of sentences per document, the product reviews and myspace comments are quite sparse.",2
"we used support vector machine (svmlight) v6.01 (joachims, 1998). as explained in dumais and chen (2000) and pang et al. (2002) given a category set, c = {+1, -1} and two pre-classified training sets, i.e., a positive sample set, t + i=1(di, +1) and a negative sample set, t i=1(di, -1), the svm finds a hyperplane that separates the two sets with maximum margin (or the largest possible distance from both sets), as illustrated in fig. 1. at pre-processing step, each training sample is converted into a real vector, xi that consists of a set of significant features representing the associated document, di. hence, tr+ = 2 and tein our setting, given a pre-classified document set, we automatically converted all the characters into lower case, and carried out tokenisation. given all the tokens found, a set of significant features was selected by using a feature selection method, i.e., document frequency as used by pang et al. (2002), so that we can compare our results with some existing results. as observed by dumais and chen (2000) and pang et al. (2002), to improve the performance of the svm, the frequencies of all the features within each document should be treated as binary and then normalised (document-length normalisation).",1
"whilst most researchers focus on assigning sentiments to documents, others focus on more specific tasks: finding the sentiments of words (hatzivassiloglou & mckeown, 1997), subjective expressions (kim & hovy, 2004; wilson, wiebe, & hoffmann, 2005), subjective sentences (pang & lee, 2004) and topics (hiroshi, tetsuya, & hideo, 2004; nasukawa & yi, 2003; yi, nasukawa, niblack, & bunescu, 2003). these tasks analyse sentiment at a fine-grained level and can be used to improve the effectiveness of sentiment classification, as shown in pang and lee (2004). instead of carrying out a sentiment classification or an opinion extraction, choi, cardie, riloff, and patwardhan (2005) focus on extracting the sources of opinions, e.g., the persons or organizations who play a crucial role in influencing other individuals' opinions. various data sources have been used, ranging from product reviews, customer feedback, the document understanding conference (duc) corpus, the multi-perspective question answering (mpqa) corpus and the wall street journal (wsj) corpus.",0
"document frequency(df). this counts the number of web pages containing a pair of an antecedent and a sentiment bearing word, i.e., the hit count returned by a search engine. the larger a df value, the greater the association strength between antecedent and word. the use of df in the context of automatic classification can be found in yang and pedersen (1997) and yang (1999). where i = {a . . . d} represents the value of each cell in a 2 x 2 contingency table. the yates continuity correction is applied to each (cid:2)2 calculation as the degree of freedom is 1. the (cid:2)2 calculation used in this experiment does not approximate the (cid:2)2 value, such as described in yang and pedersen (1997) and swan and allan (2000). the larger a (cid:2)2value, the stronger the evidence to reject the null hypothesis, which means that word and antecedent are dependent on each other. for the (cid:2)2-test, in order to reliably accept or reject h0, the expected values should be > 5. otherwise, it tends to underestimate small probabilities, which incorrectly results in accepting h1 (cochran, 1954).",0
"dave, lawrence, and pennock (2003)",1
"in a machine learning based classification, two sets of documents are required: a training and a test set. a training set (tr ) is used by an automatic classifier to learn the differentiating characteristics of documents, and a test set (te) is used to validate the performance of the automatic classifier. the machine learning based classification approach focuses on optimising either a set of parameter values with respect to a set of (binary or weighted) features or a set of induced rules with respect to a set of attribute-value pairs. for example, a support vector machines based approach focuses on finding a hyperplane that separates positive from negative sample sets by learning and optimising the weights of features as explained in section 4.2. in contrast, id3 (quinlan, 1986) and ripper (cohen, 1995) focus on reducing an initial large set of rules to improve the efficiency of a rule-based classifier by sacrificing a degree of effectiveness if necessary. sebastiani (2002) states that machine learning based classification is practical since automatic classifiers can achieve a level of accuracy comparable to that achieved by human experts. on the other hand, there are some drawbacks. the approach requires a large amount of time to assign significant features and a class to each document in the training set, and to train the automatic classifier such that a set of parameter values are optimised, or a set of induced rules are correctly constructed. in the case where the number of rules required is large, the process of acquiring and defining rules can be laborious and unreliable (dubitzky, 1997). it is especially significant if we have to deal with a huge collection of web documents, and have to collect appropriate documents for a training set. there is also no guarantee that a high level of accuracy obtained in one test set can also be obtained in another test set. in this context, we empirically examine the benefits and drawbacks of machine learning based classification approaches (section 5). 3. machine learning. we used support vector machines (svm) (joachims, 1998), the most widely used machine learning algorithm, to measure the effectiveness of machine learning approaches. we also examined the effectiveness of two induction algorithms, id3 (quinlan, 1986) and ripper (cohen, 1995). given the two rule sets generated by the rule-based classifier (rbc) and statistics based classifier (sbc), we applied two existing induction algorithms, id3 (quinlan, 1986) and ripper (cohen, 1995) provided by weka (witten, & frank, 2005), to generate two induced rule sets, and built a classifier that could use the two induced rule sets to classify a document collection. these two induced rule sets can hint about how well an induction algorithm works on an uncontrolled antecedent set, in the sense that the antecedent tokens representing attributes are not predefined, but simply derived from a preclassified document set. the expected result of using an induction algorithm was to have both an efficient rule set and better effectiveness in terms of both precision and recall.",1
"in contrast, a svm classifier, like other parametric approaches, such as naive bayes (govert, lalmas, & fuhr, 1999), rocchio (ittner, lewis, & ahn, 1995), and neural network (yin and savio, 1996) classifiers, regards a document collection as a set of significant features, each of which is assigned a weight, and each document is represented by a feature set. therefore, given a training set, the associated model focuses on optimising the weights and other parameters required, such that the model can achieve a high level of effectiveness on an unseen sample set. here, the independence of features is assumed, which is not always true, as explained in lewis (1998) and belew (2000). in addition, we sometimes have to make a trade-off between the effectiveness of the model and the time needed to train the model. an efficient algorithm is usually required to train the model. the associated classifier simply makes use of the model containing the learned weights and parameters to classify an unseen sample set.",1
"and information retrieval (sdair 1995) las vegas, usa, (pp. 301-315).",2
"in a machine learning based classification, two sets of documents are required: a training and a test set. a training set (tr ) is used by an automatic classifier to learn the differentiating characteristics of documents, and a test set (te) is used to validate the performance of the automatic classifier. the machine learning based classification approach focuses on optimising either a set of parameter values with respect to a set of (binary or weighted) features or a set of induced rules with respect to a set of attribute-value pairs. for example, a support vector machines based approach focuses on finding a hyperplane that separates positive from negative sample sets by learning and optimising the weights of features as explained in section 4.2. in contrast, id3 (quinlan, 1986) and ripper (cohen, 1995) focus on reducing an initial large set of rules to improve the efficiency of a rule-based classifier by sacrificing a degree of effectiveness if necessary. sebastiani (2002) states that machine learning based classification is practical since automatic classifiers can achieve a level of accuracy comparable to that achieved by human experts. on the other hand, there are some drawbacks. the approach requires a large amount of time to assign significant features and a class to each document in the training set, and to train the automatic classifier such that a set of parameter values are optimised, or a set of induced rules are correctly constructed. in the case where the number of rules required is large, the process of acquiring and defining rules can be laborious and unreliable (dubitzky, 1997). it is especially significant if we have to deal with a huge collection of web documents, and have to collect appropriate documents for a training set. there is also no guarantee that a high level of accuracy obtained in one test set can also be obtained in another test set. in this context, we empirically examine the benefits and drawbacks of machine learning based classification approaches (section 5).",1
"where m is the number of documents in the collection. automatic classification is defined as a process in which a classifier program determines to which class a document belongs. the main objective of a classification is to assign an appropriate class to a document with respect to a class set. the results are a set of pairs, such that each pair contains a document, di, and a class, cj, where {di, cj} d x c. di, cj means that di d is assigned with (or is classified into) cj c (sebastiani, 2002).",1
"on discrete algorithms (soda 1990) san francisco, california,",1
"in contrast, if we cannot hire human classifiers to produce a training set, we can apply the sbc and a closeness measure to use a corpus to determine the consequents of antecedents. this has three advantages. first, no human classifiers are required to produce a training set. hence, it significantly reduces cost and time. second, the sbc can show the antecedents as the reason for selecting an appropriate category, and assign the associated antecedent to the category. third, by using a human classifier to judge the assigned consequent, we can build a rule database that grows over time. to build this rule database, we created a sentiment analysis tool (sat) that can assist a human classifier in checking the correctness of a new rule and integrating it into the existing rule database if it does not exist. by using this rule database, the sat can also build a training set, which can be exploited by the svm to classify the documents that cannot be classified by both the rbc and sbc. in this respect, the use of rbc, sbc, and svm in a hybrid and semi-automatic manner can be interpreted as a complementary approach, i.e., each classifier contributes to other classifiers to achieve a good level of effectiveness, as illustrated in fig. 4. the sbc and sat provide the rbc with a rule database, and the svm with a training set. as a result, the rbc and svm can assist the sbc to achieve a better level of effectiveness and efficiency. the problem arises if we do not have our own relatively large corpus, because we will overload a search engine with a huge amount of queries (an ethical issue) and spend a lot of time to collect the hit counts required (an efficiency issue). another problem is to deal with both the coverage level and fluctuation of a search engine, which can affect the effectiveness of the sbc as shown in table 8. the coverage issues are discussed in bar-ilan (2001) and thelwall (2000), and the fluctuation issue in bar-ilan (1999) and mettrop and nieuwenhuysen (2001). the fluctuation and ethical issues are the main reasons for not being able to collect the hit counts for the s1 sample set. this means that no classifier outperforms other classifiers: they need each other to achieve the best performance. in our experimental setting, we decided to assign a document one sentiment only (binary classification), so that the f1 measure could be applied without the risk of over-fitting. the rule-based classifiers used can carry out a multiple classification, however, i.e., assigning a document more than one category. the classifiers classify each sentence within a document, and then rank all possible sentiments in descending order. for a binary classification, the classifiers only select the top rank. for a multiple classification, the classifiers can select the best n categories, where n >= 1. a further extension of the system would be to use it to create fuzzy classifications (kuncheva, 2000), i.e., assigning sentiment on a probability rather than a binary basis. fuzzy hybrid classification has been achieved before (ishibuchi, nakashima, & kuroda, 2000) and so this should be possible for our system.",0
"the first data set was downloaded from pang (2007). the second data set was a small version of the first data set, i.e., the first 200 samples extracted from the first data set. this small data set was required to test the effectiveness of the sbc and the induction algorithms (section 4.1.4), which could not handle a large data set. the third data set was proprietary data provided by market-sentinel (2007). this is a clean data set that was pre-classified by experts. the fourth data set was also proprietary, provided by thelwall (2008), extracted from myspace (2007), and pre-classified by three assessors with kappa ((cid:4)) = 100%, i.e., the three assessors completely agreed with each other. whilst the movie review data contains a lot of sentences per document, the product reviews and myspace comments are quite sparse.",2
demonstrated in turney (2002). section 4.1.3 explains our method. turney (2002),1
"to automate sentiment analysis, different approaches have been applied to predict the sentiments of words, expressions or documents. these are natural language processing (nlp) and pattern-based (hiroshi et al., 2004; konig & brill, 2006; nasukawa & yi, 2003; yi et al., 2003), machine learning algorithms, such as naive bayes (nb), maximum entropy (me), support vector machine (svm) (joachims, 1998), and unsupervised learning (turney, 2002). the statistics based classifier (sbc) used a rule set built using the following assumption. bad expressions co-occur more frequently with the word 'poor', and good expressions with the word 'excellent' (turney, 2002). we calculated the closeness between an antecedent representing an expression and a set of sentiment bearing words. the following procedure was used to statistically determine the consequent of an antecedent.",2
"the larger an mi value, the greater the association strength between antecedent and word, where mi(antecedent, word) must be >0. this means that the joint probability, p(antecedent, word) must be greater than the product of the probability of p(antecedent) and p(word). two examples of the use of this method for measuring the strength of two terms association can be found in conrad and utt (1994) and church and hanks (1989).",0
vaithyanathan (2002),1
wilson et al. (2005),1
"document frequency(df). this counts the number of web pages containing a pair of an antecedent and a sentiment bearing word, i.e., the hit count returned by a search engine. the larger a df value, the greater the association strength between antecedent and word. the use of df in the context of automatic classification can be found in yang and pedersen (1997) and yang (1999).",0
yi et al. (2003),1
"to automate sentiment analysis, different approaches have been applied to predict the sentiments of words, expressions or documents. these are natural language processing (nlp) and pattern-based (hiroshi et al., 2004; konig & brill, 2006; nasukawa & yi, 2003; yi et al., 2003), machine learning algorithms, such as naive bayes (nb), maximum entropy (me), support vector machine (svm) (joachims, 1998), and unsupervised learning (turney, 2002).",1
"[43] yuanbinwu, qi zhang, xuanjing huang, lidewu, ""phrase dependency parsing for sentiment analysis"", proceedings of the 2009 conference on empirical methods in natural language processing, pages 1533-1541, singapore, 6-7 august 2009 .",1
"of computing, volume 2, issue 8, august 2010, issn 2151-9617 .",1
all content following this page was uploaded by vinodhini g on 30 august 2014.,1
"2.1. blogs with an increasing usage of the internet, blogging and blog pages are growing rapidly. blog pages have become the most popular means to express one""s personal opinions. bloggers record the daily events in their lives and express their opinions, feelings, and emotions in a blog (chau & xu, 2007). many of these blogs contain reviews on many products, issues, etc. blogs are used as a source of opinion in many of the studies related to sentiment analysis (martin, 2005; murphy, 2006; tang et al., 2009).",0
"in order to identify potential risks, it is important for companies to collect and analyze information about their competitors' products and plans. sentiment analysis find a major role in competitive intelligence (kaiquan xu , 2011) to extract and visualize comparative relations between products from customer reviews, with the interdependencies into consideration, to help enterprises discover potential risks and further design new products and marketing strategies. opinion summarization summarizes opinions of articles by telling sentiment polarities, degree and the correlated events. with opinion summarization, a customer can easily see how the existing customers feel about a product, and the product manufacturer can get the reason why different stands people like it or what they complain about. ku, liang, and chen (2006) investigated both news and web blog articles. algorithms for opinion extraction at word, sentence and document level are proposed. the issue of relevant sentence selection is discussed, and then topical and opinionated information are summarized. opinion summarizations are visualized by representative sentences. finally, an opinionated curve showing supportive and non-supportive degree along the timeline is illustrated by an opinion tracking system.",0
"a few recent studies in this field explained the use of neural networks in sentiment classification. zhu jian (2010) proposed an individual model based on artificial neural networks to divide the movie review corpus into positive , negative and fuzzy tone which is based on the advanced recursive least squares back propagation training algorithm. long-sheng chen (2011) proposed a neural network based approach, which combines the advantages of the machine learning techniques and the information retrieval techniques. 3.2.semantic orientation",1
"[30] popescu, a. m., etzioni, o.: extracting product features and opinions from reviews, in proc. conf. human language technology and empirical methods in natural language processing, vancouver, british columbia, 2005, 339-346.",1
"[26] melville, wojciech gryc, ""sentiment analysis of blogs by combining lexical knowledge with text classification"", kdd""09, june 28-july 1, 2009, paris, france.copyright 2009 acm 978-1-60558-495-9/09/06.",1
"in web fountain"",proceedings of 21st international conference on data engineering, pp. 1073-1083, washington dc,2005. [42] yongyong zhail, yanxiang chenl, xuegang hu, ""extracting opinion features in sentiment patterns , international conference on information, networking and automation (icina),2010.",2
"languages that have been studied mostly are english and in chinese .presently, there are very few researches conducted on sentiment classification for other languages like arabic, italian and thai. this survey aims at focusing much of the work in english and a few from chinese. the emergence of sentiment analysis dates back to late 1990""s, but becomes a major emerging sub field of information management discipline only from 2000, especially from 2004 onwards, which this survey focuses. for the sake of convenience the remainder of this paper is organized as follows: section 2 presents the data",1
"languages that have been studied mostly are english and in chinese .presently, there are very few researches conducted on sentiment classification for other languages like arabic, italian and thai. this survey aims at focusing much of the work in english and a few from chinese. the emergence of sentiment analysis dates back to late 1990""s, but becomes a major emerging sub field of information management discipline only from 2000, especially from 2004 onwards, which this survey focuses. for the sake of convenience the remainder of this paper is organized as follows: section 2 presents the data",1
"[1] andrea esuli and fabrizio sebastiani, ""determining the semantic gloss classification"",proceedings of 14th acm international conference on information and knowledge management,pp. 617-624, bremen, germany, 2005. bai, and r. padman, ""markov blankets and meta-heuristic search: sentiment extraction from unstructured text, lecture notes in computer science, vol. 3932, pp. 167-187, 2006.",2
"sentiment classification is done by constructing a text classifier by extracting association rules that associate the terms of a document and its categories, by modeling the text documents as a collection of transactions where each transaction represents a text document, and the items in the transaction are the terms selected from the document and the categories the document is assigned to. then, the system discovers associations between the words in documents and the labels assigned to them. each category is considered as a separate text collection and the association rule mining is applied to it. the rules generated from all the categories separately are combined together to form the classifier (weitong huang, 2008). then the training set is used to evaluate the classification quality, for classifying the test text documents, the number of rules covered, and attributive probability are used. yulan he (2010) attempted to create a novel framework from unlabeled documents. the process begins with a collection of un-annotated text and a sentiment lexicon. an initial classifier is trained by incorporating prior information from the sentiment lexicon which consists of a list of words marked with their respective polarity. the labeled features use them directly to constrain model""s predictions on unlabeled instances using generalized expectation criteria. the initially-trained classifier using generalized expectation is then applied on the unannotated text and the documents labeled with high confidence are fed into the self-learned features extractor to acquire domain-dependent features automatically. such self-learned features are subsequently used to train another classifier which is then applied on the test set to obtain the final results.",1
"volume 2, issue 6, june 2012 www.ijarcsse.com electronics and kitchen appliances, with 1000 positive sources used for opinion mining. section 3 introduces and 1000 negative reviews for each domain. another machine learning and semantic orientation approaches for review is sentiment classification. section 4 presents some http://www.cs.uic.edu/liub/fbs/customerreviewdata.zi applications of sentiment classification. then we present p. this dataset consists of reviews of five electronics some tools available for sentiment classification in products downloaded from amazon and cnet (hu and section 4. the fifth section is about the performance liu ,2006; konig & brill ,2006 ; long sheng ,2011; zhu evaluation done. last section concludes our study and jian ,2010 ; pang and lee ,2004; bai et al. ,2005; discusses some future directions for research. kennedy and inkpen ,2006; zhou and chaovalit ,2008; yulan he 2010; rudy prabowo ,2009; rui xia ,2011).",1
"hu""s work in (hu, 2005) can be considered as the pioneer work on feature-based opinion summarization. their feature extraction algorithm is based on heuristics that depend on feature terms respective occurrence counts. they use association rule mining based on the apriori algorithm to extract frequent itemsets as explicit product features. popescu et al (2005) developed an",1
"sentiment classification is done by constructing a text classifier by extracting association rules that associate the terms of a document and its categories, by modeling the text documents as a collection of transactions where each transaction represents a text document, and the items in the transaction are the terms selected from the document and the categories the document is assigned to. then, the system discovers associations between the words in documents and the labels assigned to them. each category is considered as a separate text collection and the association rule mining is applied to it. the rules generated from all the categories separately are combined together to form the classifier (weitong huang, 2008). then the training set is used to evaluate the classification quality, for classifying the test text documents, the number of rules covered, and attributive probability are used. yulan he (2010) attempted to create a novel framework from unlabeled documents. the process begins with a collection of un-annotated text and a sentiment lexicon. an initial classifier is trained by incorporating prior information from the sentiment lexicon which consists of a list of words marked with their respective polarity. the labeled features use them directly to constrain model""s predictions on unlabeled instances using generalized expectation criteria. the initially-trained classifier using generalized expectation is then applied on the unannotated text and the documents labeled with high confidence are fed into the self-learned features extractor to acquire domain-dependent features automatically. such self-learned features are subsequently used to train another classifier which is then applied on the test set to obtain the final results.",1
"kennedy and inkpen (2005) evaluate a negation model which is fairly identical to the one proposed by polanyi and zaenen in document-level polarity classification. a simple scope for negation is chosen. a polar expression is thought to be negated if the negation word immediately precedes it. wilson et al. (2005) carry out more advanced negation modeling on expressionlevel polarity classification. the work uses supervised machine learning where negation modeling is mostly encoded as features using polar expressions. jin-cheon na (2005), reported a study in automatically classifying documents as expressing positive or negative.he investigated the use of simple linguistic processing to address the problems of negation phrase.",2
"[15] hu, liu and junsheng cheng, ""opinionobserver: analyzing and comparing opinions on theweb"", proceedings of 14th international conference onworldwideweb, pp. 342-351, chiba, japan, 2005.",2
"in sentiment analysis, the most prominent work examining the impact of different scope models for negation is jia et al. (2009). they proposed a scope static detection method to handle negation using delimiters, dynamic delimiters, and heuristic rules focused on polar expressions static delimiters are unambiguous words, such as because or unless marking the beginning of another clause. dynamic delimiters are, however, rules, using contextual information such as their pertaining part-of-speech tag. these delimiters suitably account for various complex sentence types so that only the clause containing the negation is considered. the heuristic rules focus on cases in which polar expressions in specific syntactic configurations are directly preceded by negation words which results in the polar expression becoming a delimiter itself.",1
"a few recent studies in this field explained the use of neural networks in sentiment classification. zhu jian (2010) proposed an individual model based on artificial neural networks to divide the movie review corpus into positive , negative and fuzzy tone which is based on the advanced recursive least squares back propagation training algorithm. long-sheng chen (2011) proposed a neural network based approach, which combines the advantages of the machine learning techniques and the information retrieval techniques. 3.2.semantic orientation",1
"[3] bing xu, tie-jun zhao, de-quan zheng, shan-yu wang, ""product features mining based on conditional random fields model , proceedings of the ninth international conference on machine learning and cybernetics, qingdao, 11-14 july 2010 [44] zhu jian , xu chen, wang han-shi, "" sentiment classification using the theory of anns"", the journal of china universities of posts and telecommunications, july 2010, 17(suppl.): 58-62 .",1
"article * june 2012 volume 2, issue 6, june 2012 issn: 2277 128x international journal of advanced research in computer science and software engineering research paper available online at: www.ijarcsse.com volume 2, issue 6, june 2012 www.ijarcsse.com electronics and kitchen appliances, with 1000 positive sources used for opinion mining. section 3 introduces and 1000 negative reviews for each domain. another machine learning and semantic orientation approaches for review is sentiment classification. section 4 presents some http://www.cs.uic.edu/liub/fbs/customerreviewdata.zi applications of sentiment classification. then we present p. this dataset consists of reviews of five electronics some tools available for sentiment classification in products downloaded from amazon and cnet (hu and section 4. the fifth section is about the performance liu ,2006; konig & brill ,2006 ; long sheng ,2011; zhu evaluation done. last section concludes our study and jian ,2010 ; pang and lee ,2004; bai et al. ,2005; discusses some future directions for research. kennedy and inkpen ,2006; zhou and chaovalit ,2008; yulan he 2010; rudy prabowo ,2009; rui xia ,2011). volume 2, issue 6, june 2012 www.ijarcsse.com performance for sentiment classification. when applying and qiang ye, 2009). the basic idea is to estimate the svm, naive bayes and n-gram model to the destination probabilities of categories given a test document by using reviews, ye et al. (2009) found that svm outperforms the joint probabilities of words and categories. the naive the other two classifiers. part of such a model is the assumption of word independence. the simplicity of this assumption makes the computation of naive bayes classifier far more efficient. volume 2, issue 6, june 2012 www.ijarcsse.com wordnet. their basic assumption is terms with similar assumption, orientation tend to have similar glosses. they determined achieves better performance than svm. the expanded seed term""s semantic orientation through gloss classification by statistical technique. volume 2, issue 6, june 2012 www.ijarcsse.com unsupervised information extraction system called shown that there are many other words that invert the opine, which extracted product features and opinions polarity of an opinion expressed, such as valence shifters, from reviews. opine first extracts noun phrases from connectives or modals. ""i find the functionality of the reviews and retains those with frequency greater than an new mobile less practical"", is an example for valence experimentally set threshold and then assesses those by shifter, ""perhaps it is a great phone, but i fail to see opine""s feature assessor for extracting explicit features. why"", shows the effect of connectives. an example the assessor evaluates a noun phrase by computing a sentence using modal is, ""in theory, the phone should point-wise mutual information score between the phrase have worked even under water"". as can be seen from and meronymy discriminators associated with the product these examples, negation is a difficult yet important class. popescu et al apply manual extraction rules in aspect of sentiment analysis. order to find the opinion words. volume 2, issue 6, june 2012 www.ijarcsse.com system shows the results in a graph format showing opinion of the product feature by feature (bing liu, 2005). volume 2, issue 6, june 2012 www.ijarcsse.com volume 2, issue 6, june 2012 www.ijarcsse.com table 1. summary of the survey volume 2, issue 6, june 2012 www.ijarcsse.com table 1. summary of the survey (continued) volume 2, issue 6, june 2012 www.ijarcsse.com competitive intelligence"", decision support systems 50 references (2011) 743-754. volume 2, issue 6, june 2012 www.ijarcsse.com",1
"much of the research in unsupervised sentiment classification makes use of lexical resources available. kamps et al (2004) focused on the use of lexical relations in sentiment classification. andrea esuli and fabrizio sebastiani (2005) proposed semi-supervised learning method started from expanding an initial seed set using (c) 2012, ijarcsse all rights reserved page | 285",1
"khairullah khan et al (2010) developed a method to find features of product from user review in an efficient way from text through auxiliary verbs (av) {is, was, are, the were, has, have, had}. from experiments, they found that 82% of features and 85% of opinion-oriented sentences include avs. most of existing methods utilize a rule-based mechanism or statistics to extract opinion features, but they ignore the structure characteristics of reviews. the performance has hence not been promising.",1
"languages that have been studied mostly are english and in chinese .presently, there are very few researches conducted on sentiment classification for other languages like arabic, italian and thai. this survey aims at focusing much of the work in english and a few from chinese. the emergence of sentiment analysis dates back to late 1990""s, but becomes a major emerging sub field of information management discipline only from 2000, especially from 2004 onwards, which this survey focuses. for the sake of convenience the remainder of this paper is organized as follows: section 2 presents the data",1
"when faced with tremendous amounts of online information from various online forums, information seekers usually find it very difficult to yield accurate information that is useful to them. this has motivated the research on identification of online forum hotspots, where useful information is quickly exposed to those seekers. nan li (2010) used sentiment analysis approach to provide a comprehensive and timely description of the interacting structural natural groupings of various forums, which will dynamically enable efficient detection of hotspot forums.",1
"algorithm by extracting the sentiment phrases of each review by rules of part-of-speech (pos) patterns was investigated by ting-chun peng and chia-chun shih (2010). for each unknown sentiment phrase, they used it as a query term to get top-n relevant snippets from a search engine respectively. next, by using a gathered sentiment lexicon, predictive sentiments of unknown sentiment phrases are computed based on the sentiments of nearby known sentiment words inside the snippets. they consider only opinionated sentences containing at least one detected sentiment phrase for opinion extraction. using the pos pattern opinion extraction is done. gang li & fei liu (2010) developed an approach based on the k-means clustering algorithm. the technique of tf-idf (term frequency inverse document frequency) weighting is applied on the raw data. then, a voting mechanism is used to extract a more stable clustering result. the result is obtained based on multiple implementations of the clustering process. finally, the term score is used to further enhance the clustering result. documents are clustered into positive group and negative group.",1
"volume 2, issue 6, june 2012 www.ijarcsse.com system shows the results in a graph format showing opinion of the product feature by feature (bing liu, 2005).",1
"2.1. blogs with an increasing usage of the internet, blogging and blog pages are growing rapidly. blog pages have become the most popular means to express one""s personal opinions. bloggers record the daily events in their lives and express their opinions, feelings, and emotions in a blog (chau & xu, 2007). many of these blogs contain reviews on many products, issues, etc. blogs are used as a source of opinion in many of the studies related to sentiment analysis (martin, 2005; murphy, 2006; tang et al., 2009).",0
"naive bayes is a simple but effective classification algorithm. the naive bayes algorithm is widely used algorithm for document classification (melville et al., 2009; rui xia, 2011; ziqiong, 2011; songho tan, 2008",1
"2.2. review sites for any user in making a purchasing decision, the opinions of others can be an important factor. a large and growing body of user-generated reviews is available on the internet. the reviews for products or services are usually based on opinions expressed in much unstructured format. the reviewer""s data used in most of the sentiment classification studies are collected from the e-commerce websites like www.amazon.com (product reviews), reviews), www.yelp.com www.cnet download.com reviews) and www.reviewcentre.com, which hosts millions of product reviews by consumers. other than these the available are professional review sites such as www.dpreview.com , www.zdnet.com and consumer opinion sites on broad topics and products such as www .consumerreview.com, (popescu& www.epinions.com, etzioni ,2005 ; hu,b.liu ,2006 ; qinliang mia, 2009; gamgaran somprasertsi ,2010).",1
"2.1. blogs with an increasing usage of the internet, blogging and blog pages are growing rapidly. blog pages have become the most popular means to express one""s personal opinions. bloggers record the daily events in their lives and express their opinions, feelings, and emotions in a blog (chau & xu, 2007). many of these blogs contain reviews on many products, issues, etc. blogs are used as a source of opinion in many of the studies related to sentiment analysis (martin, 2005; murphy, 2006; tang et al., 2009).",0
"kennedy and inkpen (2005) evaluate a negation model which is fairly identical to the one proposed by polanyi and zaenen in document-level polarity classification. a simple scope for negation is chosen. a polar expression is thought to be negated if the negation word immediately precedes it. wilson et al. (2005) carry out more advanced negation modeling on expressionlevel polarity classification. the work uses supervised machine learning where negation modeling is mostly encoded as features using polar expressions. jin-cheon na (2005), reported a study in automatically classifying documents as expressing positive or negative.he investigated the use of simple linguistic processing to address the problems of negation phrase.",2
"web fountain uses beginning definite base noun phrase (bbnp) heuristic for extracting product features. to assign sentiments to the features, reviews are parsed and traversed with two linguistic resources namely the sentiment lexicon and the sentiment pattern database. the sentiment lexicon defines the polarity of terms and sentiment pattern database defines sentiment extraction patterns for a sentence predicates (yi and niblack, 2005). red opal is a tool that enables users to find products based on features. it scores each product based on features from the customer reviews (christopher scaffidi, 2007). opinion observer is a sentiment analysis system for analyzing and comparing opinions on the web. the",2
"[38] tsur, d. davidov, and a. rappoport , a great catchy name: semi-supervised recognition of sarcastic sentences in online product reviews"". in proceeding of icwsm. of context dependent opinions,2010.",1
"hu""s work in (hu, 2005) can be considered as the pioneer work on feature-based opinion summarization. their feature extraction algorithm is based on heuristics that depend on feature terms respective occurrence counts. they use association rule mining based on the apriori algorithm to extract frequent itemsets as explicit product features. popescu et al (2005) developed an",1
"[19] kamps, maarten marx, robert j. mokken and maarten de rijke, ""using wordnet to measure semantic orientation of adjectives"", proceedings of 4th international conference on language resources and evaluation, pp. 1115-1118, lisbon, portugal, 2004.",2
"rudy prabowo (2009) described an extension by combining rule-based classification, supervised learning and machine learning into a new combined method. for each sample set, they carried out 10-fold cross validation. for each fold, the associated samples were divided into training and a test set. for each test sample, a hybrid classification is carried out, i.e., if one classifier fails to classify a document, the classifier passes the document onto the next classifier, until the document is classified or no other classifier exists. given a training set, the rule based classifier (rbc) used a rule generator to generate a set of rules and a set of antecedents to represent the test sample and used the rule set derived from the training set to classify the test sample. if the test sample was unclassified, the rbc passed the associated antecedents onto the statistic based classifier (sbc), if the sbc could not classify the test sample; the sbc passed the associated antecedents onto the general inquirer based classifier (gibc), which used the 3672 simple rules to determine the consequents of the antecedents. the support vector machine (svm) was given a training set to classify the test sample if the three classifiers failed to classify the same.",1
"support vector machines (svm), a discriminative classifier is considered the best text classification method (rui xia, 2011; ziqiong, 2011; songho tan, 2008 and rudy prabowo, 2009). . the support vector machine is a statistical classification method proposed by vapnik . based on the structural risk minimization principle from the computational learning theory, svm seeks a decision surface to separate the training data points into two classes and makes decisions based on the support vectors that are selected as the only effective elements in the training set. multiple variants of svm have been developed in which multi class svm is used for sentiment classification (kaiquan xu, 2011). winnow is a well-known online mistaken-driven method. it works by updating its weights in a sequence of trials. on each trial, it first makes a prediction for one document and then receives feedback; if a mistake is made, it updates its weight vector using the document. during the training phase, with a collection of training data, this process is repeated several times by iterating on the data (songho tan, 2008). besides these classifiers other classifiers like id3 and c5 are also investigated (rudy prabowo, 2009).",1
"online advertising has become one of the major revenue sources of today""s internet ecosystem. sentiment analysis find its recent application in dissatisfaction oriented online advertising guang qiu(2010) and blogger-centric contextual advertising (teng-kai fan, chia-hui chang ,2011), which refers to the assignment of personal ads to any blog page, chosen in according to bloggersinterests.",2
"volume 2, issue 6, june 2012 www.ijarcsse.com competitive intelligence"", decision support systems 50 references (2011) 743-754.",1
"web fountain uses beginning definite base noun phrase (bbnp) heuristic for extracting product features. to assign sentiments to the features, reviews are parsed and traversed with two linguistic resources namely the sentiment lexicon and the sentiment pattern database. the sentiment lexicon defines the polarity of terms and sentiment pattern database defines sentiment extraction patterns for a sentence predicates (yi and niblack, 2005). red opal is a tool that enables users to find products based on features. it scores each product based on features from the customer reviews (christopher scaffidi, 2007). opinion observer is a sentiment analysis system for analyzing and comparing opinions on the web. the",2
"[4] chaovalit,lina zhou, movie review mining: a comparison between supervised and unsupervised classification approaches, proceedings of the 38th hawaii international conference on system sciences 2005.",2
"much of the research in unsupervised sentiment classification makes use of lexical resources available. kamps et al (2004) focused on the use of lexical relations in sentiment classification. andrea esuli and fabrizio sebastiani (2005) proposed semi-supervised learning method started from expanding an initial seed set using (c) 2012, ijarcsse all rights reserved page | 285",1
"algorithm by extracting the sentiment phrases of each review by rules of part-of-speech (pos) patterns was investigated by ting-chun peng and chia-chun shih (2010). for each unknown sentiment phrase, they used it as a query term to get top-n relevant snippets from a search engine respectively. next, by using a gathered sentiment lexicon, predictive sentiments of unknown sentiment phrases are computed based on the sentiments of nearby known sentiment words inside the snippets. they consider only opinionated sentences containing at least one detected sentiment phrase for opinion extraction. using the pos pattern opinion extraction is done. gang li & fei liu (2010) developed an approach based on the k-means clustering algorithm. the technique of tf-idf (term frequency inverse document frequency) weighting is applied on the raw data. then, a voting mechanism is used to extract a more stable clustering result. the result is obtained based on multiple implementations of the clustering process. finally, the term score is used to further enhance the clustering result. documents are clustered into positive group and negative group.",1
gamgarn somprasertsri (2010) dedicated their work to properly identify the semantic relationships between product features and opinions. his approach is to mine product feature and opinion based on the consideration of information by syntactic applying ontological and knowledge with probabilistic based model.,1
"analysis"", project report, standford,2009. [11] go,lei huang and richa bhayani , ""twitter sentiment classification using distant supervision"", project report, standford,2009.",1
"besides using these above said machine learning methods individually for sentiment classification, various comparative studies have been done to find the best learning method for sentiment choice of machine classification. songbo tan (2008) presents an empirical study of sentiment categorization on chinese documents. he investigated four feature selection methods (mi,ig, chi and df) and five learning methods (centroid classifier, k-nearest neighbor, winnow classifier, naive bayes and svm) on a chinese sentiment corpus. from the results he concludes that, ig performs the best for sentimental terms selection and svm exhibits the best",2
"naive bayes is a simple but effective classification algorithm. the naive bayes algorithm is widely used algorithm for document classification (melville et al., 2009; rui xia, 2011; ziqiong, 2011; songho tan, 2008 support vector machines (svm), a discriminative classifier is considered the best text classification method (rui xia, 2011; ziqiong, 2011; songho tan, 2008 and rudy prabowo, 2009). . the support vector machine is a statistical classification method proposed by vapnik . based on the structural risk minimization principle from the computational learning theory, svm seeks a decision surface to separate the training data points into two classes and makes decisions based on the support vectors that are selected as the only effective elements in the training set. multiple variants of svm have been developed in which multi class svm is used for sentiment classification (kaiquan xu, 2011). the idea behind the centroid classification algorithm is extremely simple and straightforward (songho tan, 2008). initially the prototype vector or centroid vector for each training class is calculated, then the similarity between a testing document to all centroid is computed, finally based on these similarities, document is assigned to the class corresponding to the most similar centroid. the k-nearest neighbor (knn) is a typical example based classifier that does not build an explicit, declarative representation of the category, but relies on the category labels attached to the training documents similar to the test document. given a test document d, the system finds the k nearest neighbors among training documents. the similarity score of each nearest neighbor document to the test document is used as the weight of the classes of the neighbor document (songho tan, 2008). winnow is a well-known online mistaken-driven method. it works by updating its weights in a sequence of trials. on each trial, it first makes a prediction for one document and then receives feedback; if a mistake is made, it updates its weight vector using the document. during the training phase, with a collection of training data, this process is repeated several times by iterating on the data (songho tan, 2008). besides these classifiers other classifiers like id3 and c5 are also investigated (rudy prabowo, 2009).",1
"2.1. blogs with an increasing usage of the internet, blogging and blog pages are growing rapidly. blog pages have become the most popular means to express one""s personal opinions. bloggers record the daily events in their lives and express their opinions, feelings, and emotions in a blog (chau & xu, 2007). many of these blogs contain reviews on many products, issues, etc. blogs are used as a source of opinion in many of the studies related to sentiment analysis (martin, 2005; murphy, 2006; tang et al., 2009).",0
"[43] yuanbinwu, qi zhang, xuanjing huang, lidewu, ""phrase dependency parsing for sentiment analysis"", proceedings of the 2009 conference on empirical methods in natural language processing, pages 1533-1541, singapore, 6-7 august 2009 .",1
"[14] hu, and liu, ""mining and summarizing customer reviews"", proceedings of the tenth acm sigkdd international conference on knowledge discovery and data mining, seattle, wa, usa, 2005,pp. 168-177.",2
"kennedy and inkpen (2005) evaluate a negation model which is fairly identical to the one proposed by polanyi and zaenen in document-level polarity classification. a simple scope for negation is chosen. a polar expression is thought to be negated if the negation word immediately precedes it. wilson et al. (2005) carry out more advanced negation modeling on expressionlevel polarity classification. the work uses supervised machine learning where negation modeling is mostly encoded as features using polar expressions. jin-cheon na (2005), reported a study in automatically classifying documents as expressing positive or negative.he investigated the use of simple linguistic processing to address the problems of negation phrase.",2
"when the review where an opinion lies in, cannot provide enough contextual information to determine the orientation of opinion, chunxu wu(2009) proposed an approach which resort to other reviews discussing the same topic to mine useful contextual information, then use semantic similarity measures to judge the orientation of opinion. they attempted to tackle this problem by getting the orientation of context independent opinions , then consider the context dependent opinions using linguistic rules to infer orientation of context distinctdependent opinion ,then extract contextual information from other reviews that comment on the same product feature indistinct-dependent the context opinions.",1
"an ensemble technique is one which combines the outputs of several base classification models to form an integrated output. rui xia (2011) used this approach and made a comparative study of the effectiveness of technique for sentiment classification by ensemble efficiently sets and classification algorithms to synthesize a more accurate classification procedure. in his work, two types of feature sets are designed for sentiment classification, namely the part-of-speech based feature sets and the word-relation based feature sets. then, text classification algorithms, namely naive bayes, maximum entropy and support vector machines, are employed as base-classifiers for each of the feature sets to predict classification scores. three types of ensemble methods, namely the fixed combination, weighted combination and meta-classifier combination, are evaluated for three ensemble strategies sets, ensemble of namely ensemble of classification algorithms, and ensemble of both feature sets and classification algorithms.",1
"naive bayes is a simple but effective classification algorithm. the naive bayes algorithm is widely used algorithm for document classification (melville et al., 2009; rui xia, 2011; ziqiong, 2011; songho tan, 2008 support vector machines (svm), a discriminative classifier is considered the best text classification method (rui xia, 2011; ziqiong, 2011; songho tan, 2008 and rudy prabowo, 2009). . the support vector machine is a statistical classification method proposed by vapnik . based on the structural risk minimization principle from the computational learning theory, svm seeks a decision surface to separate the training data points into two classes and makes decisions based on the support vectors that are selected as the only effective elements in the training set. multiple variants of svm have been developed in which multi class svm is used for sentiment classification (kaiquan xu, 2011).",1
"kunpeng zhang (2009), proposed a work which used a keyword matching strategy to identify and tag product features in sentences. bing xu (2010) , presented a conditional random fields model based chinese product features identification approach, integrating the chunk features and heuristic position information in addition to the word features, part-of-speech features and context features.",1
"support vector machines (svm), a discriminative classifier is considered the best text classification method (rui xia, 2011; ziqiong, 2011; songho tan, 2008 and rudy prabowo, 2009). . the support vector machine is a statistical classification method proposed by vapnik . based on the structural risk minimization principle from the computational learning theory, svm seeks a decision surface to separate the training data points into two classes and makes decisions based on the support vectors that are selected as the only effective elements in the training set. multiple variants of svm have been developed in which multi class svm is used for sentiment classification (kaiquan xu, 2011).",1
"[45] ziqiong zhang, qiang ye, zili zhang, yijun li, ""sentiment classification of internet restaurant reviews written in cantonese"", expert systems with applications xxx (2011) xxx-xxx.",1
"volume 2, issue 6, june 2012 www.ijarcsse.com performance for sentiment classification. when applying and qiang ye, 2009). the basic idea is to estimate the svm, naive bayes and n-gram model to the destination probabilities of categories given a test document by using reviews, ye et al. (2009) found that svm outperforms the joint probabilities of words and categories. the naive the other two classifiers. part of such a model is the assumption of word independence. the simplicity of this assumption makes the computation of naive bayes classifier far more efficient.",2
"volume 2, issue 6, june 2012 www.ijarcsse.com performance for sentiment classification. when applying and qiang ye, 2009). the basic idea is to estimate the svm, naive bayes and n-gram model to the destination probabilities of categories given a test document by using reviews, ye et al. (2009) found that svm outperforms the joint probabilities of words and categories. the naive the other two classifiers. part of such a model is the assumption of word independence. the simplicity of this assumption makes the computation of naive bayes classifier far more efficient.",2
"[6] christopher scaffidi, kevin bierhoff, eric chang, mikhael felker, herman ng and chun jin, ""red opal: productfeature scoring from reviews"", proceedings of 8th acm conference on electronic commerce, pp. 182-191, new york, 2007.",2
"yongyong zhail (2010) proposed a approach of opinion feature extraction based on sentiment patterns, which takes into account the structure characteristics of reviews for higher values of precision and recall. with a self constructed database of sentiment patterns, sentiment pattern matches each review sentence to obtain its features, and then filters redundant features regarding the domain, statistics and semantic relevance of similarity.",2
"kunpeng zhang (2009), proposed a work which used a keyword matching strategy to identify and tag product features in sentences. bing xu (2010) , presented a conditional random fields model based chinese product features identification approach, integrating the chunk features and heuristic position information in addition to the word features, part-of-speech features and context features.",1
"in most of the comparative studies it is found that svm outperforms other machine learning methods in sentiment classification. ziqiong zhang (2011) showed a contradiction in the performance of svm. they focused their interest on written cantonese, a written variety of chinese. they proposed a method which utilizes completely prior-knowledge-free supervised machine learning method and proved that the chosen machine learning model could be able to draw its own conclusion from the distribution of lexical elements in a piece of cantonese review. despite its unrealistic independence",1
"chaovalit and zhou (2005) compared the semantic orientation approach with the n-gram model machine learning approach by applying to movie reviews. they confirmed from the results that the machine learning approach is more accurate but requires a significant amount of time to train the model. in comparison, the semantic orientation approach is slightly less accurate but is more efficient to use in real-time applications. the performance of semantic orientation also relies on the performance of the underlying pos tagger.",1
"naive bayes is a simple but effective classification algorithm. the naive bayes algorithm is widely used algorithm for document classification (melville et al., 2009; rui xia, 2011; ziqiong, 2011; songho tan, 2008 support vector machines (svm), a discriminative classifier is considered the best text classification method (rui xia, 2011; ziqiong, 2011; songho tan, 2008 and rudy prabowo, 2009). . the support vector machine is a statistical classification method proposed by vapnik . based on the structural risk minimization principle from the computational learning theory, svm seeks a decision surface to separate the training data points into two classes and makes decisions based on the support vectors that are selected as the only effective elements in the training set. multiple variants of svm have been developed in which multi class svm is used for sentiment classification (kaiquan xu, 2011).",1
"in terms of classification algorithms, support vector machines (svms) are widely used (abbasi et al., 2008; abbasi et al., 2008; argamon et al., 2007; gamon, 2004; mishne, 2005; wilson, wiebe, & hwa, 2006) because they seem to perform as well or better than other methods in most machine learning contexts. nevertheless, with a few exceptions (read, 2005; wilson et al., 2006), explicit comparisons with other methods have not been included in opinion mining publications.",1
"ng, v., dasgupta, s., & arifin, s.m.n. (2006). examining the role of linguistic knowledge sources in the automatic identification and classification of reviews. in proceedings of the coling/acl 20th conference (pp. 611618) college park, md: association for computational linguistics. pang, b., & lee, l. (2004). sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. in proceedings of acl 2004 (pp. 271-278). college park, md: association for computational linguistics.",1
"read, j. (2005). using emoticons to reduce dependency in machine learning techniques for sentiment classification. in proceedings of the acl 2005 student research workshop (pp. 43-48). college park, md: association for computational linguistics.",2
"hancock, j.t., gee, k., ciaccio, k., & lin, j.m.-h. (2008). i'm sad you're sad: emotional contagion in cmc. in proceedings of the acm 2008 conference on computer supported cooperative work (pp. 295-298). new york: acm press.",1
"in terms of classification algorithms, support vector machines (svms) are widely used (abbasi et al., 2008; abbasi et al., 2008; argamon et al., 2007; gamon, 2004; mishne, 2005; wilson, wiebe, & hwa, 2006) because they seem to perform as well or better than other methods in most machine learning contexts. nevertheless, with a few exceptions (read, 2005; wilson et al., 2006), explicit comparisons with other methods have not been included in opinion mining publications. as well as unigrams, but the combined performance was better than that of unigrams alone (argamon et al., 2007). linguistic features have also been successfully used to extend opinion mining to a multiaspect variant that is able to detect opinions about different aspects of a topic (snyder & barzilay, 2007). a promising future approach is the incorporation of context about the reasons why sentiment is used, such as differentiating between intention, arguments, and speculation (wilson, 2008).",1
"opinion mining algorithms often use machine learning to identify general features associated with positive and negative sentiment, where these features could be a subset of the words in the document, parts of speech or n-grams (i.e., the frequency of occurrence of all n consecutive words, where n is typically 1, 2, or 3; abbasi, chen, thoms, & fu, 2008; ng, dasgupta, & arifin, 2006; tang, tan, & cheng, 2009). other features used with some success include emoticons in online movie reviews (read, 2005), which seem so be more domain-independent than words; lexico-syntactic patterns (e.g., riloff & wiebe, 2003); and artificial features derived from adjective polarity lists (ng et al., 2006). the additional features typically provide small, but significant increases in performance. rules-based methods have also been used to identify structures in sentences associated with sentiment (prabowo & thelwall, 2009; wu, chuang, & lin, 2006). two recurring machine learning issues are feature selection and classification algorithm choice.",1
"table 1 reports the degree of intercoder agreement. basic agreement rates are reported here for comparability with sentistrength. previous emotion-judgment/annotation tasks have obtained higher intercoder scores, but without strength measures and therefore having fewer categories (e.g., wiebe et al., 2005). moreover, one previous article noted that intercoder agreement was higher on longer (blog) texts (gill, gergle, french, & oberlander, 2008), suggesting that obtaining agreement on the short texts here would be difficult. the appropriate type of intercoder reliability statistic for this kind of data with multiple coders and varying differences between categories is krippendorff's a (artstein & poesio, 2008; krippendorff, 2004). using the numerical difference in emotion score as weights, the three coder a values were 0.5743 for positive and 0.5634 for negative sentiment. these values the main reason for sentistrength's relative success seems to be procedures for decoding nonstandard spellings and methods for boosting the strength of words, which accounted for much of its performance. without these factors, the sentistrength variant based solely upon a dictionary of emotion-associated words and their estimated strengths with 57.5% was only 1.3% better than the most successful machine-learning approach on an extended set of 1-3 grams. in contrast, sentistrength was able to identify negative sentiment little better (1.8%) than the baseline, probably due to creativity in expressing negative comments or due to the difficulty in getting significantly above the baseline when one category dominates (artstein & poesio, 2008; krippendorff, 2004). it seems that both positive and negative sentiment detection in informal text language like myspace comments is challenging because of several factors: language creativity, expressions of sentiment without emotion-bearing words, and differences between human",1
(c) 2010 asis&t * published online 17 august 2010 in wiley online library (wileyonlinelibrary.com). doi: 10.1002/asi.21416,1
"opinion mining typically occurs in two or three stages, although more may be needed for some tasks (e.g., balahur et al., 2010). first, the input text is split into sections, such as sentences, and each section tested to see if it contains any sentiment: if it is subjective or objective (pang & lee, 2004). second, the subjective sentences are analyzed to detect their sentiment polarity. finally, the object about which the opinion is expressed may be extracted (e.g., gamon, aue, corstonoliver, & ringger, 2005). opinion mining normally deals with only positive and negative sentiment rather than discrete emotions (e.g., happiness, surprise), does not detect sentiment strength (but sometimes uses the strength of association of words with positive or negative sentiment, e.g., kaji & kitsuregawa, 2007), and does not simultaneously identify both positive and negative emotions. nevertheless, such opinion mining research can aid the simultaneous assessment of positive and negative sentiment strength both because of its general insights into sentiment analysis and also because most techniques could, in theory, be repurposed for this new task. for example, phrase analysis techniques could be applied to identify both positive and negative sentiment even within individual sentences (choi & cardie, 2008; wilson, 2008; wilson, wiebe, & hoffman, 2009).",1
"a complicating factor for online sentiment detection is that there are many electronic communications media in which text-based communication in english seems to frequently ignore the rules of grammar and spelling. perhaps most famous is mobile phone text language with its abbreviations, emoticons, and truncated sentences (grinter & eldridge, 2003; thurlow, 2003), but similar styles are evident in many other forms of computer-mediated communication, including chatrooms, bulletin boards, and social network sites (baron, 2003; crystal, 2006). widely recognized innovations include emoticons like :-) that are reasonably effective in conveying emotion (derks, bos, & von grumbkow, 2008; fullwood & martino, 2007) and word abbreviations like m8 (mate) and u (you) (thurlow, 2003). although sometimes seen as poor language use, these are a natural response to the technological affordances and social factors associated with a system (baron, 2003; walther & parks, 2002). these variations cause problems because typical linguistic sentiment analysis programs start with part of speech tagging (e.g., brill, 1992), which is reliant upon standard spelling and grammar, and/or apply rules that assume at least correct spelling, if not correct grammar. spelling correction can be useful in this context,",1
"emotions are perceived differently by individuals, partly because of their life experiences and partly because of personality issues (barrett, 2006) and gender (stoppard & gunn gruchy, 1993). for system development, the judgments should give a consistent perspective on sentiment in the data, rather than an estimate of the population average perception. as a result, a set of same gender (female) coders was used and initial testing conducted to identify a homogeneous subset. five coders were initially selected, but two were subsequently rejected for giving anomalous results: one gave much higher positive scores than the others, and another gave generally inconsistent results. the mean of the three coders' results was calculated for each comment and rounded. this was the gold standard for the experiments. below are some examples of texts and judgments.",1
"online communication, perhaps designed to identify and intervene when inappropriate emotions are used or to identify at risk users (e.g., huang, goh, & liew, 2007), would need to be sensitive to the strength of sentiment expressed and whether participants were appropriately balancing positive and negative sentiment. in addition, basic research to understand the role of emotion in online communication (e.g., derks, fischer, & bos, 2008; e.g., hancock, gee, ciaccio, & lin, 2008; nardi, 2005) would also benefit from fine-grained sentiment detection, as would the growing body of psychology and other social science research into the role of sentiment in various types of discussion or general discourse (balahur, kozareva, & montoyo, 2009; pennebaker, mehl, & niederhoffer, 2003; short & palmer, 2008).",1
"the social network site myspace, the source of the data used in the current study, is known for its young members, its musical orientation, and its informal communication patterns (boyd, 2008a, 2008b). probably as a result of these factors 95% of english public comments exchanged between friends contain at least one abbreviation from standard english (thelwall, 2009). common features include emoticons, texting-style abbreviations, and the use of repeated letters or punctuation for emphasis (e.g., a loooong time, hi!!!). comments are typically short (mean 18.7 words, median 13 words, 68 characters; thelwall, 2009), but positive emotion is common (thelwall, wilkinson, & uppal, 2010).",2
"a complicating factor for online sentiment detection is that there are many electronic communications media in which text-based communication in english seems to frequently ignore the rules of grammar and spelling. perhaps most famous is mobile phone text language with its abbreviations, emoticons, and truncated sentences (grinter & eldridge, 2003; thurlow, 2003), but similar styles are evident in many other forms of computer-mediated communication, including chatrooms, bulletin boards, and social network sites (baron, 2003; crystal, 2006). widely recognized innovations include emoticons like :-) that are reasonably effective in conveying emotion (derks, bos, & von grumbkow, 2008; fullwood & martino, 2007) and word abbreviations like m8 (mate) and u (you) (thurlow, 2003). although sometimes seen as poor language use, these are a natural response to the technological affordances and social factors associated with a system (baron, 2003; walther & parks, 2002). these variations cause problems because typical linguistic sentiment analysis programs start with part of speech tagging (e.g., brill, 1992), which is reliant upon standard spelling and grammar, and/or apply rules that assume at least correct spelling, if not correct grammar. spelling correction can be useful in this context,",1
"to obtain reliable human judgments of a random sample of the myspace comments, two pilot exercises were undertaken with separate samples of the data (a total of 2,600 comments). these were used to identify key judgment issues and an appropriate scale. although there are many ways to measure emotion (mauss & robinson, 2009; wiebe, wilson, & cardie, 2005), human coder subjective judgments were used as an appropriate way to gather sufficient results. a set of coder instructions was drafted and refined and an online system constructed to randomly select comments and present them to the coders. one of the key outcomes from the pilot exercise was that the coders treated expressions of energy as expressions of positive sentiment unless in an explicitly negative context. for example, ""hey!!!would be interpreted as positive because it expresses energy in a context that gives no clue as to the polarity of the emotion, so it would be accepted by most coders as positive by default. in contrast, ""loser!!!would be interpreted as more negative than ""loseras the exclamation marks are associated with a negative word. consequently, the instructions were revised to explicitly state that this conflation of ostensibly neutral energy and positive sentiment was permissible.",2
"one computer science initiative has attempted to identify various emotions in text, focusing on the six so-called basic emotions (ekman, 1992; fox, 2008) of anger, disgust, fear, joy, sadness, and surprise (strapparava & mihalcea, 2008). this initiative also measured emotion strength. a human-annotated corpus was used with the coders allocating a strength from 0 to 100 for each emotion to each text (a news headline), although inter-annotator agreement was low (pearson correlations of 0.36 to 0.68, depending on the emotion). a variety of algorithms were subsequently trained on this data set. for example, one used wordnet affect lists to generate appropriate dictionaries for the six emotions. a second approach used a naive bayes classifier trained on sets of livejournal blogs annotated by their owners with one of the six emotions. the best system (for fine-grained evaluation) was one previously designed for newspaper headlines, upar7 (chaumartin, 2007), which used linguistic parsing and tagging as well as wordnet, sentiwordnet, and wordnet affect, hence relying upon reasonably correct standard grammar and spelling.",1
"opinion mining algorithms often use machine learning to identify general features associated with positive and negative sentiment, where these features could be a subset of the words in the document, parts of speech or n-grams (i.e., the frequency of occurrence of all n consecutive words, where n is typically 1, 2, or 3; abbasi, chen, thoms, & fu, 2008; ng, dasgupta, & arifin, 2006; tang, tan, & cheng, 2009). other features used with some success include emoticons in online movie reviews (read, 2005), which seem so be more domain-independent than words; lexico-syntactic patterns (e.g., riloff & wiebe, 2003); and artificial features derived from adjective polarity lists (ng et al., 2006). the additional features typically provide small, but significant increases in performance. rules-based methods have also been used to identify structures in sentences associated with sentiment (prabowo & thelwall, 2009; wu, chuang, & lin, 2006). two recurring machine learning issues are feature selection and classification algorithm choice.",1
"grinter, r.e., & eldridge, m. (2003). wan2tlk? everyday text messaging. in proceedings of computer-human interaction conference (chi 2003) (pp. 441-448). new york: acm press.",2
"opinion mining typically occurs in two or three stages, although more may be needed for some tasks (e.g., balahur et al., 2010). first, the input text is split into sections, such as sentences, and each section tested to see if it contains any sentiment: if it is subjective or objective (pang & lee, 2004). second, the subjective sentences are analyzed to detect their sentiment polarity. finally, the object about which the opinion is expressed may be extracted (e.g., gamon, aue, corstonoliver, & ringger, 2005). opinion mining normally deals with only positive and negative sentiment rather than discrete emotions (e.g., happiness, surprise), does not detect sentiment strength (but sometimes uses the strength of association of words with positive or negative sentiment, e.g., kaji & kitsuregawa, 2007), and does not simultaneously identify both positive and negative emotions. nevertheless, such opinion mining research can aid the simultaneous assessment of positive and negative sentiment strength both because of its general insights into sentiment analysis and also because most techniques could, in theory, be repurposed for this new task. for example, phrase analysis techniques could be applied to identify both positive and negative sentiment even within individual sentences (choi & cardie, 2008; wilson, 2008; wilson, wiebe, & hoffman, 2009). an alternative opinion mining technique has used a primarily linguistic approach: simple rules based upon compositional semantics (information about likely meanings of a word based upon the surrounding text) to detect the polarity of an expression (choi & cardie, 2008). this gives good results on phrases in newswire documents that are manually coded as having at least medium level positive or negative sentiment. this approach seems particularly suited to cases where there is a large volume of grammatically correct text from which rules can be learned. nevertheless, a study of poor grammatical quality texts in online customer feedback showed that linguistic approaches could improve classification slightly when added to bag of words (1-grams) approaches, although aggressive feature reduction had a similar impact to adding linguistic features (gamon, 2004). the improvement was probably due to the large data set available (40,884 documents with an average of 2.26 sentences each), as has been previously claimed for an analysis of informal text (mishne, 2005). another approach used a lexicon of appraisal adjectives (e.g., ""sort of,""very"") together with an orientation lexicon to detect movie review polarity. this did not perform",1
"sentiment strength classification has also been developed for a three-level scheme (low, medium, and high or extreme) for subjective sentences or clauses in newswire texts using a linguistic analysis converting sentences into dependency trees reflecting their structure (wilson et al., 2006). adding dependency trees to unigrams substantially improved the performance of various classifiers compared to unigrams alone, perhaps helped by the fairly large training set (9,313 sentences), the (presumably) good quality grammar of the texts, and the fairly low initial performance on this task (34.5% to 50.9% for unigrams, rising to 48.3% to 55.0% for the three types of classifier applied to level 1 clauses). here, svm regression was outperformed by both the rule-based learning ripper (cohen, 1995) and boostexter, a boosting algorithm combining multiple weak classifiers (schapire & singer, 2000).",1
"psychology of emotion research argues that though positive and negative sentiment are important dimensions, there are many different widely socially recognized types of emotion and the strength of emotions (arousal level) can vary (e.g., cornelius, 1996; fox, 2008). in the dimensional model of emotion from psychology (russell, 1979), sentiment can always be fundamentally split into two axes: arousal (low to high) and valence (positive to negative). although this model is useful, other research has shown that positive and negative sentiment can coexist (e.g., fox, 2008, p. 127) and are relatively independent in many contexts--particularly when sentiment levels are not extreme and over longer time periods (diener & emmons, 1984; huppert & whittington, 2003; watson, 1988; watson, clark, & tellegen, 1988) and so it also seems reasonable to conceive sentiment as separately measurable positive and negative components, as encoded in a popular psychology research instrument (watson et al., 1988).",2
"a complicating factor for online sentiment detection is that there are many electronic communications media in which text-based communication in english seems to frequently ignore the rules of grammar and spelling. perhaps most famous is mobile phone text language with its abbreviations, emoticons, and truncated sentences (grinter & eldridge, 2003; thurlow, 2003), but similar styles are evident in many other forms of computer-mediated communication, including chatrooms, bulletin boards, and social network sites (baron, 2003; crystal, 2006). widely recognized innovations include emoticons like :-) that are reasonably effective in conveying emotion (derks, bos, & von grumbkow, 2008; fullwood & martino, 2007) and word abbreviations like m8 (mate) and u (you) (thurlow, 2003). although sometimes seen as poor language use, these are a natural response to the technological affordances and social factors associated with a system (baron, 2003; walther & parks, 2002). these variations cause problems because typical linguistic sentiment analysis programs start with part of speech tagging (e.g., brill, 1992), which is reliant upon standard spelling and grammar, and/or apply rules that assume at least correct spelling, if not correct grammar. spelling correction can be useful in this context,",1
"many other approaches have also been used to detect sentiment in text. one is to have a dictionary of positive and negative words (e.g., love, hate), such as that found in general inquirer (stone, dunphy, smith, & ogilvie, 1966), wordnet affect (strapparava & valitutti, 2004), sentiwordnet (baccianella, esuli, & sebastiani, 2010; esuli & sebastiani, 2006) or q-wordnet (agerri & garcia-serrano, 2010), and to count how often they occur. modifications of this approach include the identification of negating terms (das & chen, 2001), words that enhance sentiment in other words (e.g., really love, absolutely hate) and overall sentence structures (turney, 2002). a more sophisticated approach is to identify text features that could potentially be subjective in some contexts and then use contextual information to decide whether they are subjective in each new context (wiebe, wilson, bruce, bell, & martin, 2004).",1
"myspace was chosen as a source of test data for this study because it is a public environment containing a large quantity of informal text language and is important in its own right as one of the most visited web sites in the world in 2009. a random sample of myspace comments was taken by examining the profiles of every 15th member that joined on june 18, 2007, up to 40,000 and selecting those with a declared u.s. nationality and a public profile not of a musician, comedian, or film-maker. of these, those with less than two friends or no comments were rejected as inactive and those with over 1,000 friends or 4,000 comments were rejected as abnormal. a commenting friend was then identified for each remaining member, satisfying the same criteria above, and a random comment selected from each direction of communication between the two. the comments were extracted in december 2008. this produced a large essentially random sample of u.s. commenter-commentee messages. spam comments and chain messages were subsequently eliminated, as were comments containing images.",1
"opinion mining, also known as sentiment analysis, is the extraction of positive or negative opinions from (unstructured) text (pang & lee, 2008). the many applications of opinion mining include detecting movie popularity from multiple online reviews and diagnosing which parts of a vehicle are liked or disliked by owners through their comments in a dedicated site or forum. there are also applications unrelated to marketing, such as differentiating between emotional and informative social media content (denecke & nejdl, 2009).",1
"psychology of emotion research argues that though positive and negative sentiment are important dimensions, there are many different widely socially recognized types of emotion and the strength of emotions (arousal level) can vary (e.g., cornelius, 1996; fox, 2008). in the dimensional model of emotion from psychology (russell, 1979), sentiment can always be fundamentally split into two axes: arousal (low to high) and valence (positive to negative). although this model is useful, other research has shown that positive and negative sentiment can coexist (e.g., fox, 2008, p. 127) and are relatively independent in many contexts--particularly when sentiment levels are not extreme and over longer time periods (diener & emmons, 1984; huppert & whittington, 2003; watson, 1988; watson, clark, & tellegen, 1988) and so it also seems reasonable to conceive sentiment as separately measurable positive and negative components, as encoded in a popular psychology research instrument (watson et al., 1988).",2
"one computer science initiative has attempted to identify various emotions in text, focusing on the six so-called basic emotions (ekman, 1992; fox, 2008) of anger, disgust, fear, joy, sadness, and surprise (strapparava & mihalcea, 2008). this initiative also measured emotion strength. a human-annotated corpus was used with the coders allocating a strength from 0 to 100 for each emotion to each text (a news headline), although inter-annotator agreement was low (pearson correlations of 0.36 to 0.68, depending on the emotion). a variety of algorithms were subsequently trained on this data set. for example, one used wordnet affect lists to generate appropriate dictionaries for the six emotions. a second approach used a naive bayes classifier trained on sets of livejournal blogs annotated by their owners with one of the six emotions. the best system (for fine-grained evaluation) was one previously designed for newspaper headlines, upar7 (chaumartin, 2007), which used linguistic parsing and tagging as well as wordnet, sentiwordnet, and wordnet affect, hence relying upon reasonably correct standard grammar and spelling.",1
"riloff, e., & wiebe, j. (2003). learning extraction patterns for subjective expressions. in proceedings of the 2003 conference on empirical methods in natural language processing (emnlp-03). college park, md: association for computational linguistics. retrieved april 11, 2010, from http://www.cs.utah.edu/~riloff/pdfs/emnlp2003.pdf",1
"many other approaches have also been used to detect sentiment in text. one is to have a dictionary of positive and negative words (e.g., love, hate), such as that found in general inquirer (stone, dunphy, smith, & ogilvie, 1966), wordnet affect (strapparava & valitutti, 2004), sentiwordnet (baccianella, esuli, & sebastiani, 2010; esuli & sebastiani, 2006) or q-wordnet (agerri & garcia-serrano, 2010), and to count how often they occur. modifications of this approach include the identification of negating terms (das & chen, 2001), words that enhance sentiment in other words (e.g., really love, absolutely hate) and overall sentence structures (turney, 2002). a more sophisticated approach is to identify text features that could potentially be subjective in some contexts and then use contextual information to decide whether they are subjective in each new context (wiebe, wilson, bruce, bell, & martin, 2004).",1
"psychology of emotion research argues that though positive and negative sentiment are important dimensions, there are many different widely socially recognized types of emotion and the strength of emotions (arousal level) can vary (e.g., cornelius, 1996; fox, 2008). in the dimensional model of emotion from psychology (russell, 1979), sentiment can always be fundamentally split into two axes: arousal (low to high) and valence (positive to negative). although this model is useful, other research has shown that positive and negative sentiment can coexist (e.g., fox, 2008, p. 127) and are relatively independent in many contexts--particularly when sentiment levels are not extreme and over longer time periods (diener & emmons, 1984; huppert & whittington, 2003; watson, 1988; watson, clark, & tellegen, 1988) and so it also seems reasonable to conceive sentiment as separately measurable positive and negative components, as encoded in a popular psychology research instrument (watson et al., 1988). one computer science initiative has attempted to identify various emotions in text, focusing on the six so-called basic emotions (ekman, 1992; fox, 2008) of anger, disgust, fear, joy, sadness, and surprise (strapparava & mihalcea, 2008). this initiative also measured emotion strength. a human-annotated corpus was used with the coders allocating a strength from 0 to 100 for each emotion to each text (a news headline), although inter-annotator agreement was low (pearson correlations of 0.36 to 0.68, depending on the emotion). a variety of algorithms were subsequently trained on this data set. for example, one used wordnet affect lists to generate appropriate dictionaries for the six emotions. a second approach used a naive bayes classifier trained on sets of livejournal blogs annotated by their owners with one of the six emotions. the best system (for fine-grained evaluation) was one previously designed for newspaper headlines, upar7 (chaumartin, 2007), which used linguistic parsing and tagging as well as wordnet, sentiwordnet, and wordnet affect, hence relying upon reasonably correct standard grammar and spelling.",2
"psychology of emotion research argues that though positive and negative sentiment are important dimensions, there are many different widely socially recognized types of emotion and the strength of emotions (arousal level) can vary (e.g., cornelius, 1996; fox, 2008). in the dimensional model of emotion from psychology (russell, 1979), sentiment can always be fundamentally split into two axes: arousal (low to high) and valence (positive to negative). although this model is useful, other research has shown that positive and negative sentiment can coexist (e.g., fox, 2008, p. 127) and are relatively independent in many contexts--particularly when sentiment levels are not extreme and over longer time periods (diener & emmons, 1984; huppert & whittington, 2003; watson, 1988; watson, clark, & tellegen, 1988) and so it also seems reasonable to conceive sentiment as separately measurable positive and negative components, as encoded in a popular psychology research instrument (watson et al., 1988).",2
"researchers to investigate the connection between language and psychology (pennebaker et al., 2003) and also as a practical tool, for example, to detect how well people are likely to cope with bereavement based upon their language use (pennebaker, mayne, & francis, 1997). a related emotion detection approach differentiates between happy, unhappy. and neutral states based upon words used by students describing their daily lives (wu et al., 2006). this is similar to the typical positive/negative/neutral objective for opinion mining, however.",1
"opinion mining algorithms often use machine learning to identify general features associated with positive and negative sentiment, where these features could be a subset of the words in the document, parts of speech or n-grams (i.e., the frequency of occurrence of all n consecutive words, where n is typically 1, 2, or 3; abbasi, chen, thoms, & fu, 2008; ng, dasgupta, & arifin, 2006; tang, tan, & cheng, 2009). other features used with some success include emoticons in online movie reviews (read, 2005), which seem so be more domain-independent than words; lexico-syntactic patterns (e.g., riloff & wiebe, 2003); and artificial features derived from adjective polarity lists (ng et al., 2006). the additional features typically provide small, but significant increases in performance. rules-based methods have also been used to identify structures in sentences associated with sentiment (prabowo & thelwall, 2009; wu, chuang, & lin, 2006). two recurring machine learning issues are feature selection and classification algorithm choice.",1
"a complicating factor for online sentiment detection is that there are many electronic communications media in which text-based communication in english seems to frequently ignore the rules of grammar and spelling. perhaps most famous is mobile phone text language with its abbreviations, emoticons, and truncated sentences (grinter & eldridge, 2003; thurlow, 2003), but similar styles are evident in many other forms of computer-mediated communication, including chatrooms, bulletin boards, and social network sites (baron, 2003; crystal, 2006). widely recognized innovations include emoticons like :-) that are reasonably effective in conveying emotion (derks, bos, & von grumbkow, 2008; fullwood & martino, 2007) and word abbreviations like m8 (mate) and u (you) (thurlow, 2003). although sometimes seen as poor language use, these are a natural response to the technological affordances and social factors associated with a system (baron, 2003; walther & parks, 2002). these variations cause problems because typical linguistic sentiment analysis programs start with part of speech tagging (e.g., brill, 1992), which is reliant upon standard spelling and grammar, and/or apply rules that assume at least correct spelling, if not correct grammar. spelling correction can be useful in this context,",1
"information gain (riloff, patwardhan, & wiebe, 2006), or log likelihood (gamon, 2004). when using n-grams (and lexico-syntactic patterns) small improvements can also be made by pruning the feature set of features that are subsumed by simpler features that have stronger information gain values (riloff et al., 2006). for example, if ""lovehas a much higher information gain value than ""i lovethen the bigram can be eliminated without much risk of loss of power for the subsequent classification. an entropy-weighted genetic algorithm can also perform better than standard feature reduction approaches (abbasi, chen, & salem, 2008). in terms of classification algorithms, support vector machines (svms) are widely used (abbasi et al., 2008; abbasi et al., 2008; argamon et al., 2007; gamon, 2004; mishne, 2005; wilson, wiebe, & hwa, 2006) because they seem to perform as well or better than other methods in most machine learning contexts. nevertheless, with a few exceptions (read, 2005; wilson et al., 2006), explicit comparisons with other methods have not been included in opinion mining publications. an alternative opinion mining technique has used a primarily linguistic approach: simple rules based upon compositional semantics (information about likely meanings of a word based upon the surrounding text) to detect the polarity of an expression (choi & cardie, 2008). this gives good results on phrases in newswire documents that are manually coded as having at least medium level positive or negative sentiment. this approach seems particularly suited to cases where there is a large volume of grammatically correct text from which rules can be learned. nevertheless, a study of poor grammatical quality texts in online customer feedback showed that linguistic approaches could improve classification slightly when added to bag of words (1-grams) approaches, although aggressive feature reduction had a similar impact to adding linguistic features (gamon, 2004). the improvement was probably due to the large data set available (40,884 documents with an average of 2.26 sentences each), as has been previously claimed for an analysis of informal text (mishne, 2005). another approach used a lexicon of appraisal adjectives (e.g., ""sort of,""very"") together with an orientation lexicon to detect movie review polarity. this did not perform in terms of future work, a next logical step is to attempt to improve the performance of the system through linguistic processing, despite the poor grammar of the short informal text messages analyzed. previous work has shown that this approach is promising, particularly via dependency trees (wilson et al., 2009) and that, given a large enough training sample, improvements may be possible even in poor-quality text (gamon, 2004).",1
"many other approaches have also been used to detect sentiment in text. one is to have a dictionary of positive and negative words (e.g., love, hate), such as that found in general inquirer (stone, dunphy, smith, & ogilvie, 1966), wordnet affect (strapparava & valitutti, 2004), sentiwordnet (baccianella, esuli, & sebastiani, 2010; esuli & sebastiani, 2006) or q-wordnet (agerri & garcia-serrano, 2010), and to count how often they occur. modifications of this approach include the identification of negating terms (das & chen, 2001), words that enhance sentiment in other words (e.g., really love, absolutely hate) and overall sentence structures (turney, 2002). a more sophisticated approach is to identify text features that could potentially be subjective in some contexts and then use contextual information to decide whether they are subjective in each new context (wiebe, wilson, bruce, bell, & martin, 2004).",1
"a complicating factor for online sentiment detection is that there are many electronic communications media in which text-based communication in english seems to frequently ignore the rules of grammar and spelling. perhaps most famous is mobile phone text language with its abbreviations, emoticons, and truncated sentences (grinter & eldridge, 2003; thurlow, 2003), but similar styles are evident in many other forms of computer-mediated communication, including chatrooms, bulletin boards, and social network sites (baron, 2003; crystal, 2006). widely recognized innovations include emoticons like :-) that are reasonably effective in conveying emotion (derks, bos, & von grumbkow, 2008; fullwood & martino, 2007) and word abbreviations like m8 (mate) and u (you) (thurlow, 2003). although sometimes seen as poor language use, these are a natural response to the technological affordances and social factors associated with a system (baron, 2003; walther & parks, 2002). these variations cause problems because typical linguistic sentiment analysis programs start with part of speech tagging (e.g., brill, 1992), which is reliant upon standard spelling and grammar, and/or apply rules that assume at least correct spelling, if not correct grammar. spelling correction can be useful in this context,",1
"emotions are perceived differently by individuals, partly because of their life experiences and partly because of personality issues (barrett, 2006) and gender (stoppard & gunn gruchy, 1993). for system development, the judgments should give a consistent perspective on sentiment in the data, rather than an estimate of the population average perception. as a result, a set of same gender (female) coders was used and initial testing conducted to identify a homogeneous subset. five coders were initially selected, but two were subsequently rejected for giving anomalous results: one gave much higher positive scores than the others, and another gave generally inconsistent results. the mean of the three coders' results was calculated for each comment and rounded. this was the gold standard for the experiments. below are some examples of texts and judgments.",1
"a complicating factor for online sentiment detection is that there are many electronic communications media in which text-based communication in english seems to frequently ignore the rules of grammar and spelling. perhaps most famous is mobile phone text language with its abbreviations, emoticons, and truncated sentences (grinter & eldridge, 2003; thurlow, 2003), but similar styles are evident in many other forms of computer-mediated communication, including chatrooms, bulletin boards, and social network sites (baron, 2003; crystal, 2006). widely recognized innovations include emoticons like :-) that are reasonably effective in conveying emotion (derks, bos, & von grumbkow, 2008; fullwood & martino, 2007) and word abbreviations like m8 (mate) and u (you) (thurlow, 2003). although sometimes seen as poor language use, these are a natural response to the technological affordances and social factors associated with a system (baron, 2003; walther & parks, 2002). these variations cause problems because typical linguistic sentiment analysis programs start with part of speech tagging (e.g., brill, 1992), which is reliant upon standard spelling and grammar, and/or apply rules that assume at least correct spelling, if not correct grammar. spelling correction can be useful in this context,",1
"opinion mining typically occurs in two or three stages, although more may be needed for some tasks (e.g., balahur et al., 2010). first, the input text is split into sections, such as sentences, and each section tested to see if it contains any sentiment: if it is subjective or objective (pang & lee, 2004). second, the subjective sentences are analyzed to detect their sentiment polarity. finally, the object about which the opinion is expressed may be extracted (e.g., gamon, aue, corstonoliver, & ringger, 2005). opinion mining normally deals with only positive and negative sentiment rather than discrete emotions (e.g., happiness, surprise), does not detect sentiment strength (but sometimes uses the strength of association of words with positive or negative sentiment, e.g., kaji & kitsuregawa, 2007), and does not simultaneously identify both positive and negative emotions. nevertheless, such opinion mining research can aid the simultaneous assessment of positive and negative sentiment strength both because of its general insights into sentiment analysis and also because most techniques could, in theory, be repurposed for this new task. for example, phrase analysis techniques could be applied to identify both positive and negative sentiment even within individual sentences (choi & cardie, 2008; wilson, 2008; wilson, wiebe, & hoffman, 2009).",1
"in psychology, the term mood refers to mediumand longterm affective states. some blogs and social network sites allow members to describe their mood at the time of editing their status or writing a post, typically by selecting from a range of icons. the results can be used as annotated mood corpora. in theory, such corpora ought to be usable to train classifiers to identify mood from the text associated with the mood icon and one system has been designed to do this, but with limited success. this is probably because the texts analyzed are typically short (average 200 words) and there are many moods, some of which are very similar to each other, although even a binary categorization task also had limited success (mishne, 2005). a follow up project attempted to derive the proportion of posts with a given mood within a specific time period using 199 words (1-grams) and word pairs (2-grams) derived from the aggregate of all texts, rather than by classifying individual texts (mishne & de rijke, 2006). the results showed a high correlation with aggregate selfreported mood. a similar aggregation approach has been applied subsequently in a range of social science contexts (hopkins & king, 2010).",1
"psychology of emotion research argues that though positive and negative sentiment are important dimensions, there are many different widely socially recognized types of emotion and the strength of emotions (arousal level) can vary (e.g., cornelius, 1996; fox, 2008). in the dimensional model of emotion from psychology (russell, 1979), sentiment can always be fundamentally split into two axes: arousal (low to high) and valence (positive to negative). although this model is useful, other research has shown that positive and negative sentiment can coexist (e.g., fox, 2008, p. 127) and are relatively independent in many contexts--particularly when sentiment levels are not extreme and over longer time periods (diener & emmons, 1984; huppert & whittington, 2003; watson, 1988; watson, clark, & tellegen, 1988) and so it also seems reasonable to conceive sentiment as separately measurable positive and negative components, as encoded in a popular psychology research instrument (watson et al., 1988).",2
"in terms of classification algorithms, support vector machines (svms) are widely used (abbasi et al., 2008; abbasi et al., 2008; argamon et al., 2007; gamon, 2004; mishne, 2005; wilson, wiebe, & hwa, 2006) because they seem to perform as well or better than other methods in most machine learning contexts. nevertheless, with a few exceptions (read, 2005; wilson et al., 2006), explicit comparisons with other methods have not been included in opinion mining publications.",1
"myspace was chosen as a source of test data for this study because it is a public environment containing a large quantity of informal text language and is important in its own right as one of the most visited web sites in the world in 2009. a random sample of myspace comments was taken by examining the profiles of every 15th member that joined on june 18, 2007, up to 40,000 and selecting those with a declared u.s. nationality and a public profile not of a musician, comedian, or film-maker. of these, those with less than two friends or no comments were rejected as inactive and those with over 1,000 friends or 4,000 comments were rejected as abnormal. a commenting friend was then identified for each remaining member, satisfying the same criteria above, and a random comment selected from each direction of communication between the two. the comments were extracted in december 2008. this produced a large essentially random sample of u.s. commenter-commentee messages. spam comments and chain messages were subsequently eliminated, as were comments containing images.",1
"quite similar to the current study is one that measured multiple emotions and their strengths in informal text associated with a dialog system using a combination of methods, including seeking symbolic cues via repeated punctuation (e.g., !!), emoticons, and capital letters as well as translating abbreviations (neviarouskaya, prendinger, & ishizuka, 2007). the system also measured emotion intensity on a scale of 0-1 and used a dictionary of terms and intensity ratings assigned by three human judges (with moderate agreement rates: fleiss kappa 0.58). the reported evaluation on 160 human-coded sentences showed that in 68% of sentences the system agreed with the coder average to within 20%.",2
"liu, h., lieberman, h., & selker, t. (2003). a model of textual affect sensing using real-world knowledge. in proceedings of the 2003 international conference on intelligent user interfaces, iui 2003 (pp. 125-132). new york: acm press.",1
"opinion mining typically occurs in two or three stages, although more may be needed for some tasks (e.g., balahur et al., 2010). first, the input text is split into sections, such as sentences, and each section tested to see if it contains any sentiment: if it is subjective or objective (pang & lee, 2004). second, the subjective sentences are analyzed to detect their sentiment polarity. finally, the object about which the opinion is expressed may be extracted (e.g., gamon, aue, corstonoliver, & ringger, 2005). opinion mining normally deals with only positive and negative sentiment rather than discrete emotions (e.g., happiness, surprise), does not detect sentiment strength (but sometimes uses the strength of association of words with positive or negative sentiment, e.g., kaji & kitsuregawa, 2007), and does not simultaneously identify both positive and negative emotions. nevertheless, such opinion mining research can aid the simultaneous assessment of positive and negative sentiment strength both because of its general insights into sentiment analysis and also because most techniques could, in theory, be repurposed for this new task. for example, phrase analysis techniques could be applied to identify both positive and negative sentiment even within individual sentences (choi & cardie, 2008; wilson, 2008; wilson, wiebe, & hoffman, 2009).",1
"table 1 reports the degree of intercoder agreement. basic agreement rates are reported here for comparability with sentistrength. previous emotion-judgment/annotation tasks have obtained higher intercoder scores, but without strength measures and therefore having fewer categories (e.g., wiebe et al., 2005). moreover, one previous article noted that intercoder agreement was higher on longer (blog) texts (gill, gergle, french, & oberlander, 2008), suggesting that obtaining agreement on the short texts here would be difficult. the appropriate type of intercoder reliability statistic for this kind of data with multiple coders and varying differences between categories is krippendorff's a (artstein & poesio, 2008; krippendorff, 2004). using the numerical difference in emotion score as weights, the three coder a values were 0.5743 for positive and 0.5634 for negative sentiment. these values the main reason for sentistrength's relative success seems to be procedures for decoding nonstandard spellings and methods for boosting the strength of words, which accounted for much of its performance. without these factors, the sentistrength variant based solely upon a dictionary of emotion-associated words and their estimated strengths with 57.5% was only 1.3% better than the most successful machine-learning approach on an extended set of 1-3 grams. in contrast, sentistrength was able to identify negative sentiment little better (1.8%) than the baseline, probably due to creativity in expressing negative comments or due to the difficulty in getting significantly above the baseline when one category dominates (artstein & poesio, 2008; krippendorff, 2004). it seems that both positive and negative sentiment detection in informal text language like myspace comments is challenging because of several factors: language creativity, expressions of sentiment without emotion-bearing words, and differences between human",1
"but this is based upon the assumption that spelling deviations are likely to be accidental mistakes (kukich, 1992; pollock & zamora, 1984) and so current algorithms are unlikely to work well with deliberately nonstandard spellings. nevertheless, there is a range of common abbreviations and new words that a linguistic algorithm could, in principle, detect. nonlinguistic machine learning algorithms typically predict sentiment based upon occurrences of individual words, word pairs, and word triples in documents. these may also perform poorly on informal text because of spelling problems and creativity in sentiment expression, even if a large training corpus is available (see below).",1
"online communication, perhaps designed to identify and intervene when inappropriate emotions are used or to identify at risk users (e.g., huang, goh, & liew, 2007), would need to be sensitive to the strength of sentiment expressed and whether participants were appropriately balancing positive and negative sentiment. in addition, basic research to understand the role of emotion in online communication (e.g., derks, fischer, & bos, 2008; e.g., hancock, gee, ciaccio, & lin, 2008; nardi, 2005) would also benefit from fine-grained sentiment detection, as would the growing body of psychology and other social science research into the role of sentiment in various types of discussion or general discourse (balahur, kozareva, & montoyo, 2009; pennebaker, mehl, & niederhoffer, 2003; short & palmer, 2008).",1
"opinion mining algorithms often use machine learning to identify general features associated with positive and negative sentiment, where these features could be a subset of the words in the document, parts of speech or n-grams (i.e., the frequency of occurrence of all n consecutive words, where n is typically 1, 2, or 3; abbasi, chen, thoms, & fu, 2008; ng, dasgupta, & arifin, 2006; tang, tan, & cheng, 2009). other features used with some success include emoticons in online movie reviews (read, 2005), which seem so be more domain-independent than words; lexico-syntactic patterns (e.g., riloff & wiebe, 2003); and artificial features derived from adjective polarity lists (ng et al., 2006). the additional features typically provide small, but significant increases in performance. rules-based methods have also been used to identify structures in sentences associated with sentiment (prabowo & thelwall, 2009; wu, chuang, & lin, 2006). two recurring machine learning issues are feature selection and classification algorithm choice.",1
"online communication, perhaps designed to identify and intervene when inappropriate emotions are used or to identify at risk users (e.g., huang, goh, & liew, 2007), would need to be sensitive to the strength of sentiment expressed and whether participants were appropriately balancing positive and negative sentiment. in addition, basic research to understand the role of emotion in online communication (e.g., derks, fischer, & bos, 2008; e.g., hancock, gee, ciaccio, & lin, 2008; nardi, 2005) would also benefit from fine-grained sentiment detection, as would the growing body of psychology and other social science research into the role of sentiment in various types of discussion or general discourse (balahur, kozareva, & montoyo, 2009; pennebaker, mehl, & niederhoffer, 2003; short & palmer, 2008).",1
"esuli, a., & sebastiani, f. (2006). sentiwordnet: a publicly available lexical resource for opinion mining. in proceedings of language resources and evaluation (lrec) 2006. paris: european language resources association. retrieved july 28, 2009, from http://tcc.fbk.eu/ projects/ontotext/publications/lrec2006-esuli-sebastiani.pdf.",1
"agerri, r., & garcia-serrano, a. (2010, may). q-wordnet: extracting polarity from wordnet senses. paper presented at the seventh conference on international language resources and evaluation, malta. retrieved may 25, 2010, from http://www.lrec-conf.org/proceedings/lrec2010/pdf/ 2695_paper.pdf baccianella, s., esuli, a., & sebastiani, f. (2010, may). sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining. paper presented at the seventh conference on international language resources and evaluation, malta. retrieved may 25, 2010, from: http://www.lrec-conf.org/proceedings/lrec2010/pdf/2769_paper.pdf. balahur, a., kozareva, z., & montoyo, a. (2009). determining the polarity and source of opinions expressed in political debates. lecture notes in computer science, 5449, 468-480. balahur, a., steinberger, r., kabadjov, m., zavarella, v., goot, e.v.d., & halkia, m., et al. (2010, may). sentiment analysis in the news. paper presented at the seventh conference on international language resources and evaluation. retrieved may 25, 2010, from http://www.lrecconf.org/proceedings/lrec2010/pdf/2909_paper.pdf",2
"many other approaches have also been used to detect sentiment in text. one is to have a dictionary of positive and negative words (e.g., love, hate), such as that found in general inquirer (stone, dunphy, smith, & ogilvie, 1966), wordnet affect (strapparava & valitutti, 2004), sentiwordnet (baccianella, esuli, & sebastiani, 2010; esuli & sebastiani, 2006) or q-wordnet (agerri & garcia-serrano, 2010), and to count how often they occur. modifications of this approach include the identification of negating terms (das & chen, 2001), words that enhance sentiment in other words (e.g., really love, absolutely hate) and overall sentence structures (turney, 2002). a more sophisticated approach is to identify text features that could potentially be subjective in some contexts and then use contextual information to decide whether they are subjective in each new context (wiebe, wilson, bruce, bell, & martin, 2004).",1
"to obtain reliable human judgments of a random sample of the myspace comments, two pilot exercises were undertaken with separate samples of the data (a total of 2,600 comments). these were used to identify key judgment issues and an appropriate scale. although there are many ways to measure emotion (mauss & robinson, 2009; wiebe, wilson, & cardie, 2005), human coder subjective judgments were used as an appropriate way to gather sufficient results. a set of coder instructions was drafted and refined and an online system constructed to randomly select comments and present them to the coders. one of the key outcomes from the pilot exercise was that the coders treated expressions of energy as expressions of positive sentiment unless in an explicitly negative context. for example, ""hey!!!would be interpreted as positive because it expresses energy in a context that gives no clue as to the polarity of the emotion, so it would be accepted by most coders as positive by default. in contrast, ""loser!!!would be interpreted as more negative than ""loseras the exclamation marks are associated with a negative word. consequently, the instructions were revised to explicitly state that this conflation of ostensibly neutral energy and positive sentiment was permissible.",2
"in terms of classification algorithms, support vector machines (svms) are widely used (abbasi et al., 2008; abbasi et al., 2008; argamon et al., 2007; gamon, 2004; mishne, 2005; wilson, wiebe, & hwa, 2006) because they seem to perform as well or better than other methods in most machine learning contexts. nevertheless, with a few exceptions (read, 2005; wilson et al., 2006), explicit comparisons with other methods have not been included in opinion mining publications. an alternative opinion mining technique has used a primarily linguistic approach: simple rules based upon compositional semantics (information about likely meanings of a word based upon the surrounding text) to detect the polarity of an expression (choi & cardie, 2008). this gives good results on phrases in newswire documents that are manually coded as having at least medium level positive or negative sentiment. this approach seems particularly suited to cases where there is a large volume of grammatically correct text from which rules can be learned. nevertheless, a study of poor grammatical quality texts in online customer feedback showed that linguistic approaches could improve classification slightly when added to bag of words (1-grams) approaches, although aggressive feature reduction had a similar impact to adding linguistic features (gamon, 2004). the improvement was probably due to the large data set available (40,884 documents with an average of 2.26 sentences each), as has been previously claimed for an analysis of informal text (mishne, 2005). another approach used a lexicon of appraisal adjectives (e.g., ""sort of,""very"") together with an orientation lexicon to detect movie review polarity. this did not perform in psychology, the term mood refers to mediumand longterm affective states. some blogs and social network sites allow members to describe their mood at the time of editing their status or writing a post, typically by selecting from a range of icons. the results can be used as annotated mood corpora. in theory, such corpora ought to be usable to train classifiers to identify mood from the text associated with the mood icon and one system has been designed to do this, but with limited success. this is probably because the texts analyzed are typically short (average 200 words) and there are many moods, some of which are very similar to each other, although even a binary categorization task also had limited success (mishne, 2005). a follow up project attempted to derive the proportion of posts with a given mood within a specific time period using 199 words (1-grams) and word pairs (2-grams) derived from the aggregate of all texts, rather than by classifying individual texts (mishne & de rijke, 2006). the results showed a high correlation with aggregate selfreported mood. a similar aggregation approach has been applied subsequently in a range of social science contexts (hopkins & king, 2010).",1
"online communication, perhaps designed to identify and intervene when inappropriate emotions are used or to identify at risk users (e.g., huang, goh, & liew, 2007), would need to be sensitive to the strength of sentiment expressed and whether participants were appropriately balancing positive and negative sentiment. in addition, basic research to understand the role of emotion in online communication (e.g., derks, fischer, & bos, 2008; e.g., hancock, gee, ciaccio, & lin, 2008; nardi, 2005) would also benefit from fine-grained sentiment detection, as would the growing body of psychology and other social science research into the role of sentiment in various types of discussion or general discourse (balahur, kozareva, & montoyo, 2009; pennebaker, mehl, & niederhoffer, 2003; short & palmer, 2008).",1
"online communication, perhaps designed to identify and intervene when inappropriate emotions are used or to identify at risk users (e.g., huang, goh, & liew, 2007), would need to be sensitive to the strength of sentiment expressed and whether participants were appropriately balancing positive and negative sentiment. in addition, basic research to understand the role of emotion in online communication (e.g., derks, fischer, & bos, 2008; e.g., hancock, gee, ciaccio, & lin, 2008; nardi, 2005) would also benefit from fine-grained sentiment detection, as would the growing body of psychology and other social science research into the role of sentiment in various types of discussion or general discourse (balahur, kozareva, & montoyo, 2009; pennebaker, mehl, & niederhoffer, 2003; short & palmer, 2008).",1
"opinion mining algorithms often use machine learning to identify general features associated with positive and negative sentiment, where these features could be a subset of the words in the document, parts of speech or n-grams (i.e., the frequency of occurrence of all n consecutive words, where n is typically 1, 2, or 3; abbasi, chen, thoms, & fu, 2008; ng, dasgupta, & arifin, 2006; tang, tan, & cheng, 2009). other features used with some success include emoticons in online movie reviews (read, 2005), which seem so be more domain-independent than words; lexico-syntactic patterns (e.g., riloff & wiebe, 2003); and artificial features derived from adjective polarity lists (ng et al., 2006). the additional features typically provide small, but significant increases in performance. rules-based methods have also been used to identify structures in sentences associated with sentiment (prabowo & thelwall, 2009; wu, chuang, & lin, 2006). two recurring machine learning issues are feature selection and classification algorithm choice.",1
"online communication, perhaps designed to identify and intervene when inappropriate emotions are used or to identify at risk users (e.g., huang, goh, & liew, 2007), would need to be sensitive to the strength of sentiment expressed and whether participants were appropriately balancing positive and negative sentiment. in addition, basic research to understand the role of emotion in online communication (e.g., derks, fischer, & bos, 2008; e.g., hancock, gee, ciaccio, & lin, 2008; nardi, 2005) would also benefit from fine-grained sentiment detection, as would the growing body of psychology and other social science research into the role of sentiment in various types of discussion or general discourse (balahur, kozareva, & montoyo, 2009; pennebaker, mehl, & niederhoffer, 2003; short & palmer, 2008).",1
"table 1 reports the degree of intercoder agreement. basic agreement rates are reported here for comparability with sentistrength. previous emotion-judgment/annotation tasks have obtained higher intercoder scores, but without strength measures and therefore having fewer categories (e.g., wiebe et al., 2005). moreover, one previous article noted that intercoder agreement was higher on longer (blog) texts (gill, gergle, french, & oberlander, 2008), suggesting that obtaining agreement on the short texts here would be difficult. the appropriate type of intercoder reliability statistic for this kind of data with multiple coders and varying differences between categories is krippendorff's a (artstein & poesio, 2008; krippendorff, 2004). using the numerical difference in emotion score as weights, the three coder a values were 0.5743 for positive and 0.5634 for negative sentiment. these values",1
"many other approaches have also been used to detect sentiment in text. one is to have a dictionary of positive and negative words (e.g., love, hate), such as that found in general inquirer (stone, dunphy, smith, & ogilvie, 1966), wordnet affect (strapparava & valitutti, 2004), sentiwordnet (baccianella, esuli, & sebastiani, 2010; esuli & sebastiani, 2006) or q-wordnet (agerri & garcia-serrano, 2010), and to count how often they occur. modifications of this approach include the identification of negating terms (das & chen, 2001), words that enhance sentiment in other words (e.g., really love, absolutely hate) and overall sentence structures (turney, 2002). a more sophisticated approach is to identify text features that could potentially be subjective in some contexts and then use contextual information to decide whether they are subjective in each new context (wiebe, wilson, bruce, bell, & martin, 2004).",1
"opinion mining typically occurs in two or three stages, although more may be needed for some tasks (e.g., balahur et al., 2010). first, the input text is split into sections, such as sentences, and each section tested to see if it contains any sentiment: if it is subjective or objective (pang & lee, 2004). second, the subjective sentences are analyzed to detect their sentiment polarity. finally, the object about which the opinion is expressed may be extracted (e.g., gamon, aue, corstonoliver, & ringger, 2005). opinion mining normally deals with only positive and negative sentiment rather than discrete emotions (e.g., happiness, surprise), does not detect sentiment strength (but sometimes uses the strength of association of words with positive or negative sentiment, e.g., kaji & kitsuregawa, 2007), and does not simultaneously identify both positive and negative emotions. nevertheless, such opinion mining research can aid the simultaneous assessment of positive and negative sentiment strength both because of its general insights into sentiment analysis and also because most techniques could, in theory, be repurposed for this new task. for example, phrase analysis techniques could be applied to identify both positive and negative sentiment even within individual sentences (choi & cardie, 2008; wilson, 2008; wilson, wiebe, & hoffman, 2009).",1
"in addition to the research discussed above concerning strength detection for multiple emotions (strapparava & mihalcea, 2008), there is some work on positive-negative sentiment strength detection. one previous study used modified sentiment analysis techniques to predict the strength of human ratings on a scale of 1 to 5 for movie reviews (pang & lee, 2005). this is a kind of sentiment strength evaluation with a combined scale for positive and negative sentiment. experiments with human judgments led the authors to merge two of the categories and so the final task was a four-category classification, with a three-category version also constructed for testing purposes. a comparison of multiclass svm classification with svm regression suggested that svm regression worked slightly better than multiclass svm classification when all four categories were used, but not when only three categories were used. it seems likely that the relative performance of svm regression would increase further as the number of categories increases because the ordering of the classes is implicit information that the multiclass svm does not use, but that svm regression does. slight improvements were also gained when information about the percentage of positive sentences in each review was added. this may not be relevant to corpora of very short texts, however.",2
"opinion mining, also known as sentiment analysis, is the extraction of positive or negative opinions from (unstructured) text (pang & lee, 2008). the many applications of opinion mining include detecting movie popularity from multiple online reviews and diagnosing which parts of a vehicle are liked or disliked by owners through their comments in a dedicated site or forum. there are also applications unrelated to marketing, such as differentiating between emotional and informative social media content (denecke & nejdl, 2009). although sentiment analysis is normally concerned with opinions (pang & lee, 2008), wilson (2008) has generalized this to the psychological task of identifying the author's hidden internal state from their text. for the myspace data, the objective was not to determine opinions or the author's internal state, however, but to identify the role of expressed sentiment for online communication. hence, the focus of the task was to identify the sentiment expressed in each message, whether reflecting the author's hidden internal state, the intended message interpretation, or the reader's hidden internal state.",1
"researchers to investigate the connection between language and psychology (pennebaker et al., 2003) and also as a practical tool, for example, to detect how well people are likely to cope with bereavement based upon their language use (pennebaker, mayne, & francis, 1997). a related emotion detection approach differentiates between happy, unhappy. and neutral states based upon words used by students describing their daily lives (wu et al., 2006). this is similar to the typical positive/negative/neutral objective for opinion mining, however.",1
"but this is based upon the assumption that spelling deviations are likely to be accidental mistakes (kukich, 1992; pollock & zamora, 1984) and so current algorithms are unlikely to work well with deliberately nonstandard spellings. nevertheless, there is a range of common abbreviations and new words that a linguistic algorithm could, in principle, detect. nonlinguistic machine learning algorithms typically predict sentiment based upon occurrences of individual words, word pairs, and word triples in documents. these may also perform poorly on informal text because of spelling problems and creativity in sentiment expression, even if a large training corpus is available (see below). * a spelling correction algorithm identifies the standard spellings of words that have been misspelled by the inclusion of repeated letters. for example hellllloooo would be identified as ""helloby this algorithm. the algorithm (a) automatically deletes repeated letters above twice (e.g., helllo hello); (b) deletes repeated letters occurring twice for letters rarely occurring twice in english (e.g., niice nice), and (c) deletes letters occurring twice if not a standard word but would form a standard word if deleted (e.g., nnice nice but not hoop hop nor baaz baz). formal spelling correction algorithms (see pollock & zamora, 1984) were tried but not used as they made very few corrections and had problems with names and slang.",1
"opinion mining algorithms often use machine learning to identify general features associated with positive and negative sentiment, where these features could be a subset of the words in the document, parts of speech or n-grams (i.e., the frequency of occurrence of all n consecutive words, where n is typically 1, 2, or 3; abbasi, chen, thoms, & fu, 2008; ng, dasgupta, & arifin, 2006; tang, tan, & cheng, 2009). other features used with some success include emoticons in online movie reviews (read, 2005), which seem so be more domain-independent than words; lexico-syntactic patterns (e.g., riloff & wiebe, 2003); and artificial features derived from adjective polarity lists (ng et al., 2006). the additional features typically provide small, but significant increases in performance. rules-based methods have also been used to identify structures in sentences associated with sentiment (prabowo & thelwall, 2009; wu, chuang, & lin, 2006). two recurring machine learning issues are feature selection and classification algorithm choice.",1
"opinion mining algorithms often use machine learning to identify general features associated with positive and negative sentiment, where these features could be a subset of the words in the document, parts of speech or n-grams (i.e., the frequency of occurrence of all n consecutive words, where n is typically 1, 2, or 3; abbasi, chen, thoms, & fu, 2008; ng, dasgupta, & arifin, 2006; tang, tan, & cheng, 2009). other features used with some success include emoticons in online movie reviews (read, 2005), which seem so be more domain-independent than words; lexico-syntactic patterns (e.g., riloff & wiebe, 2003); and artificial features derived from adjective polarity lists (ng et al., 2006). the additional features typically provide small, but significant increases in performance. rules-based methods have also been used to identify structures in sentences associated with sentiment (prabowo & thelwall, 2009; wu, chuang, & lin, 2006). two recurring machine learning issues are feature selection and classification algorithm choice. in terms of classification algorithms, support vector machines (svms) are widely used (abbasi et al., 2008; abbasi et al., 2008; argamon et al., 2007; gamon, 2004; mishne, 2005; wilson, wiebe, & hwa, 2006) because they seem to perform as well or better than other methods in most machine learning contexts. nevertheless, with a few exceptions (read, 2005; wilson et al., 2006), explicit comparisons with other methods have not been included in opinion mining publications.",1
"in psychology, the term mood refers to mediumand longterm affective states. some blogs and social network sites allow members to describe their mood at the time of editing their status or writing a post, typically by selecting from a range of icons. the results can be used as annotated mood corpora. in theory, such corpora ought to be usable to train classifiers to identify mood from the text associated with the mood icon and one system has been designed to do this, but with limited success. this is probably because the texts analyzed are typically short (average 200 words) and there are many moods, some of which are very similar to each other, although even a binary categorization task also had limited success (mishne, 2005). a follow up project attempted to derive the proportion of posts with a given mood within a specific time period using 199 words (1-grams) and word pairs (2-grams) derived from the aggregate of all texts, rather than by classifying individual texts (mishne & de rijke, 2006). the results showed a high correlation with aggregate selfreported mood. a similar aggregation approach has been applied subsequently in a range of social science contexts (hopkins & king, 2010).",1
"opinion mining algorithms often use machine learning to identify general features associated with positive and negative sentiment, where these features could be a subset of the words in the document, parts of speech or n-grams (i.e., the frequency of occurrence of all n consecutive words, where n is typically 1, 2, or 3; abbasi, chen, thoms, & fu, 2008; ng, dasgupta, & arifin, 2006; tang, tan, & cheng, 2009). other features used with some success include emoticons in online movie reviews (read, 2005), which seem so be more domain-independent than words; lexico-syntactic patterns (e.g., riloff & wiebe, 2003); and artificial features derived from adjective polarity lists (ng et al., 2006). the additional features typically provide small, but significant increases in performance. rules-based methods have also been used to identify structures in sentences associated with sentiment (prabowo & thelwall, 2009; wu, chuang, & lin, 2006). two recurring machine learning issues are feature selection and classification algorithm choice.",1
"information gain (riloff, patwardhan, & wiebe, 2006), or log likelihood (gamon, 2004). when using n-grams (and lexico-syntactic patterns) small improvements can also be made by pruning the feature set of features that are subsumed by simpler features that have stronger information gain values (riloff et al., 2006). for example, if ""lovehas a much higher information gain value than ""i lovethen the bigram can be eliminated without much risk of loss of power for the subsequent classification. an entropy-weighted genetic algorithm can also perform better than standard feature reduction approaches (abbasi, chen, & salem, 2008).",1
"opinion mining typically occurs in two or three stages, although more may be needed for some tasks (e.g., balahur et al., 2010). first, the input text is split into sections, such as sentences, and each section tested to see if it contains any sentiment: if it is subjective or objective (pang & lee, 2004). second, the subjective sentences are analyzed to detect their sentiment polarity. finally, the object about which the opinion is expressed may be extracted (e.g., gamon, aue, corstonoliver, & ringger, 2005). opinion mining normally deals with only positive and negative sentiment rather than discrete emotions (e.g., happiness, surprise), does not detect sentiment strength (but sometimes uses the strength of association of words with positive or negative sentiment, e.g., kaji & kitsuregawa, 2007), and does not simultaneously identify both positive and negative emotions. nevertheless, such opinion mining research can aid the simultaneous assessment of positive and negative sentiment strength both because of its general insights into sentiment analysis and also because most techniques could, in theory, be repurposed for this new task. for example, phrase analysis techniques could be applied to identify both positive and negative sentiment even within individual sentences (choi & cardie, 2008; wilson, 2008; wilson, wiebe, & hoffman, 2009).",1
"psychology of emotion research argues that though positive and negative sentiment are important dimensions, there are many different widely socially recognized types of emotion and the strength of emotions (arousal level) can vary (e.g., cornelius, 1996; fox, 2008). in the dimensional model of emotion from psychology (russell, 1979), sentiment can always be fundamentally split into two axes: arousal (low to high) and valence (positive to negative). although this model is useful, other research has shown that positive and negative sentiment can coexist (e.g., fox, 2008, p. 127) and are relatively independent in many contexts--particularly when sentiment levels are not extreme and over longer time periods (diener & emmons, 1984; huppert & whittington, 2003; watson, 1988; watson, clark, & tellegen, 1988) and so it also seems reasonable to conceive sentiment as separately measurable positive and negative components, as encoded in a popular psychology research instrument (watson et al., 1988).",2
"information gain (riloff, patwardhan, & wiebe, 2006), or log likelihood (gamon, 2004). when using n-grams (and lexico-syntactic patterns) small improvements can also be made by pruning the feature set of features that are subsumed by simpler features that have stronger information gain values (riloff et al., 2006). for example, if ""lovehas a much higher information gain value than ""i lovethen the bigram can be eliminated without much risk of loss of power for the subsequent classification. an entropy-weighted genetic algorithm can also perform better than standard feature reduction approaches (abbasi, chen, & salem, 2008).",1
"sentiment strength classification has also been developed for a three-level scheme (low, medium, and high or extreme) for subjective sentences or clauses in newswire texts using a linguistic analysis converting sentences into dependency trees reflecting their structure (wilson et al., 2006). adding dependency trees to unigrams substantially improved the performance of various classifiers compared to unigrams alone, perhaps helped by the fairly large training set (9,313 sentences), the (presumably) good quality grammar of the texts, and the fairly low initial performance on this task (34.5% to 50.9% for unigrams, rising to 48.3% to 55.0% for the three types of classifier applied to level 1 clauses). here, svm regression was outperformed by both the rule-based learning ripper (cohen, 1995) and boostexter, a boosting algorithm combining multiple weak classifiers (schapire & singer, 2000).",1
"many other approaches have also been used to detect sentiment in text. one is to have a dictionary of positive and negative words (e.g., love, hate), such as that found in general inquirer (stone, dunphy, smith, & ogilvie, 1966), wordnet affect (strapparava & valitutti, 2004), sentiwordnet (baccianella, esuli, & sebastiani, 2010; esuli & sebastiani, 2006) or q-wordnet (agerri & garcia-serrano, 2010), and to count how often they occur. modifications of this approach include the identification of negating terms (das & chen, 2001), words that enhance sentiment in other words (e.g., really love, absolutely hate) and overall sentence structures (turney, 2002). a more sophisticated approach is to identify text features that could potentially be subjective in some contexts and then use contextual information to decide whether they are subjective in each new context (wiebe, wilson, bruce, bell, & martin, 2004).",1
"linguistic processing has also been combined with a preexisting large collection of subjective common sense statement patterns and applied to relatively informal and domain-independent in e-mail messages to detect multiple emotions (liu, lieberman, & selker, 2003). this was part of an e-mail support system, however, and",1
"chaumartin, f.-r. (2007). upar7: a knowledge-based system for headline sentiment tagging. in e. aqirre, l. marquez, & r. wicentowski (eds.), proceedings of the fourth international workshop on semantic evaluations (semeval-2007) (pp. 422-425). college park, md: association for computational linguistics.",2
"online communication, perhaps designed to identify and intervene when inappropriate emotions are used or to identify at risk users (e.g., huang, goh, & liew, 2007), would need to be sensitive to the strength of sentiment expressed and whether participants were appropriately balancing positive and negative sentiment. in addition, basic research to understand the role of emotion in online communication (e.g., derks, fischer, & bos, 2008; e.g., hancock, gee, ciaccio, & lin, 2008; nardi, 2005) would also benefit from fine-grained sentiment detection, as would the growing body of psychology and other social science research into the role of sentiment in various types of discussion or general discourse (balahur, kozareva, & montoyo, 2009; pennebaker, mehl, & niederhoffer, 2003; short & palmer, 2008).",1
"mishne, g. (2005, august). experiments with mood classification in blog posts. paper presented at the first workshop for stylistic analysis of text for information access (style 2005) at sigir 2005, salvador, brazil. retrieved august 3, 2010, from http://staff.science.uva.nl/ gilad/pubs/style2005-blogmoods.pdf",1
"as well as unigrams, but the combined performance was better than that of unigrams alone (argamon et al., 2007). linguistic features have also been successfully used to extend opinion mining to a multiaspect variant that is able to detect opinions about different aspects of a topic (snyder & barzilay, 2007). a promising future approach is the incorporation of context about the reasons why sentiment is used, such as differentiating between intention, arguments, and speculation (wilson, 2008).",1
"one computer science initiative has attempted to identify various emotions in text, focusing on the six so-called basic emotions (ekman, 1992; fox, 2008) of anger, disgust, fear, joy, sadness, and surprise (strapparava & mihalcea, 2008). this initiative also measured emotion strength. a human-annotated corpus was used with the coders allocating a strength from 0 to 100 for each emotion to each text (a news headline), although inter-annotator agreement was low (pearson correlations of 0.36 to 0.68, depending on the emotion). a variety of algorithms were subsequently trained on this data set. for example, one used wordnet affect lists to generate appropriate dictionaries for the six emotions. a second approach used a naive bayes classifier trained on sets of livejournal blogs annotated by their owners with one of the six emotions. the best system (for fine-grained evaluation) was one previously designed for newspaper headlines, upar7 (chaumartin, 2007), which used linguistic parsing and tagging as well as wordnet, sentiwordnet, and wordnet affect, hence relying upon reasonably correct standard grammar and spelling. in addition to the research discussed above concerning strength detection for multiple emotions (strapparava & mihalcea, 2008), there is some work on positive-negative sentiment strength detection. one previous study used modified sentiment analysis techniques to predict the strength of human ratings on a scale of 1 to 5 for movie reviews (pang & lee, 2005). this is a kind of sentiment strength evaluation with a combined scale for positive and negative sentiment. experiments with human judgments led the authors to merge two of the categories and so the final task was a four-category classification, with a three-category version also constructed for testing purposes. a comparison of multiclass svm classification with svm regression suggested that svm regression worked slightly better than multiclass svm classification when all four categories were used, but not when only three categories were used. it seems likely that the relative performance of svm regression would increase further as the number of categories increases because the ordering of the classes is implicit information that the multiclass svm does not use, but that svm regression does. slight improvements were also gained when information about the percentage of positive sentences in each review was added. this may not be relevant to corpora of very short texts, however.",2
"many other approaches have also been used to detect sentiment in text. one is to have a dictionary of positive and negative words (e.g., love, hate), such as that found in general inquirer (stone, dunphy, smith, & ogilvie, 1966), wordnet affect (strapparava & valitutti, 2004), sentiwordnet (baccianella, esuli, & sebastiani, 2010; esuli & sebastiani, 2006) or q-wordnet (agerri & garcia-serrano, 2010), and to count how often they occur. modifications of this approach include the identification of negating terms (das & chen, 2001), words that enhance sentiment in other words (e.g., really love, absolutely hate) and overall sentence structures (turney, 2002). a more sophisticated approach is to identify text features that could potentially be subjective in some contexts and then use contextual information to decide whether they are subjective in each new context (wiebe, wilson, bruce, bell, & martin, 2004).",1
"mishne, g. (2005, august). experiments with mood classification in blog posts. paper presented at the first workshop for stylistic analysis of text for information access (style 2005) at sigir 2005, salvador, brazil. retrieved august 3, 2010, from http://staff.science.uva.nl/ gilad/pubs/style2005-blogmoods.pdf",1
"mishne, g. (2005, august). experiments with mood classification in blog posts. paper presented at the first workshop for stylistic analysis of text for information access (style 2005) at sigir 2005, salvador, brazil. retrieved august 3, 2010, from http://staff.science.uva.nl/ gilad/pubs/style2005-blogmoods.pdf",1
journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi journal of the american society for information science and technology--december 2010 doi: 10.1002/asi,1
"psychology of emotion research argues that though positive and negative sentiment are important dimensions, there are many different widely socially recognized types of emotion and the strength of emotions (arousal level) can vary (e.g., cornelius, 1996; fox, 2008). in the dimensional model of emotion from psychology (russell, 1979), sentiment can always be fundamentally split into two axes: arousal (low to high) and valence (positive to negative). although this model is useful, other research has shown that positive and negative sentiment can coexist (e.g., fox, 2008, p. 127) and are relatively independent in many contexts--particularly when sentiment levels are not extreme and over longer time periods (diener & emmons, 1984; huppert & whittington, 2003; watson, 1988; watson, clark, & tellegen, 1988) and so it also seems reasonable to conceive sentiment as separately measurable positive and negative components, as encoded in a popular psychology research instrument (watson et al., 1988).",2
"liu, h., lieberman, h., & selker, t. (2003). a model of textual affect sensing using real-world knowledge. in proceedings of the 2003 international conference on intelligent user interfaces, iui 2003 (pp. 125-132). new york: acm press. riloff, e., & wiebe, j. (2003). learning extraction patterns for subjective expressions. in proceedings of the 2003 conference on empirical methods in natural language processing (emnlp-03). college park, md: association for computational linguistics. retrieved april 11, 2010, from http://www.cs.utah.edu/~riloff/pdfs/emnlp2003.pdf",1
"kaji, n., & kitsuregawa, m. (2007). building lexicon for sentiment analysis from massive collection of html documents. in proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (pp. 1075-1083). college park, md: association for computational linguistics. retrieved july 28, 2010, from http://www.aclweb.org/anthology/d/d1007/d1007-1115.pdf krippendorff, k. (2004). content analysis: an introduction to its methodology. thousand oaks, ca: sage.",1
"strapparava, c., & mihalcea, r. (2008). learning to identify emotions in text. in proceedings of the 2008 acm symposium on applied computing (pp. 1556-1560). new york: acm press.",1
"the social network site myspace, the source of the data used in the current study, is known for its young members, its musical orientation, and its informal communication patterns (boyd, 2008a, 2008b). probably as a result of these factors 95% of english public comments exchanged between friends contain at least one abbreviation from standard english (thelwall, 2009). common features include emoticons, texting-style abbreviations, and the use of repeated letters or punctuation for emphasis (e.g., a loooong time, hi!!!). comments are typically short (mean 18.7 words, median 13 words, 68 characters; thelwall, 2009), but positive emotion is common (thelwall, wilkinson, & uppal, 2010). opinion mining algorithms often use machine learning to identify general features associated with positive and negative sentiment, where these features could be a subset of the words in the document, parts of speech or n-grams (i.e., the frequency of occurrence of all n consecutive words, where n is typically 1, 2, or 3; abbasi, chen, thoms, & fu, 2008; ng, dasgupta, & arifin, 2006; tang, tan, & cheng, 2009). other features used with some success include emoticons in online movie reviews (read, 2005), which seem so be more domain-independent than words; lexico-syntactic patterns (e.g., riloff & wiebe, 2003); and artificial features derived from adjective polarity lists (ng et al., 2006). the additional features typically provide small, but significant increases in performance. rules-based methods have also been used to identify structures in sentences associated with sentiment (prabowo & thelwall, 2009; wu, chuang, & lin, 2006). two recurring machine learning issues are feature selection and classification algorithm choice.",2
"a complicating factor for online sentiment detection is that there are many electronic communications media in which text-based communication in english seems to frequently ignore the rules of grammar and spelling. perhaps most famous is mobile phone text language with its abbreviations, emoticons, and truncated sentences (grinter & eldridge, 2003; thurlow, 2003), but similar styles are evident in many other forms of computer-mediated communication, including chatrooms, bulletin boards, and social network sites (baron, 2003; crystal, 2006). widely recognized innovations include emoticons like :-) that are reasonably effective in conveying emotion (derks, bos, & von grumbkow, 2008; fullwood & martino, 2007) and word abbreviations like m8 (mate) and u (you) (thurlow, 2003). although sometimes seen as poor language use, these are a natural response to the technological affordances and social factors associated with a system (baron, 2003; walther & parks, 2002). these variations cause problems because typical linguistic sentiment analysis programs start with part of speech tagging (e.g., brill, 1992), which is reliant upon standard spelling and grammar, and/or apply rules that assume at least correct spelling, if not correct grammar. spelling correction can be useful in this context,",1
"thurlow, c. (2003). generation txt? the sociolinguistics of young people's text-messaging. discourse analysis online, 1(1). retrieved january 3, 2008, from http://extra.shu.ac.uk/daol/articles/v2001/n2001/a2003/ thurlow2002003-paper.html",0
"many other approaches have also been used to detect sentiment in text. one is to have a dictionary of positive and negative words (e.g., love, hate), such as that found in general inquirer (stone, dunphy, smith, & ogilvie, 1966), wordnet affect (strapparava & valitutti, 2004), sentiwordnet (baccianella, esuli, & sebastiani, 2010; esuli & sebastiani, 2006) or q-wordnet (agerri & garcia-serrano, 2010), and to count how often they occur. modifications of this approach include the identification of negating terms (das & chen, 2001), words that enhance sentiment in other words (e.g., really love, absolutely hate) and overall sentence structures (turney, 2002). a more sophisticated approach is to identify text features that could potentially be subjective in some contexts and then use contextual information to decide whether they are subjective in each new context (wiebe, wilson, bruce, bell, & martin, 2004).",1
"the social network site myspace, the source of the data used in the current study, is known for its young members, its musical orientation, and its informal communication patterns (boyd, 2008a, 2008b). probably as a result of these factors 95% of english public comments exchanged between friends contain at least one abbreviation from standard english (thelwall, 2009). common features include emoticons, texting-style abbreviations, and the use of repeated letters or punctuation for emphasis (e.g., a loooong time, hi!!!). comments are typically short (mean 18.7 words, median 13 words, 68 characters; thelwall, 2009), but positive emotion is common (thelwall, wilkinson, & uppal, 2010).",2
"a complicating factor for online sentiment detection is that there are many electronic communications media in which text-based communication in english seems to frequently ignore the rules of grammar and spelling. perhaps most famous is mobile phone text language with its abbreviations, emoticons, and truncated sentences (grinter & eldridge, 2003; thurlow, 2003), but similar styles are evident in many other forms of computer-mediated communication, including chatrooms, bulletin boards, and social network sites (baron, 2003; crystal, 2006). widely recognized innovations include emoticons like :-) that are reasonably effective in conveying emotion (derks, bos, & von grumbkow, 2008; fullwood & martino, 2007) and word abbreviations like m8 (mate) and u (you) (thurlow, 2003). although sometimes seen as poor language use, these are a natural response to the technological affordances and social factors associated with a system (baron, 2003; walther & parks, 2002). these variations cause problems because typical linguistic sentiment analysis programs start with part of speech tagging (e.g., brill, 1992), which is reliant upon standard spelling and grammar, and/or apply rules that assume at least correct spelling, if not correct grammar. spelling correction can be useful in this context,",1
"psychology of emotion research argues that though positive and negative sentiment are important dimensions, there are many different widely socially recognized types of emotion and the strength of emotions (arousal level) can vary (e.g., cornelius, 1996; fox, 2008). in the dimensional model of emotion from psychology (russell, 1979), sentiment can always be fundamentally split into two axes: arousal (low to high) and valence (positive to negative). although this model is useful, other research has shown that positive and negative sentiment can coexist (e.g., fox, 2008, p. 127) and are relatively independent in many contexts--particularly when sentiment levels are not extreme and over longer time periods (diener & emmons, 1984; huppert & whittington, 2003; watson, 1988; watson, clark, & tellegen, 1988) and so it also seems reasonable to conceive sentiment as separately measurable positive and negative components, as encoded in a popular psychology research instrument (watson et al., 1988).",2
"psychology of emotion research argues that though positive and negative sentiment are important dimensions, there are many different widely socially recognized types of emotion and the strength of emotions (arousal level) can vary (e.g., cornelius, 1996; fox, 2008). in the dimensional model of emotion from psychology (russell, 1979), sentiment can always be fundamentally split into two axes: arousal (low to high) and valence (positive to negative). although this model is useful, other research has shown that positive and negative sentiment can coexist (e.g., fox, 2008, p. 127) and are relatively independent in many contexts--particularly when sentiment levels are not extreme and over longer time periods (diener & emmons, 1984; huppert & whittington, 2003; watson, 1988; watson, clark, & tellegen, 1988) and so it also seems reasonable to conceive sentiment as separately measurable positive and negative components, as encoded in a popular psychology research instrument (watson et al., 1988).",2
"the coders were given verbal instructions for coding each text as well as a booklet explaining the task (motivated by wiebe et al., 2005), with the key instructions reproduced in the appendix here. the booklet also contained a list of emoticons and acronyms with explanations and background context of the task for motivation purposes. an early version of the booklet included examples of comments with associated positive and negative sentiment judgments, but these had little impact in practice on coders during the pilot-testing phase. the set of examples was therefore not used so that intercoder reliability could be more realistically assessed without the possibility that some of the comments were too similar to the examples given. table 1 reports the degree of intercoder agreement. basic agreement rates are reported here for comparability with sentistrength. previous emotion-judgment/annotation tasks have obtained higher intercoder scores, but without strength measures and therefore having fewer categories (e.g., wiebe et al., 2005). moreover, one previous article noted that intercoder agreement was higher on longer (blog) texts (gill, gergle, french, & oberlander, 2008), suggesting that obtaining agreement on the short texts here would be difficult. the appropriate type of intercoder reliability statistic for this kind of data with multiple coders and varying differences between categories is krippendorff's a (artstein & poesio, 2008; krippendorff, 2004). using the numerical difference in emotion score as weights, the three coder a values were 0.5743 for positive and 0.5634 for negative sentiment. these values",1
"information gain (riloff, patwardhan, & wiebe, 2006), or log likelihood (gamon, 2004). when using n-grams (and lexico-syntactic patterns) small improvements can also be made by pruning the feature set of features that are subsumed by simpler features that have stronger information gain values (riloff et al., 2006). for example, if ""lovehas a much higher information gain value than ""i lovethen the bigram can be eliminated without much risk of loss of power for the subsequent classification. an entropy-weighted genetic algorithm can also perform better than standard feature reduction approaches (abbasi, chen, & salem, 2008).",1
"although sentiment analysis is normally concerned with opinions (pang & lee, 2008), wilson (2008) has generalized this to the psychological task of identifying the author's hidden internal state from their text. for the myspace data, the objective was not to determine opinions or the author's internal state, however, but to identify the role of expressed sentiment for online communication. hence, the focus of the task was to identify the sentiment expressed in each message, whether reflecting the author's hidden internal state, the intended message interpretation, or the reader's hidden internal state.",1
"in terms of classification algorithms, support vector machines (svms) are widely used (abbasi et al., 2008; abbasi et al., 2008; argamon et al., 2007; gamon, 2004; mishne, 2005; wilson, wiebe, & hwa, 2006) because they seem to perform as well or better than other methods in most machine learning contexts. nevertheless, with a few exceptions (read, 2005; wilson et al., 2006), explicit comparisons with other methods have not been included in opinion mining publications. sentiment strength classification has also been developed for a three-level scheme (low, medium, and high or extreme) for subjective sentences or clauses in newswire texts using a linguistic analysis converting sentences into dependency trees reflecting their structure (wilson et al., 2006). adding dependency trees to unigrams substantially improved the performance of various classifiers compared to unigrams alone, perhaps helped by the fairly large training set (9,313 sentences), the (presumably) good quality grammar of the texts, and the fairly low initial performance on this task (34.5% to 50.9% for unigrams, rising to 48.3% to 55.0% for the three types of classifier applied to level 1 clauses). here, svm regression was outperformed by both the rule-based learning ripper (cohen, 1995) and boostexter, a boosting algorithm combining multiple weak classifiers (schapire & singer, 2000). myspace language, significantly above the best standard machine-learning approaches, which had a performance of up to 58.5%--in line with those for a previous four-category opinion intensity classification task (wilson et al., 2006). the standard version of sentistrength was also better than standard machine-learning methods when their performance was improved (or not, in some cases) with the use of subsumption and information gain feature reduction, but the difference was not statistically significant. a slightly modified version of sentistrength was statistically significantly better than the improved machine learning methods, however. this is good evidence of the efficacy of sentistrength for positive sentiment strength detection given the range of different algorithms and parameters that it was compared against (9 algorithms x11 feature set sizes, x7 feature set types = 693 variations, plus 9 algorithms x10 feature set sizes x3 a values = 270 variations for subsumption), which gives lower-performing algorithms a reasonable statistical chance of outperforming sentistrength through chance, but none did.",2
"in terms of future work, a next logical step is to attempt to improve the performance of the system through linguistic processing, despite the poor grammar of the short informal text messages analyzed. previous work has shown that this approach is promising, particularly via dependency trees (wilson et al., 2009) and that, given a large enough training sample, improvements may be possible even in poor-quality text (gamon, 2004).",1
"opinion mining typically occurs in two or three stages, although more may be needed for some tasks (e.g., balahur et al., 2010). first, the input text is split into sections, such as sentences, and each section tested to see if it contains any sentiment: if it is subjective or objective (pang & lee, 2004). second, the subjective sentences are analyzed to detect their sentiment polarity. finally, the object about which the opinion is expressed may be extracted (e.g., gamon, aue, corstonoliver, & ringger, 2005). opinion mining normally deals with only positive and negative sentiment rather than discrete emotions (e.g., happiness, surprise), does not detect sentiment strength (but sometimes uses the strength of association of words with positive or negative sentiment, e.g., kaji & kitsuregawa, 2007), and does not simultaneously identify both positive and negative emotions. nevertheless, such opinion mining research can aid the simultaneous assessment of positive and negative sentiment strength both because of its general insights into sentiment analysis and also because most techniques could, in theory, be repurposed for this new task. for example, phrase analysis techniques could be applied to identify both positive and negative sentiment even within individual sentences (choi & cardie, 2008; wilson, 2008; wilson, wiebe, & hoffman, 2009). as well as unigrams, but the combined performance was better than that of unigrams alone (argamon et al., 2007). linguistic features have also been successfully used to extend opinion mining to a multiaspect variant that is able to detect opinions about different aspects of a topic (snyder & barzilay, 2007). a promising future approach is the incorporation of context about the reasons why sentiment is used, such as differentiating between intention, arguments, and speculation (wilson, 2008).",1
"sentistrength was tested on a set of 1,041 myspace comments that were different from the comments used in the development phase and were classified by three people (see table 1), and the average was used as the gold standard. a 10-fold cross-validation approach was used. the results were compared to random allocation and to the baseline majority class classification (a positive sentiment of 2 and a negative sentiment of 1). sentistrength was also compared to a range of standard machine-learning classification algorithms in weka (witten & frank, 2005) using the frequencies of each word in the sentiment word list as the feature set.",1
"researchers to investigate the connection between language and psychology (pennebaker et al., 2003) and also as a practical tool, for example, to detect how well people are likely to cope with bereavement based upon their language use (pennebaker, mayne, & francis, 1997). a related emotion detection approach differentiates between happy, unhappy. and neutral states based upon words used by students describing their daily lives (wu et al., 2006). this is similar to the typical positive/negative/neutral objective for opinion mining, however.",1
"for instance, there are studies on mobility, using scopus' unique historic author-affiliation records, such as by caroline wagner and koen jonkers on international collaboration, mobility, and openness (wagner & jonkers, 2017), funding and collaboration (leydesdorff, bornmann, & wagner, 2019), and author (pina, barac, buljan, grimaldo, & marusic, 2019) and institutional (lee, 2012) collaboration networks. another example of author-mobility analyses can be found in a bibliometric study to measure knowledge transfer (aman, 2018). the mobility analysis using scopus author profiles also informs the research policy of governments, such as through the european commission's joint research center (jrc) report on the rise of china as an industrial and innovation powerhouse (preziosi et al., 2019).",1
"a second type of analysis supported by scopus data is government science policy evaluations. the uk department of business, innovation and skills (bis) commissioned a report that entailed a comparative study of the uk's international research base, in 2011 (elsevier, 2011) and 2013 (elsevier, 2013). in 2016, another refresh of this report was issued by the newly renamed department for business, energy, & industrial strategy (beis) (elsevier, 2016). more examples of the use of scopus data for research policy reports include those of the european research council (erc) and other large government bodies, often dealing with program evaluations and research landscape analyses. many of these evaluations include different data sources of various types (macroeconomic data, for instance), as well as deep qualitative evaluations in which elsevier works in consortia.",1
"the first group of large-scale analyses deals with national research assessments. the first national assessment supported by scopus was the excellence in research for australia (era) of the australian research council (arc) in 2010, later repeated in 2012 and 2015. the first edition of the research excellence framework (ref 2014) national assessment in the uk also used scopus data. the ref 2014 was held by the higher education funding council for england (hefce) and the funding bodies for scotland, wales, and northern ireland. up to four research outputs per active researcher were submitted by the uk's higher education institutions (heis) and matched to the corresponding records in scopus for 11 out of 36 ""units of assessment(uoas; i.e., broad subject areas). for each of these 11 uoas, scopus citation counts of the submitted outputs were compared with that uoa's citation benchmarks (also obtained from scopus as contextual data) and used as an additional assessment criterion by the ref's expert panels, besides peer review of the scientific content of the outputs. prior to the announcement of the ref 2014 assessment results by hefce, the ref results analysis tool provided by elsevier allowed the uk's heis to compare and evaluate their own ref performance across several benchmarks and metrics. more examples of national assessments where scopus data were used include the 2013-2014 assessment in portugal held by the fct (""fundacao para a ciencia e a technologia""), the asn (""abilitazione scientifica nazionale"") national accreditation rounds (2012-2013, 2016-2018, and 2018-2020), vqr (""valutazione della qualita della ricerca"") national assessment in italy (2012-2013, 2016-2017), and national university corporation evaluation (nuce) national assessment in japan held in 2016-2017 and to be held in 2020 by the national institution for academic degrees and quality enhancement of higher education (niad-qe).",1
"scopus indexes many different elements of scientific publications, obtained from external publishers, such as publication title, abstract, keywords, author names and linked affiliations, references, and drug terms (berkvens, 2012). content in scopus contains publications from scientific publishers from all over the world. elsevier, the owner of scopus, is also a scientific publisher. about 9.9% of the serial titles (i.e., journals and book series) in scopus are published by elsevier (this amounts to an article share in scopus of 17.4% between 2012 and 2018); the other 90.1% of serial titles (and 82.6% of articles, respectively) are produced by an extensive list of global publishers (figure 1a). in addition, subject coverage of the serial titles in scopus is quite balanced among the four main subject categories (figure 1b). scopus also includes non-english content, as long as an english title and abstract, as well as references in roman script, are available. the first group of large-scale analyses deals with national research assessments. the first national assessment supported by scopus was the excellence in research for australia (era) of the australian research council (arc) in 2010, later repeated in 2012 and 2015. the first edition of the research excellence framework (ref 2014) national assessment in the uk also used scopus data. the ref 2014 was held by the higher education funding council for england (hefce) and the funding bodies for scotland, wales, and northern ireland. up to four research outputs per active researcher were submitted by the uk's higher education institutions (heis) and matched to the corresponding records in scopus for 11 out of 36 ""units of assessment(uoas; i.e., broad subject areas). for each of these 11 uoas, scopus citation counts of the submitted outputs were compared with that uoa's citation benchmarks (also obtained from scopus as contextual data) and used as an additional assessment criterion by the ref's expert panels, besides peer review of the scientific content of the outputs. prior to the announcement of the ref 2014 assessment results by hefce, the ref results analysis tool provided by elsevier allowed the uk's heis to compare and evaluate their own ref performance across several benchmarks and metrics. more examples of national assessments where scopus data were used include the 2013-2014 assessment in portugal held by the fct (""fundacao para a ciencia e a technologia""), the asn (""abilitazione scientifica nazionale"") national accreditation rounds (2012-2013, 2016-2018, and 2018-2020), vqr (""valutazione della qualita della ricerca"") national assessment in italy (2012-2013, 2016-2017), and national university corporation evaluation (nuce) national assessment in japan held in 2016-2017 and to be held in 2020 by the national institution for academic degrees and quality enhancement of higher education (niad-qe). engineering indicators (sei), for the 2016 (national science board, 2016) and 2018 (national science board, 2018) editions, and will be used in future editions of the sei reports up to 2022. scopus data are also the source of bibliometric indicators for the european research area in the context of the 2010-2014 study ""analysis and regular update of bibliometric indicators for the european commission(science-metrix, 2014) and have recently been selected for the continuation and improvement of this study, now called ""provision and analysis of key indicators in research and innovation,for the three coming years.",2
"another form of common analysis performed using scopus data is around network visualization and spatial bibliometrics (bornmann & de moya anegon, 2019; bornmann & waltman, 2011; leydesdorff & persson, 2010; mutz, bornmann, de moya anegon, & stefaner, 2014) as well as research building new visualization techniques (leydesdorff, 2010; mischo & schlembach, 2018).",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
elsevier. (2011). international comparative performance of the uk research base--2011. retrieved from https://www.elsevier.com/ research-intelligence/resource-library/international-comparativeperformance-of-the-uk-research-base-2011,1
elsevier. (2013). international comparative performance of the uk research base--2013. retrieved from https://www.elsevier.com/ research-intelligence/research-initiatives/ bis2013,1
elsevier. (2016). international comparative performance of the uk research base--2016. retrieved from https://www.elsevier.com/ research-intelligence/research-initiatives/ beis2016,1
elsevier. (2016). international comparative performance of the uk research base--2016. retrieved from https://www.elsevier.com/ research-intelligence/research-initiatives/ beis2016,1
"scopus indexes many different elements of scientific publications, obtained from external publishers, such as publication title, abstract, keywords, author names and linked affiliations, references, and drug terms (berkvens, 2012). content in scopus contains publications from scientific publishers from all over the world. elsevier, the owner of scopus, is also a scientific publisher. about 9.9% of the serial titles (i.e., journals and book series) in scopus are published by elsevier (this amounts to an article share in scopus of 17.4% between 2012 and 2018); the other 90.1% of serial titles (and 82.6% of articles, respectively) are produced by an extensive list of global publishers (figure 1a). in addition, subject coverage of the serial titles in scopus is quite balanced among the four main subject categories (figure 1b). scopus also includes non-english content, as long as an english title and abstract, as well as references in roman script, are available.",2
"an important aspect of the scopus database is the high coverage and availability of first names, even for relatively old records: from 25% of authorships in 1970-1974 and 52% between 1995-1999 to 82% of authorships in 2015-2019 (figure 1). this feature strengthens the disambiguation of authors and allows, for instance, gender-based longitudinal analyses that leverage first names (elsevier, 2017; lerchenmueller & sorenson, 2018). another relevant aspect in the analytical context is the availability of author-affiliation links in publications throughout the database historically. this enables studies dealing with the mobility of researchers, by analyzing the author affiliations and how they change over time.",2
"scopus indexes many different elements of scientific publications, obtained from external publishers, such as publication title, abstract, keywords, author names and linked affiliations, references, and drug terms (berkvens, 2012). content in scopus contains publications from scientific publishers from all over the world. elsevier, the owner of scopus, is also a scientific publisher. about 9.9% of the serial titles (i.e., journals and book series) in scopus are published by elsevier (this amounts to an article share in scopus of 17.4% between 2012 and 2018); the other 90.1% of serial titles (and 82.6% of articles, respectively) are produced by an extensive list of global publishers (figure 1a). in addition, subject coverage of the serial titles in scopus is quite balanced among the four main subject categories (figure 1b). scopus also includes non-english content, as long as an english title and abstract, as well as references in roman script, are available.",2
elsevier. (2013). international comparative performance of the uk research base--2013. retrieved from https://www.elsevier.com/ research-intelligence/research-initiatives/ bis2013,1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"engineering indicators (sei), for the 2016 (national science board, 2016) and 2018 (national science board, 2018) editions, and will be used in future editions of the sei reports up to 2022. scopus data are also the source of bibliometric indicators for the european research area in the context of the 2010-2014 study ""analysis and regular update of bibliometric indicators for the european commission(science-metrix, 2014) and have recently been selected for the continuation and improvement of this study, now called ""provision and analysis of key indicators in research and innovation,for the three coming years.",1
"engineering indicators (sei), for the 2016 (national science board, 2016) and 2018 (national science board, 2018) editions, and will be used in future editions of the sei reports up to 2022. scopus data are also the source of bibliometric indicators for the european research area in the context of the 2010-2014 study ""analysis and regular update of bibliometric indicators for the european commission(science-metrix, 2014) and have recently been selected for the continuation and improvement of this study, now called ""provision and analysis of key indicators in research and innovation,for the three coming years.",1
"another form of common analysis performed using scopus data is around network visualization and spatial bibliometrics (bornmann & de moya anegon, 2019; bornmann & waltman, 2011; leydesdorff & persson, 2010; mutz, bornmann, de moya anegon, & stefaner, 2014) as well as research building new visualization techniques (leydesdorff, 2010; mischo & schlembach, 2018).",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"in addition, scopus' availability of author first names historically, combined with author profiling, enables studies using author gender assignments: for example, ""the gender gap in early-career transitions in the life sciences(lerchenmueller & sorenson, 2018) and ""gender differences in research areas, methods and topics: can people and thing orientations explain the results?(thelwall, bailey, tobin, & bradshaw, 2019). in addition, scopus author profiles have been used to study the recent phenomenon of hyperprolific authorships (ioannidis, klavans, & boyack, 2018) and for an author database of highly cited researchers (ioannidis, baas, klavans, & boyack, 2019; van noorden & singh chawla, 2019).",1
"in addition, scopus' availability of author first names historically, combined with author profiling, enables studies using author gender assignments: for example, ""the gender gap in early-career transitions in the life sciences(lerchenmueller & sorenson, 2018) and ""gender differences in research areas, methods and topics: can people and thing orientations explain the results?(thelwall, bailey, tobin, & bradshaw, 2019). in addition, scopus author profiles have been used to study the recent phenomenon of hyperprolific authorships (ioannidis, klavans, & boyack, 2018) and for an author database of highly cited researchers (ioannidis, baas, klavans, & boyack, 2019; van noorden & singh chawla, 2019).",1
"in addition, scopus' availability of author first names historically, combined with author profiling, enables studies using author gender assignments: for example, ""the gender gap in early-career transitions in the life sciences(lerchenmueller & sorenson, 2018) and ""gender differences in research areas, methods and topics: can people and thing orientations explain the results?(thelwall, bailey, tobin, & bradshaw, 2019). in addition, scopus author profiles have been used to study the recent phenomenon of hyperprolific authorships (ioannidis, klavans, & boyack, 2018) and for an author database of highly cited researchers (ioannidis, baas, klavans, & boyack, 2019; van noorden & singh chawla, 2019).",1
"in addition, scopus' availability of author first names historically, combined with author profiling, enables studies using author gender assignments: for example, ""the gender gap in early-career transitions in the life sciences(lerchenmueller & sorenson, 2018) and ""gender differences in research areas, methods and topics: can people and thing orientations explain the results?(thelwall, bailey, tobin, & bradshaw, 2019). in addition, scopus author profiles have been used to study the recent phenomenon of hyperprolific authorships (ioannidis, klavans, & boyack, 2018) and for an author database of highly cited researchers (ioannidis, baas, klavans, & boyack, 2019; van noorden & singh chawla, 2019). on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"in addition, scopus data have been available in bulk for research groups. research groups working with bulk scopus data include cwts (cwts, 2019), scimago (scimago lab, 2019), dzhw (dzhw, 2019), scitech strategies (scitech strategies, 2018), and others through tailored agreements that have been established between these groups and elsevier.",1
"to maintain scopus as a high-quality data source and push the boundaries of quality forward, scopus introduced internal review processes to constantly monitor preidentified areas of quality focus, such as processing, profile quality, and completeness and accuracy of source data. this allows the content team to identify early trends in the data and to monitor progress on key initiatives to increase quality. for instance, under this program, digital object identifier (doi) completion rates went up from 87.8% at the start of the program to 99.8% in december 2018. the completeness was measured across a gold record set for which each should have a doi. other main focus areas where significant improvements have been made over the past few years, and where continuous investments are being made, are completeness of indexed publication records for the serial titles covered (by weekly comparisons against the crossref database), the removal of duplicate medline and article in press records, the correctness and completeness of citation links (by measuring against a gold set), of the author and institution profiles and publication record metadata (such as document type classifications, publication years, article numbers, country codes, and funding information), as well as improving the timeliness and currency of newly indexed content. these quality review processes employ machine learning approaches, supplemented with manual validation, and concern legacy content (i.e., content already indexed in scopus) as well as continuously improving and fine-tuning the capturing procedures for newly indexed content.",1
"in addition, scopus data have been available in bulk for research groups. research groups working with bulk scopus data include cwts (cwts, 2019), scimago (scimago lab, 2019), dzhw (dzhw, 2019), scitech strategies (scitech strategies, 2018), and others through tailored agreements that have been established between these groups and elsevier.",1
"a second type of analysis supported by scopus data is government science policy evaluations. the uk department of business, innovation and skills (bis) commissioned a report that entailed a comparative study of the uk's international research base, in 2011 (elsevier, 2011) and 2013 (elsevier, 2013). in 2016, another refresh of this report was issued by the newly renamed department for business, energy, & industrial strategy (beis) (elsevier, 2016). more examples of the use of scopus data for research policy reports include those of the european research council (erc) and other large government bodies, often dealing with program evaluations and research landscape analyses. many of these evaluations include different data sources of various types (macroeconomic data, for instance), as well as deep qualitative evaluations in which elsevier works in consortia.",1
"a second type of analysis supported by scopus data is government science policy evaluations. the uk department of business, innovation and skills (bis) commissioned a report that entailed a comparative study of the uk's international research base, in 2011 (elsevier, 2011) and 2013 (elsevier, 2013). in 2016, another refresh of this report was issued by the newly renamed department for business, energy, & industrial strategy (beis) (elsevier, 2016). more examples of the use of scopus data for research policy reports include those of the european research council (erc) and other large government bodies, often dealing with program evaluations and research landscape analyses. many of these evaluations include different data sources of various types (macroeconomic data, for instance), as well as deep qualitative evaluations in which elsevier works in consortia.",1
"a second type of analysis supported by scopus data is government science policy evaluations. the uk department of business, innovation and skills (bis) commissioned a report that entailed a comparative study of the uk's international research base, in 2011 (elsevier, 2011) and 2013 (elsevier, 2013). in 2016, another refresh of this report was issued by the newly renamed department for business, energy, & industrial strategy (beis) (elsevier, 2016). more examples of the use of scopus data for research policy reports include those of the european research council (erc) and other large government bodies, often dealing with program evaluations and research landscape analyses. many of these evaluations include different data sources of various types (macroeconomic data, for instance), as well as deep qualitative evaluations in which elsevier works in consortia.",1
"an important aspect of the scopus database is the high coverage and availability of first names, even for relatively old records: from 25% of authorships in 1970-1974 and 52% between 1995-1999 to 82% of authorships in 2015-2019 (figure 1). this feature strengthens the disambiguation of authors and allows, for instance, gender-based longitudinal analyses that leverage first names (elsevier, 2017; lerchenmueller & sorenson, 2018). another relevant aspect in the analytical context is the availability of author-affiliation links in publications throughout the database historically. this enables studies dealing with the mobility of researchers, by analyzing the author affiliations and how they change over time.",2
"has been installed. this means that titles that have been selected for inclusion in scopus may be discontinued and no longer indexed going forward. there are three different identification techniques applied, using (a) external feedback (i.e., formally raised concerns about publication standards), (b) heuristics (metrics) to flag underperforming journals, and (c) a machine learning approach to flag outlier behavior, which each lead to titles being tagged for re-evaluation. the ultimate decision to (de)select content lies with the external and independent scopus csab (for full details, please see elsevier, 2019b; holland, brimblecombe, meester, & steiginga, 2019). for publications of which the csab determines they no longer meet the quality standards for inclusion in scopus, indexing of new content is discontinued, but content already indexed remains in scopus, to ensure the integrity of the scientific record as well the stability and consistency of research trend analytics. since 2014, application programming interfaces (apis) have taken over the role of providing access to raw data, allowing free use for scientific purposes, such as the text-and-data-mining resources (elsevier, 2019c) and scopus apis for academic research purposes (elsevier, 2019a). use of the apis does not require a scopus subscription1; without a subscription, users will have limited access to basic metadata for most citation records, as well as to basic search functionality. full access to scopus apis is only granted to subscribers of scopus.",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"an important aspect of the scopus database is the high coverage and availability of first names, even for relatively old records: from 25% of authorships in 1970-1974 and 52% between 1995-1999 to 82% of authorships in 2015-2019 (figure 1). this feature strengthens the disambiguation of authors and allows, for instance, gender-based longitudinal analyses that leverage first names (elsevier, 2017; lerchenmueller & sorenson, 2018). another relevant aspect in the analytical context is the availability of author-affiliation links in publications throughout the database historically. this enables studies dealing with the mobility of researchers, by analyzing the author affiliations and how they change over time.",2
"in 2004, elsevier launched scopus as a new search and discovery tool (schotten, el aisati, meester, steiginga, & ross, 2017). scopus is an abstract and citation database consisting of peer-reviewed scientific content. at its launch, it contained about 27 million publication records (1966-2004). since then, the content of the database has grown to over 76 million records at the time of writing, covering publications from 1788-2019, making it among the largest curated bibliographic abstract and citation databases today. approximately 3 million new items are being added every year. the content in scopus is sourced from over 39,100 serial titles (with the most recently published content indexed from over 24,500 titles), since its official launch in 2004, scopus has been used globally in many large-scale analyses. there are three types of large studies where scopus data are used in a central role.",1
"the first group of large-scale analyses deals with national research assessments. the first national assessment supported by scopus was the excellence in research for australia (era) of the australian research council (arc) in 2010, later repeated in 2012 and 2015. the first edition of the research excellence framework (ref 2014) national assessment in the uk also used scopus data. the ref 2014 was held by the higher education funding council for england (hefce) and the funding bodies for scotland, wales, and northern ireland. up to four research outputs per active researcher were submitted by the uk's higher education institutions (heis) and matched to the corresponding records in scopus for 11 out of 36 ""units of assessment(uoas; i.e., broad subject areas). for each of these 11 uoas, scopus citation counts of the submitted outputs were compared with that uoa's citation benchmarks (also obtained from scopus as contextual data) and used as an additional assessment criterion by the ref's expert panels, besides peer review of the scientific content of the outputs. prior to the announcement of the ref 2014 assessment results by hefce, the ref results analysis tool provided by elsevier allowed the uk's heis to compare and evaluate their own ref performance across several benchmarks and metrics. more examples of national assessments where scopus data were used include the 2013-2014 assessment in portugal held by the fct (""fundacao para a ciencia e a technologia""), the asn (""abilitazione scientifica nazionale"") national accreditation rounds (2012-2013, 2016-2018, and 2018-2020), vqr (""valutazione della qualita della ricerca"") national assessment in italy (2012-2013, 2016-2017), and national university corporation evaluation (nuce) national assessment in japan held in 2016-2017 and to be held in 2020 by the national institution for academic degrees and quality enhancement of higher education (niad-qe).",1
"a second type of analysis supported by scopus data is government science policy evaluations. the uk department of business, innovation and skills (bis) commissioned a report that entailed a comparative study of the uk's international research base, in 2011 (elsevier, 2011) and 2013 (elsevier, 2013). in 2016, another refresh of this report was issued by the newly renamed department for business, energy, & industrial strategy (beis) (elsevier, 2016). more examples of the use of scopus data for research policy reports include those of the european research council (erc) and other large government bodies, often dealing with program evaluations and research landscape analyses. many of these evaluations include different data sources of various types (macroeconomic data, for instance), as well as deep qualitative evaluations in which elsevier works in consortia.",1
"a substantial number of author profiles in scopus have been curated. curation can be initiated through a variety of sources. a well-established process is through open researcher and contributor id (orcid), an open, non-proprietary registry of unique, persistent author identification codes (what is orcid?, 2018). orcid is managed by a non-profit organization with the same name established in 2012. researchers can export, import, or link their publications and curated metadata between scopus and orcid. similarly, researchers can use a feature on scopus.com called the ""author feedback wizard(afw) to improve their author profile. finally, yet another process that results in improved, curated scopus author profiles is a commercial service offered by elsevier to subscribers of its ""pureuniversity administration product, called profile refinement service. pure customers can opt to have their profiles refined upon request or refined every 4 months. elsevier uses the same service to proactively refine author profiles regardless of pure subscription whenever needed. the first group of large-scale analyses deals with national research assessments. the first national assessment supported by scopus was the excellence in research for australia (era) of the australian research council (arc) in 2010, later repeated in 2012 and 2015. the first edition of the research excellence framework (ref 2014) national assessment in the uk also used scopus data. the ref 2014 was held by the higher education funding council for england (hefce) and the funding bodies for scotland, wales, and northern ireland. up to four research outputs per active researcher were submitted by the uk's higher education institutions (heis) and matched to the corresponding records in scopus for 11 out of 36 ""units of assessment(uoas; i.e., broad subject areas). for each of these 11 uoas, scopus citation counts of the submitted outputs were compared with that uoa's citation benchmarks (also obtained from scopus as contextual data) and used as an additional assessment criterion by the ref's expert panels, besides peer review of the scientific content of the outputs. prior to the announcement of the ref 2014 assessment results by hefce, the ref results analysis tool provided by elsevier allowed the uk's heis to compare and evaluate their own ref performance across several benchmarks and metrics. more examples of national assessments where scopus data were used include the 2013-2014 assessment in portugal held by the fct (""fundacao para a ciencia e a technologia""), the asn (""abilitazione scientifica nazionale"") national accreditation rounds (2012-2013, 2016-2018, and 2018-2020), vqr (""valutazione della qualita della ricerca"") national assessment in italy (2012-2013, 2016-2017), and national university corporation evaluation (nuce) national assessment in japan held in 2016-2017 and to be held in 2020 by the national institution for academic degrees and quality enhancement of higher education (niad-qe).",1
"an important aspect of the scopus database is the high coverage and availability of first names, even for relatively old records: from 25% of authorships in 1970-1974 and 52% between 1995-1999 to 82% of authorships in 2015-2019 (figure 1). this feature strengthens the disambiguation of authors and allows, for instance, gender-based longitudinal analyses that leverage first names (elsevier, 2017; lerchenmueller & sorenson, 2018). another relevant aspect in the analytical context is the availability of author-affiliation links in publications throughout the database historically. this enables studies dealing with the mobility of researchers, by analyzing the author affiliations and how they change over time.",2
"the first group of large-scale analyses deals with national research assessments. the first national assessment supported by scopus was the excellence in research for australia (era) of the australian research council (arc) in 2010, later repeated in 2012 and 2015. the first edition of the research excellence framework (ref 2014) national assessment in the uk also used scopus data. the ref 2014 was held by the higher education funding council for england (hefce) and the funding bodies for scotland, wales, and northern ireland. up to four research outputs per active researcher were submitted by the uk's higher education institutions (heis) and matched to the corresponding records in scopus for 11 out of 36 ""units of assessment(uoas; i.e., broad subject areas). for each of these 11 uoas, scopus citation counts of the submitted outputs were compared with that uoa's citation benchmarks (also obtained from scopus as contextual data) and used as an additional assessment criterion by the ref's expert panels, besides peer review of the scientific content of the outputs. prior to the announcement of the ref 2014 assessment results by hefce, the ref results analysis tool provided by elsevier allowed the uk's heis to compare and evaluate their own ref performance across several benchmarks and metrics. more examples of national assessments where scopus data were used include the 2013-2014 assessment in portugal held by the fct (""fundacao para a ciencia e a technologia""), the asn (""abilitazione scientifica nazionale"") national accreditation rounds (2012-2013, 2016-2018, and 2018-2020), vqr (""valutazione della qualita della ricerca"") national assessment in italy (2012-2013, 2016-2017), and national university corporation evaluation (nuce) national assessment in japan held in 2016-2017 and to be held in 2020 by the national institution for academic degrees and quality enhancement of higher education (niad-qe). a second type of analysis supported by scopus data is government science policy evaluations. the uk department of business, innovation and skills (bis) commissioned a report that entailed a comparative study of the uk's international research base, in 2011 (elsevier, 2011) and 2013 (elsevier, 2013). in 2016, another refresh of this report was issued by the newly renamed department for business, energy, & industrial strategy (beis) (elsevier, 2016). more examples of the use of scopus data for research policy reports include those of the european research council (erc) and other large government bodies, often dealing with program evaluations and research landscape analyses. many of these evaluations include different data sources of various types (macroeconomic data, for instance), as well as deep qualitative evaluations in which elsevier works in consortia.",1
"the first group of large-scale analyses deals with national research assessments. the first national assessment supported by scopus was the excellence in research for australia (era) of the australian research council (arc) in 2010, later repeated in 2012 and 2015. the first edition of the research excellence framework (ref 2014) national assessment in the uk also used scopus data. the ref 2014 was held by the higher education funding council for england (hefce) and the funding bodies for scotland, wales, and northern ireland. up to four research outputs per active researcher were submitted by the uk's higher education institutions (heis) and matched to the corresponding records in scopus for 11 out of 36 ""units of assessment(uoas; i.e., broad subject areas). for each of these 11 uoas, scopus citation counts of the submitted outputs were compared with that uoa's citation benchmarks (also obtained from scopus as contextual data) and used as an additional assessment criterion by the ref's expert panels, besides peer review of the scientific content of the outputs. prior to the announcement of the ref 2014 assessment results by hefce, the ref results analysis tool provided by elsevier allowed the uk's heis to compare and evaluate their own ref performance across several benchmarks and metrics. more examples of national assessments where scopus data were used include the 2013-2014 assessment in portugal held by the fct (""fundacao para a ciencia e a technologia""), the asn (""abilitazione scientifica nazionale"") national accreditation rounds (2012-2013, 2016-2018, and 2018-2020), vqr (""valutazione della qualita della ricerca"") national assessment in italy (2012-2013, 2016-2017), and national university corporation evaluation (nuce) national assessment in japan held in 2016-2017 and to be held in 2020 by the national institution for academic degrees and quality enhancement of higher education (niad-qe).",1
national science board. (2016). science and engineering indicators 2016. national science foundation. retrieved from https:// www.nsf.gov/statistics/2016/nsb20161/,1
national science board. (2018). science and engineering indicators 2018. national science foundation. retrieved from https:// www.nsf.gov/statistics/2018/nsb20181/,2
elsevier. (2011). international comparative performance of the uk research base--2011. retrieved from https://www.elsevier.com/ research-intelligence/resource-library/international-comparativeperformance-of-the-uk-research-base-2011,1
"aman, v. (2018). a new bibliometric approach to measure knowledge transfer of internationally mobile scientists. scientometrics, 117(1), 227-247. https://doi.org/10.1007/s11192-018-2864-x baas, j., & fennell, c. (2019). when peer reviewers go rogue-estimated prevalence of citation manipulation by reviewers based on the citation patterns of 69,000 reviewers. issi 2019, september 2-5, 2019, rome, italy https://www.issi2019.org/. retrieved from https://ssrn.com/abstract=3339568",1
"aman, v. (2018). a new bibliometric approach to measure knowledge transfer of internationally mobile scientists. scientometrics, 117(1), 227-247. https://doi.org/10.1007/s11192-018-2864-x baas, j., & fennell, c. (2019). when peer reviewers go rogue-estimated prevalence of citation manipulation by reviewers based on the citation patterns of 69,000 reviewers. issi 2019, september 2-5, 2019, rome, italy https://www.issi2019.org/. retrieved from https://ssrn.com/abstract=3339568",1
"received: 04 july 2019 accepted: 26 october 2019 all above efforts combined have led to approximately 1.8 million scopus author profiles that have been manually enhanced (scopus index, july 2019). this total has been verified using a ""manual curatedflag in the xml data of scopus author profile records. nonetheless, we must emphasize that scopus creates author profiles that are being actively updated by algorithms for all",1
"scopus is among the largest curated abstract and citation databases, with a wide global and regional coverage of scientific journals, conference proceedings, and books, while ensuring only the highest quality data are indexed through rigorous content selection and re-evaluation by an independent content selection and advisory board. additionally, extensive quality assurance processes continuously monitor and improve all data elements in scopus. besides enriched metadata records of scientific articles, scopus offers comprehensive author and institution profiles, obtained from advanced profiling algorithms and manual curation, ensuring high precision and recall. the trustworthiness of scopus has led to its use as bibliometric data source for large-scale analyses in research assessments, research landscape studies, science policy evaluations, and university rankings. scopus data have been offered for free for selected studies by the academic research community, such as through application programming interfaces, which have led to many publications employing scopus data to investigate topics such as researcher mobility, network visualizations, and spatial bibliometrics. in june 2019, the international center for the study of research was launched, with an advisory board consisting of bibliometricians, aiming to work with the scientometric research community and offering a virtual laboratory where researchers will be able to utilize scopus data. this single example shows that those who create and those who use scopus suffer no lack of imagination to ask challenging questions, and scopus itself offers a firm base on which to begin seeking answers. the remaining piece of the puzzle is a collective one: how can the bibliometric research community and the creators of scopus best come together to address these challenges together? in june 2019, the icsr (international center for the study of research, 2019) was launched, with a wide-ranging brief and the support of an advisory board, including experts in research policy, research evaluation, and bibliometrics, to be a",1
"there are also examples of studies using the full scopus database to build new algorithms: richard klavans and kevin boyack developed algorithms on top of the database, resulting in topics of prominence (klavans & boyack, 2017), which are now prominently displayed in elsevier's scival research performance product (which uses scopus data as one of its data sources).",1
"in addition, scopus data have been available in bulk for research groups. research groups working with bulk scopus data include cwts (cwts, 2019), scimago (scimago lab, 2019), dzhw (dzhw, 2019), scitech strategies (scitech strategies, 2018), and others through tailored agreements that have been established between these groups and elsevier.",1
"for instance, there are studies on mobility, using scopus' unique historic author-affiliation records, such as by caroline wagner and koen jonkers on international collaboration, mobility, and openness (wagner & jonkers, 2017), funding and collaboration (leydesdorff, bornmann, & wagner, 2019), and author (pina, barac, buljan, grimaldo, & marusic, 2019) and institutional (lee, 2012) collaboration networks. another example of author-mobility analyses can be found in a bibliometric study to measure knowledge transfer (aman, 2018). the mobility analysis using scopus author profiles also informs the research policy of governments, such as through the european commission's joint research center (jrc) report on the rise of china as an industrial and innovation powerhouse (preziosi et al., 2019).",1
"an important aspect of the scopus database is the high coverage and availability of first names, even for relatively old records: from 25% of authorships in 1970-1974 and 52% between 1995-1999 to 82% of authorships in 2015-2019 (figure 1). this feature strengthens the disambiguation of authors and allows, for instance, gender-based longitudinal analyses that leverage first names (elsevier, 2017; lerchenmueller & sorenson, 2018). another relevant aspect in the analytical context is the availability of author-affiliation links in publications throughout the database historically. this enables studies dealing with the mobility of researchers, by analyzing the author affiliations and how they change over time. in addition, scopus' availability of author first names historically, combined with author profiling, enables studies using author gender assignments: for example, ""the gender gap in early-career transitions in the life sciences(lerchenmueller & sorenson, 2018) and ""gender differences in research areas, methods and topics: can people and thing orientations explain the results?(thelwall, bailey, tobin, & bradshaw, 2019). in addition, scopus author profiles have been used to study the recent phenomenon of hyperprolific authorships (ioannidis, klavans, & boyack, 2018) and for an author database of highly cited researchers (ioannidis, baas, klavans, & boyack, 2019; van noorden & singh chawla, 2019).",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"another form of common analysis performed using scopus data is around network visualization and spatial bibliometrics (bornmann & de moya anegon, 2019; bornmann & waltman, 2011; leydesdorff & persson, 2010; mutz, bornmann, de moya anegon, & stefaner, 2014) as well as research building new visualization techniques (leydesdorff, 2010; mischo & schlembach, 2018).",1
"another form of common analysis performed using scopus data is around network visualization and spatial bibliometrics (bornmann & de moya anegon, 2019; bornmann & waltman, 2011; leydesdorff & persson, 2010; mutz, bornmann, de moya anegon, & stefaner, 2014) as well as research building new visualization techniques (leydesdorff, 2010; mischo & schlembach, 2018).",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"for instance, there are studies on mobility, using scopus' unique historic author-affiliation records, such as by caroline wagner and koen jonkers on international collaboration, mobility, and openness (wagner & jonkers, 2017), funding and collaboration (leydesdorff, bornmann, & wagner, 2019), and author (pina, barac, buljan, grimaldo, & marusic, 2019) and institutional (lee, 2012) collaboration networks. another example of author-mobility analyses can be found in a bibliometric study to measure knowledge transfer (aman, 2018). the mobility analysis using scopus author profiles also informs the research policy of governments, such as through the european commission's joint research center (jrc) report on the rise of china as an industrial and innovation powerhouse (preziosi et al., 2019).",1
"another form of common analysis performed using scopus data is around network visualization and spatial bibliometrics (bornmann & de moya anegon, 2019; bornmann & waltman, 2011; leydesdorff & persson, 2010; mutz, bornmann, de moya anegon, & stefaner, 2014) as well as research building new visualization techniques (leydesdorff, 2010; mischo & schlembach, 2018).",1
national science board. (2016). science and engineering indicators 2016. national science foundation. retrieved from https:// www.nsf.gov/statistics/2016/nsb20161/,1
national science board. (2018). science and engineering indicators 2018. national science foundation. retrieved from https:// www.nsf.gov/statistics/2018/nsb20181/,2
received: 04 july 2019 accepted: 26 october 2019,1
"for instance, there are studies on mobility, using scopus' unique historic author-affiliation records, such as by caroline wagner and koen jonkers on international collaboration, mobility, and openness (wagner & jonkers, 2017), funding and collaboration (leydesdorff, bornmann, & wagner, 2019), and author (pina, barac, buljan, grimaldo, & marusic, 2019) and institutional (lee, 2012) collaboration networks. another example of author-mobility analyses can be found in a bibliometric study to measure knowledge transfer (aman, 2018). the mobility analysis using scopus author profiles also informs the research policy of governments, such as through the european commission's joint research center (jrc) report on the rise of china as an industrial and innovation powerhouse (preziosi et al., 2019).",1
"the first group of large-scale analyses deals with national research assessments. the first national assessment supported by scopus was the excellence in research for australia (era) of the australian research council (arc) in 2010, later repeated in 2012 and 2015. the first edition of the research excellence framework (ref 2014) national assessment in the uk also used scopus data. the ref 2014 was held by the higher education funding council for england (hefce) and the funding bodies for scotland, wales, and northern ireland. up to four research outputs per active researcher were submitted by the uk's higher education institutions (heis) and matched to the corresponding records in scopus for 11 out of 36 ""units of assessment(uoas; i.e., broad subject areas). for each of these 11 uoas, scopus citation counts of the submitted outputs were compared with that uoa's citation benchmarks (also obtained from scopus as contextual data) and used as an additional assessment criterion by the ref's expert panels, besides peer review of the scientific content of the outputs. prior to the announcement of the ref 2014 assessment results by hefce, the ref results analysis tool provided by elsevier allowed the uk's heis to compare and evaluate their own ref performance across several benchmarks and metrics. more examples of national assessments where scopus data were used include the 2013-2014 assessment in portugal held by the fct (""fundacao para a ciencia e a technologia""), the asn (""abilitazione scientifica nazionale"") national accreditation rounds (2012-2013, 2016-2018, and 2018-2020), vqr (""valutazione della qualita della ricerca"") national assessment in italy (2012-2013, 2016-2017), and national university corporation evaluation (nuce) national assessment in japan held in 2016-2017 and to be held in 2020 by the national institution for academic degrees and quality enhancement of higher education (niad-qe).",1
"the mission of the international center for the study of research (icsr) is to advance research evaluation in all fields of knowledge production. to foster this development, the icsr provides access to a working environment where new ideas and hypotheses can be tested against high-quality, large data sets. this platform, offering a virtual laboratory, will allow researchers to collaboratively develop indicators and methodologies. elsevier is providing computational access to scopus data for research purposes on this platform, free of charge. this will also enhance the reproducibility of scientometric studies, by enabling other researchers to verify published research findings using the same data set and methodologies with shared code. researchers can use the environment for such academic, noncommercial purposes, and access will be organized by the icsr to review submitted proposals for use of the lab as well as actively reaching out to researchers to collaboratively work on specific research problems. the platform allows researchers to create and extract aggregate derivatives that can be published as part of their work, under the condition that the source of the data is acknowledged. at the moment of writing, this platform is not yet publicly available and will be announced through the icsr website (international center for the study of research, 2019). this single example shows that those who create and those who use scopus suffer no lack of imagination to ask challenging questions, and scopus itself offers a firm base on which to begin seeking answers. the remaining piece of the puzzle is a collective one: how can the bibliometric research community and the creators of scopus best come together to address these challenges together? in june 2019, the icsr (international center for the study of research, 2019) was launched, with a wide-ranging brief and the support of an advisory board, including experts in research policy, research evaluation, and bibliometrics, to be a",1
"in 2004, elsevier launched scopus as a new search and discovery tool (schotten, el aisati, meester, steiginga, & ross, 2017). scopus is an abstract and citation database consisting of peer-reviewed scientific content. at its launch, it contained about 27 million publication records (1966-2004). since then, the content of the database has grown to over 76 million records at the time of writing, covering publications from 1788-2019, making it among the largest curated bibliographic abstract and citation databases today. approximately 3 million new items are being added every year. the content in scopus is sourced from over 39,100 serial titles (with the most recently published content indexed from over 24,500 titles),",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"engineering indicators (sei), for the 2016 (national science board, 2016) and 2018 (national science board, 2018) editions, and will be used in future editions of the sei reports up to 2022. scopus data are also the source of bibliometric indicators for the european research area in the context of the 2010-2014 study ""analysis and regular update of bibliometric indicators for the european commission(science-metrix, 2014) and have recently been selected for the continuation and improvement of this study, now called ""provision and analysis of key indicators in research and innovation,for the three coming years.",1
"the third type of analysis is that of university rankings. university rankings are often composed of combinations of evaluations for which only part is a bibliometric resource. different ranking bodies have variations of subjective and objective data sources to provide ranked lists of universities. these rankings and the media attention they draw provide a platform for academia to engage with the public. elsevier provides publication output, citation, and international collaboration data from scopus for each university to organizations in the field of university rankings. these include the world university rankings and its various derived regional, global subject, young university, and world reputation rankings, as well as the wall street journal/the college rankings, which are all issued by times higher education (the, since 2014); and the world university rankings and its various derived rankings issued by quacquarelli symonds (qs, since 2015); as well as various other regional and subjectspecific rankings, such as the best chinese universities ranking issued by shanghairanking consultancy (since 2015), perspektywy in poland, maclean's in canada, national institutional ranking framework (nirf) in india, the financial times global mba ranking, and the frankfurter allgemeine zeitung economists ranking in germany. since 2014, application programming interfaces (apis) have taken over the role of providing access to raw data, allowing free use for scientific purposes, such as the text-and-data-mining resources (elsevier, 2019c) and scopus apis for academic research purposes (elsevier, 2019a). use of the apis does not require a scopus subscription1; without a subscription, users will have limited access to basic metadata for most citation records, as well as to basic search functionality. full access to scopus apis is only granted to subscribers of scopus.",1
"the third type of analysis is that of university rankings. university rankings are often composed of combinations of evaluations for which only part is a bibliometric resource. different ranking bodies have variations of subjective and objective data sources to provide ranked lists of universities. these rankings and the media attention they draw provide a platform for academia to engage with the public. elsevier provides publication output, citation, and international collaboration data from scopus for each university to organizations in the field of university rankings. these include the world university rankings and its various derived regional, global subject, young university, and world reputation rankings, as well as the wall street journal/the college rankings, which are all issued by times higher education (the, since 2014); and the world university rankings and its various derived rankings issued by quacquarelli symonds (qs, since 2015); as well as various other regional and subjectspecific rankings, such as the best chinese universities ranking issued by shanghairanking consultancy (since 2015), perspektywy in poland, maclean's in canada, national institutional ranking framework (nirf) in india, the financial times global mba ranking, and the frankfurter allgemeine zeitung economists ranking in germany.",1
"another form of common analysis performed using scopus data is around network visualization and spatial bibliometrics (bornmann & de moya anegon, 2019; bornmann & waltman, 2011; leydesdorff & persson, 2010; mutz, bornmann, de moya anegon, & stefaner, 2014) as well as research building new visualization techniques (leydesdorff, 2010; mischo & schlembach, 2018).",1
"has been installed. this means that titles that have been selected for inclusion in scopus may be discontinued and no longer indexed going forward. there are three different identification techniques applied, using (a) external feedback (i.e., formally raised concerns about publication standards), (b) heuristics (metrics) to flag underperforming journals, and (c) a machine learning approach to flag outlier behavior, which each lead to titles being tagged for re-evaluation. the ultimate decision to (de)select content lies with the external and independent scopus csab (for full details, please see elsevier, 2019b; holland, brimblecombe, meester, & steiginga, 2019). for publications of which the csab determines they no longer meet the quality standards for inclusion in scopus, indexing of new content is discontinued, but content already indexed remains in scopus, to ensure the integrity of the scientific record as well the stability and consistency of research trend analytics.",1
"in addition, scopus data have been available in bulk for research groups. research groups working with bulk scopus data include cwts (cwts, 2019), scimago (scimago lab, 2019), dzhw (dzhw, 2019), scitech strategies (scitech strategies, 2018), and others through tailored agreements that have been established between these groups and elsevier.",1
"engineering indicators (sei), for the 2016 (national science board, 2016) and 2018 (national science board, 2018) editions, and will be used in future editions of the sei reports up to 2022. scopus data are also the source of bibliometric indicators for the european research area in the context of the 2010-2014 study ""analysis and regular update of bibliometric indicators for the european commission(science-metrix, 2014) and have recently been selected for the continuation and improvement of this study, now called ""provision and analysis of key indicators in research and innovation,for the three coming years.",1
"the first group of large-scale analyses deals with national research assessments. the first national assessment supported by scopus was the excellence in research for australia (era) of the australian research council (arc) in 2010, later repeated in 2012 and 2015. the first edition of the research excellence framework (ref 2014) national assessment in the uk also used scopus data. the ref 2014 was held by the higher education funding council for england (hefce) and the funding bodies for scotland, wales, and northern ireland. up to four research outputs per active researcher were submitted by the uk's higher education institutions (heis) and matched to the corresponding records in scopus for 11 out of 36 ""units of assessment(uoas; i.e., broad subject areas). for each of these 11 uoas, scopus citation counts of the submitted outputs were compared with that uoa's citation benchmarks (also obtained from scopus as contextual data) and used as an additional assessment criterion by the ref's expert panels, besides peer review of the scientific content of the outputs. prior to the announcement of the ref 2014 assessment results by hefce, the ref results analysis tool provided by elsevier allowed the uk's heis to compare and evaluate their own ref performance across several benchmarks and metrics. more examples of national assessments where scopus data were used include the 2013-2014 assessment in portugal held by the fct (""fundacao para a ciencia e a technologia""), the asn (""abilitazione scientifica nazionale"") national accreditation rounds (2012-2013, 2016-2018, and 2018-2020), vqr (""valutazione della qualita della ricerca"") national assessment in italy (2012-2013, 2016-2017), and national university corporation evaluation (nuce) national assessment in japan held in 2016-2017 and to be held in 2020 by the national institution for academic degrees and quality enhancement of higher education (niad-qe).",1
"engineering indicators (sei), for the 2016 (national science board, 2016) and 2018 (national science board, 2018) editions, and will be used in future editions of the sei reports up to 2022. scopus data are also the source of bibliometric indicators for the european research area in the context of the 2010-2014 study ""analysis and regular update of bibliometric indicators for the european commission(science-metrix, 2014) and have recently been selected for the continuation and improvement of this study, now called ""provision and analysis of key indicators in research and innovation,for the three coming years.",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"on indicators (thelwall, 2019; thelwall & fairclough, 2015), on correlation between citations and mendeley readership (maflahi & thelwall, 2016; thelwall & wilson, 2016), on journal usage (schloegl & gorraiz, 2010), and studies revisiting bibliometric laws (thelwall & wilson, 2014). scopus data were also used to analyze initiatives in open science, particularly open access (solomon, laakso, & bjork, 2013), citizen science (follett & strezov, 2015) and new tools in the scientific space, such as researchgate (thelwall & kousha, 2017). they have been used to evaluate the fate of rejected manuscripts (bornmann et al., 2009), to investigate potential citation manipulation by reviewers (baas & fennell, 2019; singh chawla, 2019) and to study the development of multidisciplinarity (levitt & thelwall, 2008). at present, scopus data are used for bibliometric analysis to inform the eu open science monitor (the lisbon council, cwts, & esade, 2018).",1
"engineering indicators (sei), for the 2016 (national science board, 2016) and 2018 (national science board, 2018) editions, and will be used in future editions of the sei reports up to 2022. scopus data are also the source of bibliometric indicators for the european research area in the context of the 2010-2014 study ""analysis and regular update of bibliometric indicators for the european commission(science-metrix, 2014) and have recently been selected for the continuation and improvement of this study, now called ""provision and analysis of key indicators in research and innovation,for the three coming years.",1
"next to the aim of supporting the academic community with robust results using reliable data in the analyses mentioned, providing access to raw data is an essential component in securing advancement of the bibliometric field. until 2014, elsevier supported bibliometricians with data using the elsevier bibliometric research program (ebrp). the program provided precompiled data sets to researchers, after a scientific board reviewed and approved a submitted proposal. its aim was to enable external research groups or individual researchers in the field of bibliometrics and quantitative research assessment to carry out strategic research using elsevier data and to present the outcomes in peer-reviewed journal papers and at international conferences.",1
"for instance, there are studies on mobility, using scopus' unique historic author-affiliation records, such as by caroline wagner and koen jonkers on international collaboration, mobility, and openness (wagner & jonkers, 2017), funding and collaboration (leydesdorff, bornmann, & wagner, 2019), and author (pina, barac, buljan, grimaldo, & marusic, 2019) and institutional (lee, 2012) collaboration networks. another example of author-mobility analyses can be found in a bibliometric study to measure knowledge transfer (aman, 2018). the mobility analysis using scopus author profiles also informs the research policy of governments, such as through the european commission's joint research center (jrc) report on the rise of china as an industrial and innovation powerhouse (preziosi et al., 2019).",1
"for instance, there are studies on mobility, using scopus' unique historic author-affiliation records, such as by caroline wagner and koen jonkers on international collaboration, mobility, and openness (wagner & jonkers, 2017), funding and collaboration (leydesdorff, bornmann, & wagner, 2019), and author (pina, barac, buljan, grimaldo, & marusic, 2019) and institutional (lee, 2012) collaboration networks. another example of author-mobility analyses can be found in a bibliometric study to measure knowledge transfer (aman, 2018). the mobility analysis using scopus author profiles also informs the research policy of governments, such as through the european commission's joint research center (jrc) report on the rise of china as an industrial and innovation powerhouse (preziosi et al., 2019).",1
